**<font size=8>fast.ai</font>**

## æ€»

| æ ‡é¢˜ | æ—¥æœŸ | ç¬”è®° |
| :--: | :--: | :--: |
|[å®˜ç½‘](https://www.fast.ai/)|-|-|
|[è®ºå›](https://forums.fast.ai/)|-|åŒ…å«é’ˆå¯¹å„ç‰ˆæœ¬è¯¾ç¨‹çš„æ¨¡å—|
|[Practical Deep Learning for Coders 2022](https://course.fast.ai/)|2022|2022ç‰ˆï¼Œ2 parts|
|Practical Deep Learning for Coders 2020|2020|2020ç‰ˆï¼Œå·²ç»è¢«æ›¿ä»£|
|[Practical Deep Learning for Coders 2019](https://course19.fast.ai/ )|2019|2019ç‰ˆï¼Œ2 partsï¼›[hiromisçš„notes (github.com)](https://github.com/hiromis/notes/tree/master)|
|Practical Deep Learning for Coders 2018|2018|2018ç‰ˆï¼Œå·²ç»è¢«æ›¿ä»£|
|[fast.ai](https://docs.fast.ai/)|-|fastaiçš„documentation|
|[Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD]([Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD: Howard, Jeremy, Gugger, Sylvain: 9781492045526: Amazon.com: Books](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527))|-|ä¸€æœ¬ä¹¦ï¼Œ[å…è´¹]([Practical Deep Learning for Coders - The book](https://course.fast.ai/Resources/book.html))|
|[new fast.ai course: A Code-First Introduction to Natural Language Processing â€“ fast.ai](https://www.fast.ai/posts/2019-07-08-fastai-nlp.html)|-|å…¶ä»–ç±»å‹è¯¾ç¨‹|



## [Practical Deep Learning 2022](https://course.fast.ai/) & [Fastbook](https://github.com/fastai/fastbook)

### 1: Getting started (PDL2022)

#### *Is it a bird? Creating a model from your own data*

*   æµç¨‹

  1. ä¸‹è½½åº“ï¼Œå‡†å¤‡å¥½
  2. ç”Ÿæˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
  3. è®­ç»ƒ
  4. éªŒè¯
* ä¸€äº›åˆå­¦

  ```python
  from fastcore.all import * 
  'å°½ç®¡fastaiå¯ä»¥è‡ªåŠ¨å¤„ç†å¯¹fastcoreçš„ä¾èµ–ï¼Œä½¿fastaiå¯ä»¥ä½¿ç”¨fastcoreçš„éƒ¨åˆ†åŠŸèƒ½ï¼Œä½†æ˜¯ä¸ºäº†ä»£ç æ›´ç®€æ´ï¼Œä¾ç„¶æ˜¾å¼åœ°å¯¼å…¥äº†fastcore'
  
  L #è½¬æ¢æˆfastcoreä¸­çš„å”¯ä¸€ä¸€ä¸ªç±»L
  dest = (path/o) #path/o
  failed = verify_images(get_image_files(path)) #failedæ˜¯Lç±»
  ```
  ```python
  from fastai.vision.all import *
  
  Image.open('path').to_thumb(200,200) #Image.open().to_thumb()ç­‰æ¯”ä¾‹æ”¾å¤§
  download_images(dest, urls=search_images(f'{o} photo')) #download_images
  resize_images(path/o, max_size=200, dest=path/o) #resize_images
  get_image_files(path) #get_image_files
  verify_images(get_image_files(path)) #verify_images
  failed.map(Path.unlink) #failed.map
  dls = DataBlock( #DataBlock
    blocks = (ImageBlock, CategoryBlock), #ImageBlock, CategoryBlock
    get_items = get_image_files, #get_image_files
    splitter = RandomSplitter(valid_pct = 0.2, seed = 42), #RandomSplitter
    get_y = parent_label, #parent_label
    item_tfms = [Resize(200, method = 'squish')] #Resize
  ).dataloaders(path, bs = 32) #dataloaders
  dls.show_batch(max_n=6) #show_batch
  learn = vision_learner(dls, resnet18, metrics = error_rate) #vision_learner
  learn.fine_tune(5) #fine_tune
  cat_or_dog,_,pros_cat = learn.predict(PILImage.create('dog.jpg')) #predict; PILImage.createä¹Ÿå¯å±äºfastaiï¼Œåªæ˜¯å®ƒæœ‰è‡ªå·±çš„åº•å±‚ä¾èµ–åº“
  ```

  ```python
  from duckduckgo_search import DDGS 'å¯¼å…¥åº“'
  from fastdownload import download_url 'ç”¨äºä¸‹è½½åº“'
  import time 'é‰´äºæ•ˆç‡éå¸¸é‡è¦ï¼Œå› æ­¤è¦è®°å½•time'
  ```

### 1: intro (fastbook1)

#### 1.1 åˆå­¦

```python
#å›¾åƒè¯†åˆ«
path = untar_data(URLs.PETS)/'images' 
#untar_data()ä»fastaiå†…ç½®çš„æ•°æ®åº“ä¸­ä¸‹è½½è§£å‹æ•°æ®å¹¶è¿›å…¥'images'çš„æ–‡ä»¶å¤¹ä¸­
#untar_data(URLs.PETS)è¿”å›äº†ä¸€ä¸ªPathå¯¹è±¡ï¼Œæ˜¯fastaiå†…ç½®åº“PETSçš„è·¯å¾„

def is_cat(x): return x[0].isupper() #è¿™ä¸ªæ•°æ®é›†ä¸­ï¼ŒCatæ˜¯å¤§å†™ï¼Œdogæ˜¯å°å†™ï¼Œä»¥æ­¤æ¥åŒºåˆ†çŒ«å’Œç‹—
dls = ImageDataLoaders.from_name_func( 
    path, 
    get_image_files(path),  #è·å¾—æ‰€æœ‰å›¾åƒæ–‡ä»¶
    valid_pct=0.2, seed=42,
    label_func=is_cat,  #è¿”å›Trueæˆ–False
    item_tfms=Resize(224) #ç¼©æ”¾åˆ°224*224åƒç´ 
)
#ImageDataLoaders.from_name_funcæ˜¯fastaiçš„é«˜çº§å°è£…ï¼Œåˆ›å»ºæ•°æ®åŠ è½½å™¨æ›´åŠ ç®€æ´ï¼Œä¸“é—¨ç”¨äºä»æ–‡ä»¶åæå–æ ‡ç­¾çš„ä»»åŠ¡
#å¦‚æœä½¿ç”¨DataBlockğŸ‘‡
dls = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=is_cat,
    item_tfms=Resize(224)
).dataloaders(path)
#`item_tfms`åº”ç”¨äºæ¯ä¸ªé¡¹ç›®ï¼ˆåœ¨æœ¬ä¾‹ä¸­ï¼Œæ¯ä¸ªé¡¹ç›®éƒ½è¢«è°ƒæ•´ä¸º 224 åƒç´ çš„æ­£æ–¹å½¢ï¼‰ï¼Œè€Œ`batch_tfms`åº”ç”¨äºä¸€æ¬¡å¤„ç†ä¸€æ‰¹é¡¹ç›®çš„ GPU

learn = vision_learner(dls, resnet34, metrics=error_rate) #error_rate & accuracy
learn.fine_tune(1)
```

```python
from types import SimpleNamespace
uploader = SimpleNamespace(data = ['images/chapter1_cat_example.jpg'])
#ç›¸å½“äºuploader = {'data':['images/chapter1_cat_example.jpg']}
```

```python
#Image Classification
#segmentation
path = untar_data(URLs.CAMVID_TINY) #å†…ç½®åº“
dls = SegmentationDataLoaders.from_label_func(
    path, bs=8, fnames = get_image_files(path/"images"),
    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',
    codes = np.loadtxt(path/'codes.txt', dtype=str) #è¿™ä¸ªæ˜¯å‚¨å­˜äº†åˆ†å‰²ä»»åŠ¡ä¸­çš„ç±»åˆ«çš„æ–‡ä»¶ï¼Œæ¯”å¦‚é“è·¯ã€å»ºç­‘ã€æ±½è½¦ç­‰ï¼Œoå°±æ˜¯fnamesä¸­çš„ä¸€ä¸ªå…ƒç´ ï¼Œo.stemæ˜¯æ–‡ä»¶åï¼Œo.suffixæ˜¯æ‰©å±•å
)
#label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}' #è‹¥oæ˜¯images/cat.jpgï¼Œåˆ™è¿”å›labels/cat_P.jpg

learn = unet_learner(dls, resnet34)
learn.fine_tune(8)

learn.show_results(max_n=6, figsize=(7,8))
```

```python
#NLP
from fastai.text.all import *

dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')
learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)
learn.fine_tune(4, 1e-2)

learn.predict("I really liked that movie!")
#ç»“æœï¼š('pos', tensor(1), tensor([0.0041, 0.9959]))
```

```python
#Tabular
from fastai.tabular.all import *
path = untar_data(URLs.ADULT_SAMPLE)

dls = TabularDataLoaders.from_csv(path/'adult.csv', #è¿™æ˜¯æŒ‡å®šæ•°æ®æ–‡ä»¶
                                  path=path, #è¿™æ˜¯æŒ‡å®šæ•°æ®æ–‡ä»¶æ‰€åœ¨çš„è·¯å¾„
                                  y_names="salary",
    cat_names = ['workclass', 'education', 'marital-status', 'occupation',
                 'relationship', 'race'],
    cont_names = ['age', 'fnlwgt', 'education-num'],
    procs = [Categorify, FillMissing, Normalize]) #è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†çš„æ­¥éª¤ã€‚fastai ä¼šè‡ªåŠ¨åº”ç”¨è¿™äº›é¢„å¤„ç†æ­¥éª¤ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œé¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬ï¼šCategorify: å°†åˆ†ç±»å˜é‡è½¬æ¢ä¸ºç±»åˆ«ç±»å‹ï¼ˆé€šå¸¸æ˜¯æ•´æ•°ç¼–ç ï¼‰ï¼›FillMissing: å¡«å……ç¼ºå¤±å€¼ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼›Normalize: å¯¹è¿ç»­å˜é‡è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼ˆé€šå¸¸æ˜¯å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®ï¼‰ã€‚

learn = tabular_learner(dls, metrics=accuracy) #salaryæ˜¯åˆ†ç±»æ˜¯å¦ä¸ºé«˜æ”¶å…¥è€…ï¼Œæ‰€ä»¥metricsä»ç”¨accuracyæˆ–error_rateå¦‚æœæ˜¯è¿ç»­å˜é‡ï¼Œåˆ™ä¸èƒ½ä½¿ç”¨è¿™ä¸ªmetrics
learn.fit_one_cycle(3) #æ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‰€ä»¥ä¸ç”¨fine_tune
```

```python
#Collaborative Filteringï¼Œåè°ƒè¿‡æ»¤çš„ç»“æ„ä¸€èˆ¬æ¯”è¾ƒç®€å•
#æ ¹æ®ç”¨æˆ·ä»¥å‰çš„è§‚å½±ä¹ æƒ¯ï¼Œé¢„æµ‹ç”¨æˆ·å¯èƒ½å–œæ¬¢çš„ç”µå½±
from fastai.collab import *
path = untar_data(URLs.ML_SAMPLE)
dls = CollabDataLoaders.from_csv(path/'ratings.csv') #è¿™æ˜¯ç”¨äº†é»˜è®¤è®¾ç½®ï¼Œcsvä¸­æœ€åä¸€åˆ—æ˜¯å› å˜é‡ï¼›ååŒè¿‡æ»¤çš„ç‰¹å¾é€šå¸¸ä¹Ÿéƒ½æ˜¯ç¦»æ•£çš„åˆ†ç±»å‚æ•°ï¼Œå¦‚æœæœ‰è¿ç»­å‚æ•°ï¼Œå°±ä¸èƒ½ç”¨CollabDataLoadersç±»ç®€å•é¢„æµ‹
learn = collab_learner(dls, y_range=(0.5,5.5))
learn.fine_tune(10) #è¿™é‡Œä½¿ç”¨äº†fine_tuneè€Œä¸æ˜¯fit_one_cycle
learn.show_results()
```

#### 1.2 **æ€æ ·å¿«é€Ÿè·å–fastaiä¸­æ–¹æ³•çš„è§£é‡Š-doc**

```python
doc(learn.predict)
'''è¿”å›learn.predictæ–¹æ³•çš„è§£é‡Š'''
```

#### 1.3 è¿‡æ‹Ÿåˆ

å³ä½¿æ‚¨çš„æ¨¡å‹å°šæœªå®Œå…¨è®°ä½æ‰€æœ‰æ•°æ®ï¼Œåœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µå¯èƒ½å·²ç»è®°ä½äº†å…¶ä¸­çš„æŸäº›éƒ¨åˆ†ã€‚å› æ­¤ï¼Œæ‚¨è®­ç»ƒçš„æ—¶é—´è¶Šé•¿ï¼Œæ‚¨åœ¨è®­ç»ƒé›†ä¸Šçš„å‡†ç¡®æ€§å°±ä¼šè¶Šå¥½ï¼›éªŒè¯é›†çš„å‡†ç¡®æ€§ä¹Ÿä¼šåœ¨ä¸€æ®µæ—¶é—´å†…æé«˜ï¼Œä½†æœ€ç»ˆä¼šå¼€å§‹å˜å·®ï¼Œå› ä¸ºæ¨¡å‹å¼€å§‹è®°ä½è®­ç»ƒé›†è€Œä¸æ˜¯åœ¨æ•°æ®ä¸­æ‰¾åˆ°å¯æ³›åŒ–çš„æ½œåœ¨æ¨¡å¼ã€‚å½“è¿™ç§æƒ…å†µå‘ç”Ÿæ—¶ï¼Œæˆ‘ä»¬è¯´æ¨¡å‹*è¿‡æ‹Ÿåˆ*ã€‚

æˆ‘ä»¬æœ‰å¾ˆå¤šé¿å…è¿‡æ‹Ÿåˆçš„åŠæ³•ï¼Œä½†åªæœ‰çœŸçš„å‡ºç°è¿‡æ‹Ÿåˆäº†æ‰ä¼šç”¨è¿™äº›åŠæ³•ã€‚æˆ‘ä»¬ç»å¸¸çœ‹åˆ°ä¸€äº›äººè®­ç»ƒæ¨¡å‹ï¼Œä»–ä»¬æœ‰å……è¶³çš„æ•°æ®ï¼Œä½†æ˜¯è¿‡æ—©åœ°ä½¿ç”¨äº†é¿å…è¿‡æ‹Ÿåˆçš„åŠæ³•ï¼Œç»“æœå¯¼è‡´æ¨¡å‹çš„å‡†ç¡®æ€§ä¸å¥½ï¼Œè¿˜ä¸å¦‚è¿‡æ‹Ÿåˆäº†çš„æ¨¡å‹å‡†ç¡®æ€§é«˜ã€‚

#### 1.4 æ¶æ„

* CNNï¼šåˆ›å»ºè®¡ç®—æœºè§†è§‰æ¨¡å‹çš„å½“å‰æœ€å…ˆè¿›æ–¹æ³•

  ResNetï¼Œä¸€ç§æ ‡å‡†æ¶æ„ï¼Œæœ‰18ã€34ã€50ã€101å’Œ152

#### 1.5 é¢„è®­ç»ƒ/è¿ç§»å­¦ä¹ 

ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ˜¯æˆ‘ä»¬è®­ç»ƒæ›´å‡†ç¡®ã€æ›´å¿«é€Ÿã€ä½¿ç”¨æ›´å°‘æ•°æ®å’Œæ›´å°‘æ—¶é—´å’Œé‡‘é’±çš„æœ€é‡è¦æ–¹æ³•ã€‚æ‚¨å¯èƒ½ä¼šè®¤ä¸ºä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å°†æ˜¯å­¦æœ¯æ·±åº¦å­¦ä¹ ä¸­æœ€ç ”ç©¶çš„é¢†åŸŸ...ä½†æ‚¨ä¼šéå¸¸ã€éå¸¸é”™è¯¯ï¼é¢„è®­ç»ƒæ¨¡å‹çš„é‡è¦æ€§é€šå¸¸åœ¨å¤§å¤šæ•°è¯¾ç¨‹ã€ä¹¦ç±æˆ–è½¯ä»¶åº“åŠŸèƒ½ä¸­**æ²¡æœ‰**å¾—åˆ°è®¤å¯æˆ–è®¨è®ºï¼Œå¹¶ä¸”åœ¨å­¦æœ¯è®ºæ–‡ä¸­å¾ˆå°‘è¢«è€ƒè™‘ã€‚å½“æˆ‘ä»¬åœ¨ 2020 å¹´åˆå†™è¿™ç¯‡æ–‡ç« æ—¶ï¼Œäº‹æƒ…åˆšåˆšå¼€å§‹æ”¹å˜ï¼Œä½†è¿™å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ã€‚å› æ­¤è¦å°å¿ƒï¼šæ‚¨ä¸ä¹‹äº¤è°ˆçš„å¤§å¤šæ•°äººå¯èƒ½ä¼šä¸¥é‡ä½ä¼°æ‚¨å¯ä»¥åœ¨æ·±åº¦å­¦ä¹ ä¸­ä½¿ç”¨å°‘é‡èµ„æºåšäº›ä»€ä¹ˆï¼Œå› ä¸ºä»–ä»¬å¯èƒ½ä¸ä¼šæ·±å…¥äº†è§£å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚

ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ¥æ‰§è¡Œä¸€ä¸ªä¸å…¶æœ€åˆè®­ç»ƒç›®çš„ä¸åŒçš„ä»»åŠ¡è¢«ç§°ä¸º***è¿ç§»å­¦ä¹ ***ã€‚ä¸å¹¸çš„æ˜¯ï¼Œ**ç”±äºè¿ç§»å­¦ä¹ ç ”ç©¶ä¸è¶³ï¼Œå¾ˆå°‘æœ‰é¢†åŸŸæä¾›é¢„è®­ç»ƒæ¨¡å‹**ã€‚ä¾‹å¦‚ï¼Œç›®å‰åœ¨åŒ»å­¦é¢†åŸŸå¾ˆå°‘æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨ï¼Œè¿™ä½¿å¾—åœ¨è¯¥é¢†åŸŸä½¿ç”¨è¿ç§»å­¦ä¹ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œç›®å‰è¿˜ä¸æ¸…æ¥šå¦‚ä½•å°†è¿ç§»å­¦ä¹ åº”ç”¨äºè¯¸å¦‚æ—¶é—´åºåˆ—åˆ†æä¹‹ç±»çš„ä»»åŠ¡ã€‚

#### 1.6 head

When using a pretrained model, `vision_learner` will remove the last layer, since that is always specifically customized to the original training task (i.e. ImageNet dataset classification), and replace it with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the *head*.

#### 1.7 è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†

åœ¨ç”¨è®­ç»ƒé›†è®­ç»ƒåï¼Œæˆ‘ä»¬ç”¨éªŒè¯é›†æŸ¥çœ‹è®­ç»ƒæ•ˆæœï¼Œæ ¹æ®éªŒè¯é›†çš„æ•ˆæœï¼Œè°ƒæ•´è¶…å‚æ•°hyperparameterï¼Œå› æ­¤ï¼ŒéªŒè¯é›†ä»ç„¶åŠæš´éœ²åœ¨è®­ç»ƒæ¨¡å‹ä¸­ã€‚ä¸ºäº†èƒ½æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ•ˆæœï¼Œç”¨éªŒè¯é›†æ˜¾ç„¶æ˜¯ä¸ç†æƒ³çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜ä¼šéš”ç»å‡ºä¸€ä¸ªå®Œå…¨æ²¡æœ‰ç”¨è¿‡çš„æµ‹è¯•é›†ã€‚

å½“ç„¶å¦‚æœæ•°æ®é‡ä¸å¤Ÿï¼Œæµ‹è¯•é›†å¹¶ä¸ä¸€å®šæ˜¯å¿…é¡»çš„ï¼Œä½†æ˜¯å½“ç„¶æœ‰æ˜¯æœ€å¥½çš„ã€‚

åœ¨æˆ‘ä»¬å®é™…çš„è®­ç»ƒä¸­ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†çš„é€‰æ‹©å¾ˆæœ‰è®²ç©¶ã€‚æ¯”å¦‚æˆ‘ä»¬é¢„æµ‹æ—¶é—´åºåˆ—ï¼Œæœ€å¥½çš„åˆ’åˆ†æ˜¯æŠŠæœ€è¿‘çš„ä¸€æ®µæ—¶é—´ä½œä¸ºéªŒè¯é›†/æµ‹è¯•é›†ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥è¯„ä¼°æ¨¡å‹å¯¹æœªæ¥çš„é¢„æµ‹æ•ˆæœï¼›æ¯”å¦‚æˆ‘ä»¬è¯†åˆ«é©¾é©¶å‘˜çš„è¡Œä¸ºï¼Œæœ€å¥½çš„åˆ’åˆ†æ˜¯æŠŠä¸€ä¸ªé©¾é©¶å‘˜å®Œå…¨éš”ç»æˆéªŒè¯é›†/æµ‹è¯•é›†ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥è¯„ä¼°æ¨¡å‹å¯¹ä¸åŒçš„äººæ˜¯ä¸æ˜¯éƒ½æœ‰å¾ˆå¥½çš„è¯†åˆ«æ•ˆæœã€‚

#### 1.8 å…¶ä»–

* æ—¶é—´åºåˆ—è½¬æ¢æˆå›¾åƒ

  æ—¶é—´åºåˆ—æ•°æ®æœ‰å„ç§è½¬æ¢æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œfast.ai å­¦ç”Ÿ Ignacio Oguiza ä½¿ç”¨ä¸€ç§ç§°ä¸º**Gramian Angular Difference Fieldï¼ˆGADFï¼‰**çš„æŠ€æœ¯ï¼Œä»ä¸€ä¸ªæ—¶é—´åºåˆ—æ•°æ®é›†ä¸­ä¸ºæ©„æ¦„æ²¹åˆ†ç±»åˆ›å»ºå›¾åƒï¼Œä½ å¯ä»¥åœ¨å›¾ 1-15 ä¸­çœ‹åˆ°ç»“æœã€‚ç„¶åï¼Œä»–å°†è¿™äº›å›¾åƒè¾“å…¥åˆ°ä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡å‹ä¸­ï¼Œå°±åƒä½ åœ¨æœ¬ç« ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚å°½ç®¡åªæœ‰ 30 ä¸ªè®­ç»ƒé›†å›¾åƒï¼Œä½†ä»–çš„ç»“æœå‡†ç¡®ç‡è¶…è¿‡ 90%ï¼Œæ¥è¿‘æœ€å…ˆè¿›æ°´å¹³ã€‚

  ![](img/dlcf_0115.png)

  å¦ä¸€ä¸ªæœ‰è¶£çš„ fast.ai å­¦ç”Ÿé¡¹ç›®ç¤ºä¾‹æ¥è‡ª Gleb Esmanã€‚ä»–åœ¨ Splunk ä¸Šè¿›è¡Œæ¬ºè¯ˆæ£€æµ‹ï¼Œä½¿ç”¨äº†ç”¨æˆ·é¼ æ ‡ç§»åŠ¨å’Œé¼ æ ‡ç‚¹å‡»çš„æ•°æ®é›†ã€‚ä»–é€šè¿‡ç»˜åˆ¶æ˜¾ç¤ºé¼ æ ‡æŒ‡é’ˆä½ç½®ã€é€Ÿåº¦å’ŒåŠ é€Ÿåº¦çš„å›¾åƒï¼Œä½¿ç”¨å½©è‰²çº¿æ¡ï¼Œå¹¶ä½¿ç”¨[å°å½©è‰²åœ†åœˆ](https://oreil.ly/6-I_X)æ˜¾ç¤ºç‚¹å‡»ï¼Œå°†è¿™äº›è½¬æ¢ä¸ºå›¾ç‰‡ï¼Œå¦‚å›¾ 1-16 æ‰€ç¤ºã€‚ä»–å°†è¿™äº›è¾“å…¥åˆ°ä¸€ä¸ªå›¾åƒè¯†åˆ«æ¨¡å‹ä¸­ï¼Œå°±åƒæˆ‘ä»¬åœ¨æœ¬ç« ä¸­ä½¿ç”¨çš„é‚£æ ·ï¼Œæ•ˆæœéå¸¸å¥½ï¼Œå¯¼è‡´äº†è¿™ç§æ–¹æ³•åœ¨æ¬ºè¯ˆåˆ†ææ–¹é¢çš„ä¸“åˆ©ï¼

  ![](img/dlcf_0116.png)

  å¦ä¸€ä¸ªä¾‹å­æ¥è‡ª Mahmoud Kalash ç­‰äººçš„è®ºæ–‡â€œä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œæ¶æ„è½¯ä»¶åˆ†ç±»â€ï¼Œè§£é‡Šäº†â€œæ¶æ„è½¯ä»¶äºŒè¿›åˆ¶æ–‡ä»¶è¢«åˆ†æˆ 8 ä½åºåˆ—ï¼Œç„¶åè½¬æ¢ä¸ºç­‰æ•ˆçš„åè¿›åˆ¶å€¼ã€‚è¿™ä¸ªåè¿›åˆ¶å‘é‡è¢«é‡å¡‘ï¼Œç”Ÿæˆäº†ä¸€ä¸ªä»£è¡¨æ¶æ„è½¯ä»¶æ ·æœ¬çš„ç°åº¦å›¾åƒâ€ï¼Œå¦‚å›¾ 1-17 æ‰€ç¤ºã€‚

  ![](img/dlcf_0117.png)

#### 1.9 æœ¯è¯­

  | Term             | Meaning                                                      |
  | ---------------- | ------------------------------------------------------------ |
  | Label            | The data that we're trying to predict, such as "dog" or "cat" |
  | Architecture     | The _template_ of the model that we're trying to fit; the actual mathematical function that we're passing the input data and parameters to |
  | Model            | The combination of the architecture with a particular set of parameters |
  | Parameters       | The values in the model that change what task it can do, and are updated through model training |
  | Fit/æ‹Ÿåˆ         | Update the parameters of the model such that the predictions of the model using the input data match the target labels |
  | Train            | A synonym for _fit_                                          |
  | Pretrained model | A model that has already been trained, generally using a large dataset, and will be fine-tuned |
  | Fine-tune        | Update a pretrained model for a different task               |
  | Epoch            | One complete pass through the input data                     |
  | Loss             | A measure of how good the model is, chosen to drive training via SGD |
  | Metric           | A measurement of how good the model is, using the validation set, chosen for human consumption |
  | Validation set   | A set of data held out from training, used only for measuring how good the model is |
  | Training set     | The data used for fitting the model; does not include any data from the validation set |
  | Overfitting      | Training a model in such a way that it _remembers_ specific features of the input data, rather than generalizing well to data not seen during training |
  | CNN              | Convolutional neural network; a type of neural network that works particularly well for computer vision tasks |

### 2ï¼šDeployment (PDL2022)-æš‚æ—¶è·³è¿‡

### 2: Production (fastbook)-æš‚æ—¶è·³è¿‡

### 3: Neural net foundations (PDL2022)

#### *How does a neural net really work?*

paperspace

### 3: mnist_basics (fastbook4)

#### 3.1 åƒç´ ï¼šè®¡ç®—æœºè§†è§‰çš„åŸºç¡€

```python
from fastcore.all import *
from fastai.vision.all import *

path = untar_data(URLs.MNIST_SAMPLE)
path.ls()  #Lç±»
(path/'train').ls()

threes = (path/'train'/'3').ls().sorted()
sevens = (path/'train'/'7').ls().sorted()
threes

im3 = Image.open(threes[1]) #PILæ‰“å¼€image
im3

im3_t = tensor(im3)[4:12,4:10]
print(im3_t)
len(im3_t)
df = pd.DataFrame(im3_t)
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')

seven_tensors = [tensor(Image.open(o)) for o in sevens]
three_tensors = [tensor(Image.open(o)) for o in threes]
len(three_tensors), len(seven_tensors) #â€˜6265â€™æ–‡ä»¶å¤¹7è½¬æ¢ä¸ºseven_tensorsï¼Œè¿™å¤§æ¦‚å¯ä»¥ç®—ä½œæ˜¯ä¸€ä¸ªå¼ é‡çš„listï¼ˆLç±»ï¼‰
show_image(three_tensors[1]) #ä¸ç”¨PILæ‰“å¼€imageï¼Œç”¨fasiaiä¸­çš„show_imageæ‰“å¼€tensorä»£è¡¨çš„imageï¼Œä¹Ÿæ˜¯ä¸ªå›¾ç‰‡

stacked_sevens = torch.stack(seven_tensors).float()/255 #æŠŠè¿™ä¸ªtensorçš„Lç”¨torch.stackæ–¹æ³•å †å æˆä¸€ä¸ª3-rankå¼ é‡
stacked_threes = torch.stack(three_tensors).float()/255
stacked_threes.shape #torch.Size([6131, 28, 28])
len(stacked_sevens.shape) #å¼ é‡çš„ç§©ï¼Œä¹Ÿå°±æ˜¯å¼ é‡çš„ç»´åº¦

mean3 = stacked_threes.mean(0) #æ²¿ç€ç»´åº¦0æ±‚å¹³å‡å€¼ï¼Œå˜æˆ2-rank
show_image(mean3)
mean7 = stacked_sevens.mean(0)

a_3 = stacked_threes[1] #floatï¼Œç¬¬ä¸€å¼ å›¾
F.l1_loss(a_3, mean7) #l1æ˜¯æ ‡å‡†æ•°å­¦æœ¯è¯­å¹³å‡ç»å¯¹å€¼çš„ç¼©å†™ï¼ˆåœ¨æ•°å­¦ä¸­ç§°ä¸ºL1 èŒƒæ•°ï¼‰
F.mse_loss(a_3, mean7).sqrt() #mseå‡æ–¹è¯¯å·®ï¼Œsqrt()å¼€æ ¹ï¼ŒRMSEå‡æ–¹æ ¹è¯¯å·®æ˜¯L2èŒƒæ•°
#MSEç›¸æ¯”L1èŒƒæ•°æ¥è¯´ä¼šæ›´ç‹ åœ°æƒ©ç½šå¤§çš„è¯¯å·®ï¼Œè€Œå¯¹å°è¯¯å·®æ›´åŠ å®½å®¹
```

#### 3.2 NumPy æ•°ç»„å’Œ PyTorch å¼ é‡

* [NumPy](https://numpy.org) æ˜¯ Python ä¸­ç”¨äºç§‘å­¦å’Œæ•°å€¼ç¼–ç¨‹æœ€å¹¿æ³›ä½¿ç”¨çš„åº“ã€‚å®ƒæä¾›äº†ç±»ä¼¼çš„åŠŸèƒ½å’Œç±»ä¼¼çš„ APIï¼Œä¸ PyTorch æä¾›çš„åŠŸèƒ½ç›¸ä¼¼ï¼›ç„¶è€Œï¼Œå®ƒä¸æ”¯æŒä½¿ç”¨ GPU æˆ–è®¡ç®—æ¢¯åº¦ï¼Œè¿™ä¸¤è€…å¯¹äºæ·±åº¦å­¦ä¹ éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚

| #    | Numpy                                          | Pytorch Tensor                        |
| ---- | ---------------------------------------------- | ------------------------------------- |
| 1    | ä¸è§„åˆ™æ•°ç»„ï¼šå¯ä»¥æ˜¯æ•°ç»„çš„æ•°ç»„ï¼Œå†…éƒ¨æ•°ç»„å¤§å°ä¸åŒ | ä¸å¯ä»¥æ˜¯ä¸è§„åˆ™çš„                      |
| 2    | ä¸èƒ½å­˜åœ¨GPUä¸Š                                  | å¯ä»¥å­˜å‚¨åœ¨GPUä¸Šï¼Œåç»­è®­ç»ƒæ›´å¿«         |
| 3    | ä¸èƒ½è®¡ç®—å¯¼æ•°                                   | å¯ä»¥è‡ªåŠ¨è®¡ç®—å¯¼æ•°ï¼Œå¯ä»¥è¿›è¡ŒSGDæ¢¯åº¦è®¡ç®— |

```py
data = [[1,2,3],[4,5,6]]
arr = array (data)
tns = tensor(data)
tns[1]  #tensor([4, 5, 6])
tns[:, 1] #tensor([2, 5])
tns +1 # tensor([[2, 3, 4],[5, 6, 7]])
```

#### 3.3 ä½¿ç”¨Broadcastingè®¡ç®—Metrics

* å¯ä»¥ä½¿ç”¨MSEæˆ–L1èŒƒæ•°ä½œä¸ºmetrcsï¼Œä½†æ˜¯æœ‰æ—¶å€™ä¸å¤ªå¥½ç†è§£ï¼Œæ‰€ä»¥ä¸€èˆ¬æƒ…å†µä¸‹ä½¿ç”¨**accuracy**ä½œä¸ºmetrics

```python
valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]).float()/255
valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]).float()/255
valid_3_tens.shape, valid_7_tens.shape #validä¸­çš„å›¾ç‰‡è½¬æ¢æˆä¸€ä¸ªrank-3çš„å½’ä¸€åŒ–tensor

def mnist_distance(a,b):    return (a-b).abs().mean((-1,-2)) #å®šä¹‰ä¸€ä¸ªæ–¹æ³•è®¡ç®—L1èŒƒæ•°ï¼Œ-1/-2æ˜¯å‘Šè¯‰tensorè¦å¯¹æœ€åä¸¤ä¸ªè½´è¿›è¡Œå¹³å‡
mnist_distance(a_3, mean3)
valid_3_dist = mnist_distance(valid_3_tens, mean3) #è¿™é‡Œè‡ªåŠ¨ä½¿ç”¨äº†broadcastingå°†valid_3_tenså®½å±•äº†ä¸€ä¸ªç§©-1
valid_3_dist.shape, valid_3_dist #(torch.Size([1010])

def is_3(x): return mnist_distance(x, mean3) < mnist_distance(x, mean7) #ä¸æ˜¯3å°±æ˜¯7

accuracy_3s = is_3(valid_3_tens).float().mean()
```

#### 3.4 SGDéšæœºæ¢¯åº¦ä¸‹é™

![](img/dlcf_0401.png)

æ³¨æ„ç‰¹æ®Šæ–¹æ³•`requires_grad_`ï¼Ÿè¿™æ˜¯æˆ‘ä»¬å‘Šè¯‰ PyTorch æˆ‘ä»¬æƒ³è¦è®¡ç®—æ¢¯åº¦çš„ç¥å¥‡å’’è¯­ã€‚è¿™å®è´¨ä¸Šæ˜¯ç»™å˜é‡æ‰“ä¸Šæ ‡è®°ï¼Œè¿™æ · PyTorch å°±ä¼šè®°ä½å¦‚ä½•è®¡ç®—æ‚¨è¦æ±‚çš„å…¶ä»–ç›´æ¥è®¡ç®—çš„æ¢¯åº¦ã€‚

```python
xt = tensor(3.).requires_grad_() #è®©pytorchçŸ¥é“æˆ‘ä»¬åé¢ä¼šè¦æ±‚è®¡ç®—è¿™ä¸ªtensorçš„æ¢¯åº¦
yt = f(xt)
yt.backward() #backward,å…¶å®å°±æ˜¯calculates_grad
xt.grad #è®¡ç®—æ¢¯åº¦ï¼Œtensor(6.)
```

#### 3.5 å­¦ä¹ ç‡

```python
w -= w.grad * lr #lrå­¦ä¹ ç‡
```

#### 3.6 å®ä¾‹

```python
time = torch.arange(0,20).float()
speed = torch.randn(20)*3 + 0.75*(time-9.5)**2+1 #åœ¨yä¸­æ·»åŠ äº†å™ªå£°
plt.scatter(time,speed)

#å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°ï¼Œç”¨å®ƒæ¥æ‹Ÿåˆ(time,speed)
def f(t,params): 
    a,b,c = params
    return a*(t**2)+b*t+c

#è®¡ç®—loss
def mse(preds, targets): return ((preds-targets)**2).mean()

#åˆå§‹åŒ–éšæœºparameters
params = torch.randn(3).requires_grad_()

#å®šä¹‰å­¦ä¹ ç‡
lr = 1e-5

#å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ‹Ÿåˆ
def apply_step(params, prn=True):
    preds = f(time,params)
    loss = mse(preds, speed)
    loss.backward()
    params.data -= lr*params.grad.data #è¿™ä¸ªå¿…é¡»å¾—åŠ .dataæ–¹æ³•ï¼Œå¦åˆ™ä¼šæŠ¥é”™
    params.grad = None
    if prn: print(loss.item()) #loss.item()ä¸å†æ˜¯tensor
    return params

#å¾ªç¯
while loss>3:
    apply_step(params)
```

#### 3.7 MNIST codes

* sigmoidï¼šæˆ‘ä»¬é¢„æµ‹è¿™ä¸ªpredctionsæ€»æ˜¯åœ¨0~1ï¼Œä½†å®é™…ä¸Šå®ƒå¯èƒ½åœ¨è¿™ä¸ªèŒƒå›´ä¹‹å¤–ï¼Œå°±éœ€è¦é‡‡ç”¨ä¸€ç§æ–¹æ³•æŠŠå®ƒæ”¾è¿›æ¥

```python
import matplotlib.pyplot as plt
def plot_function(f, title=None, min=-1, max=1):
    x = torch.arange(min, max, 0.1)
    y = tensor([f(xi) for xi in x])
    plt.plot(x, y)
    if title:
        plt.title(title)
    plt.show()
plot_function(torch.sigmoid, title='sigmoid',min=-4, max=4)
```

* å°æ‰¹æ¬¡ï¼šä¸ºæ•´ä¸ªæ•°æ®é›†è®¡ç®—å°†éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚ä¸ºå•ä¸ªæ•°æ®é¡¹è®¡ç®—å°†ä¸ä¼šä½¿ç”¨å¤ªå¤šä¿¡æ¯ï¼Œå› æ­¤ä¼šå¯¼è‡´ä¸ç²¾ç¡®å’Œä¸ç¨³å®šçš„æ¢¯åº¦ã€‚æ‚¨å°†è´¹åŠ›æ›´æ–°æƒé‡ï¼Œä½†åªè€ƒè™‘è¿™å°†å¦‚ä½•æ”¹å–„æ¨¡å‹åœ¨è¯¥å•ä¸ªæ•°æ®é¡¹ä¸Šçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åšå‡ºå¦¥åï¼šæˆ‘ä»¬ä¸€æ¬¡è®¡ç®—å‡ ä¸ªæ•°æ®é¡¹çš„å¹³å‡æŸå¤±ã€‚è¿™è¢«ç§°ä¸º*å°æ‰¹æ¬¡* **mini-batch**ã€‚é€‰æ‹©ä¸€ä¸ªå¥½çš„æ‰¹æ¬¡å¤§å°æ˜¯æ‚¨ä½œä¸ºæ·±åº¦å­¦ä¹ ä»ä¸šè€…éœ€è¦åšå‡ºçš„å†³å®šä¹‹ä¸€ï¼Œä»¥ä¾¿å¿«é€Ÿå‡†ç¡®åœ°è®­ç»ƒæ‚¨çš„æ¨¡å‹ã€‚
* æ¯”å¦‚æœ‰2000ç»„æ•°æ®åˆ†ä¸º4*500çš„mini-batchï¼Œåœ¨ä¸€ä¸ªepochä¸­å°±ä¼š4æ¬¡æ›´æ–°parameters

```python
coll = range(15)
dl = DataLoader(coll, batch_size=5, shuffle=True)
list(dl)
'''[tensor([11,  4,  3,  0,  2]),
 tensor([ 8,  7, 13, 14,  5]),
 tensor([12,  9,  6, 10,  1])]'''
ds = L(enumerate(string.ascii_lowercase))
dl = DataLoader(ds, batch_size=6, shuffle=True)
list(dl)
'''[(tensor([20,  7, 12, 24, 22,  9]), ('u', 'h', 'm', 'y', 'w', 'j')),
 (tensor([ 1, 25, 17, 19,  3,  6]), ('b', 'z', 'r', 't', 'd', 'g')),
 (tensor([13,  0,  5, 10, 18,  2]), ('n', 'a', 'f', 'k', 's', 'c')),
 (tensor([ 8,  4, 21, 15, 23, 16]), ('i', 'e', 'v', 'p', 'x', 'q')),
 (tensor([14, 11]), ('o', 'l'))]'''
```

##### nn.Linearï¼šåšçš„äº‹æƒ…å¦‚ä¸‹

```python
#åˆå§‹åŒ–å‚æ•°
def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()
weights = init_params((28*28,1)) # torch.Size([784,1])
bias = init_params(1) # torch.Size(1)
#è®¡ç®—predsçš„å‡½æ•°
def linear1(xb): return xb@weights+bias

#å¯ä»¥ç®€åŒ–ä¸º
linear_model = nn.Linear(28*28,1)
w,b=linear_model.parameters()
'''ä¸Šé¢çš„linear1å°±å˜æˆäº†linear_model'''
```

##### SGDï¼šä¸‹é¢ä¸€å…±ä¸‰ç‰ˆï¼Œæ…¢æ…¢ç®€åŒ–åç”¨åˆ°SGD()ç±»

```python
'ç¬¬ä¸€ç‰ˆï¼šå…¨éƒ¨è‡ªå·±å®šä¹‰å‡½æ•°'
#å®šä¹‰è®¡ç®—æ¢¯åº¦çš„å‡½æ•°
def calc_grad(xb,yb,model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è®¡ç®—éªŒè¯é›†çš„å‡†ç¡®ç‡
def valid_epoch(model):
    accs = [batch_accuracy(model(xb),yb) for i in valid_dl]
    return round(torch.stack(accs).mean().item(),4)
#æ›´æ–°æ¢¯åº¦çš„å‡½æ•°-------------------------ä¸»è¦æ˜¯è¿™ä¸€æ­¥å˜æˆäº†ç¬¬äºŒç‰ˆçš„ç±»+æ›´æ–°æ¢¯åº¦çš„å‡½æ•°
def train_epoch(model, lr, params):
    for xb,yb in dl:
        calc_grad(xb,yb,model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()
#è·‘èµ·æ¥
lr = 1.
params = weights,bias
for i in range(20):
    train_epoch(linear1,lr,params)
    print(valid_epoch(linear1))
------------------------------------------------------------------------------------------------------------
'ç¬¬äºŒç‰ˆï¼šåˆ›å»ºäº†ä¸€ä¸ªç±»ï¼ŒæŠŠä¸€äº›å‡½æ•°æ•´åˆè¿›ç±»é‡Œé¢'
#å®šä¹‰è®¡ç®—æ¢¯åº¦çš„å‡½æ•°
def calc_grad(xb,yb,model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è®¡ç®—éªŒè¯é›†çš„å‡†ç¡®ç‡
def valid_epoch(model):
    accs = [batch_accuracy(model(xb),yb) for i in valid_dl]
    return round(torch.stack(accs).mean().item(),4)
#åˆ›å»ºä¸€ä¸ªä¼˜åŒ–å™¨çš„ç±»ï¼ŒæŠŠæ›´æ–°æ¢¯åº¦å’Œå½’é›¶æ¢¯åº¦ä½œä¸ºæ–¹æ³•æ”¾è¿›äº†ç±»ä¸­--------------------------è¿™ä¸€æ­¥å˜æˆäº†ç¬¬ä¸‰ç‰ˆçš„SGD()
class BasicOptim:
    def __init__(self,params,lr):
        self.params = list(params)
        self.lr = lr
    def step(self, *args, **kwargs):
        for p in self.params:
            p.data -= p.grad.data*self.lr
    def zero_grad(self, *args, **kwargs):
        for p in self.params:
            p.grad = None
lr=1.
opt = BasicOptim(linear_model.parameters(),lr)
#æ›´æ–°æ¢¯åº¦çš„å‡½æ•°-----------------------------
def train_epoch(model):
    for xb,yb in dl:
        cal_grad(xb,yb,model)
        opt.step()
        opt.zero_grad()
#è·‘èµ·æ¥
def train_model(model,epochs):
    for i in range(epochs):
        train_epoch(model)
        print(valid_epoch(model))
train_model(linear_model,20)
------------------------------------------------------------------------------------------------------------
'ç¬¬ä¸‰ç‰ˆï¼šç”¨äº†ç°æˆçš„SGDç±»'
#å®šä¹‰è®¡ç®—æ¢¯åº¦çš„å‡½æ•°
def calc_grad(xb,yb,model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è®¡ç®—éªŒè¯é›†çš„å‡†ç¡®ç‡
def valid_epoch(model):
    accs = [batch_accuracy(model(xb),yb) for i in valid_dl]
    return round(torch.stack(accs).mean().item(),4)
#å®ä¾‹åŒ–ä¸€ä¸ªç±»-------------------------------------------------------
lr=1.
opt=SGD(linear_model.parameters(),lr)
#æ›´æ–°æ¢¯åº¦çš„å‡½æ•°-----------------------------------------------------
def train_epoch(model):
    for xb,yb in dl:
        cal_grad(xb,yb,model)
        opt.step()
        opt.zero_grad()
#è·‘èµ·æ¥
def train_model(model,epochs):
    for i in range(epochs):
        train_epoch(model)
        print(valid_epoch(model))
train_model(linear_model,20)
```

##### Learn.fitï¼š

```python
'ç¬¬ä¸€ç‰ˆ'
#åˆå§‹åŒ–å‚æ•°&å®ä¾‹åŒ–è®¡ç®—predsçš„å‡½æ•°
linear_model = nn.Linear(28*28,1)
'è§ä¸Šnn.Linear'
#å®šä¹‰è®¡ç®—lossçš„å‡½æ•°
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
#ä¼˜åŒ–æ­¥éª¤optimization step
#å®šä¹‰è®¡ç®—æ¢¯åº¦çš„å‡½æ•°
def calc_grad(xb,yb,model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
#å®ä¾‹åŒ–ç±»+æ›´æ–°æ¢¯åº¦çš„å‡½æ•°-----------------------------------------------------
lr=1.
opt=SGD(linear_model.parameters(),lr)
#æ›´æ–°æ¢¯åº¦çš„å‡½æ•°
def train_epoch(model):
    for xb,yb in dl:
        cal_grad(xb,yb,model)
        opt.step()
        opt.zero_grad()
'è§ä¸ŠSGD'
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è®¡ç®—éªŒè¯é›†çš„å‡†ç¡®ç‡
def valid_epoch(model):
    accs = [batch_accuracy(model(xb),yb) for i in valid_dl]
    return round(torch.stack(accs).mean().item(),4)
#è·‘èµ·æ¥----------------------------------------------------------------------
def train_model(model,epochs):
    for i in range(epochs):
        train_epoch(model)
        print(valid_epoch(model))
train_model(linear_model,10)

------------------------------------------------------------------------------------------------------------
'ç®€åŒ–ç‰ˆï¼šä¸ç”¨train_modeläº†'
#åˆå§‹åŒ–å‚æ•°&å®ä¾‹åŒ–è®¡ç®—predsçš„å‡½æ•°
linear_model = nn.Linear(28*28,1)
'è§ä¸Šnn.Linear'
#å®šä¹‰è®¡ç®—lossçš„å‡½æ•°
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è·‘èµ·æ¥
learn = Learner(dls,nn.Linear(28*28,1),opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy)
lr=1.
learn.fit(10,lr=lr)
```

##### å®ä¾‹


```python
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1,28*28) #æŠŠstacked_threeså’Œstacked_sevensç»„åˆæˆ1ä¸ªtensorï¼Œå¹¶ä¸”æ¯å¼ å›¾ç‰‡çš„æ•°æ®28*28é¦–å°¾ç›¸è¿æˆä¸€ä¸ªrank-1çš„tensor
#train_x.shape # torch.Size([12396,784])

train_y = tensor([1]*len(threes)+[0]*len(sevens)).unsqueeze(1) #unsqueeze(1)æ˜¯å°†å®ƒæ‰©å±•ä¸ºrank-2ï¼Œå¦åˆ™sizeæ˜¯([12396])
#train_y.shape # torch.Size([12396,1])

dset = list(zip(train_x, train_y)) #list(zip())å¯ä»¥æŠŠä¸¤ä¸ªtensorç»„åˆæˆ1ä¸ªå…ƒç»„
#x,y = dset[0] #å°±å¯ä»¥è¿™æ ·ç´¢å¼•äº†

valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1,28*28)
valid_y = tensor([1]*len(valid_3_tens)+[0]*len(valid_7_tens)).unsqueeze(1)
valid_dset = list(zip(valid_x,valid_y))

#å‡†å¤‡å¥½dataloader
dl = DataLoader(dset, batch_size=256)
valid_dl = DataLoader(valid_dset, batch_size=256)
dls = DataLoaders(dl,valid_dl)

#è®¡ç®—å‡†ç¡®ç‡
#corrects = (preds>0.0).float() == train_y
#corrects.float().mean().item()
#å¦‚æœç”¨å‡†ç¡®ç‡åšlosså°±ä¼šå¯¼è‡´lossçš„å˜åŒ–ä¸æ˜æ˜¾ï¼ˆåªæœ‰ä»0è·³åˆ°1çš„æ—¶å€™æ‰å˜åŒ–ï¼‰ï¼Œè¿™æ ·ä¼šå¯¼è‡´backwardè®¡ç®—çš„gradå¾ˆå¤šæ—¶å€™ä¸º0ï¼Œæ— æ³•æ‹Ÿåˆ.å®ƒä¸èƒ½æœ‰å¤§çš„å¹³å¦éƒ¨åˆ†å’Œå¤§çš„è·³è·ƒï¼Œè€Œå¿…é¡»æ˜¯ç›¸å½“å¹³æ»‘çš„.

#å®šä¹‰è®¡ç®—lossçš„å‡½æ•°
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è·‘èµ·æ¥
learn = Learner(dls,nn.Linear(28*28,1),opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy)
lr=1.
learn.fit(10,lr=lr) 
```

##### **æ±‡æ€»codes**

```python
from fastcore.all import *
from fastai.vision.all import *
path = untar_data(URLs.MNIST_SAMPLE)
threes = (path/'train'/'3').ls().sorted()
sevens = (path/'train'/'7').ls().sorted()
seven_tensors = [tensor(Image.open(o)) for o in sevens]
three_tensors = [tensor(Image.open(o)) for o in threes]
stacked_sevens = torch.stack(seven_tensors).float()/255 #æŠŠè¿™ä¸ªtensorçš„Lç”¨torch.stackæ–¹æ³•å †å æˆä¸€ä¸ª3-rankå¼ é‡
stacked_threes = torch.stack(three_tensors).float()/255
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1,28*28) 
train_y = tensor([1]*len(threes)+[0]*len(sevens)).unsqueeze(1)
dset = list(zip(train_x, train_y))
valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]).float()/255
valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]).float()/255
valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1,28*28)
valid_y = tensor([1]*len(valid_3_tens)+[0]*len(valid_7_tens)).unsqueeze(1)
valid_dset = list(zip(valid_x,valid_y))
dl = DataLoader(dset, batch_size=256)
valid_dl = DataLoader(valid_dset, batch_size=256)
dls = DataLoaders(dl,valid_dl)
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
learn = Learner(dls,nn.Linear(28*28,1),opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy)
lr=1.
learn.fit(10,lr=lr) 
```

#### 3.8 æ·»åŠ éçº¿æ€§

*rectified linear unit*

##### Activation Function

* F.relu

![](img/dlcf_04in16.png)

* F.sigmoid

##### å®ä¾‹

```python
#å‰é¢å¦‚ä¸Šä¸€ä¸ªå®ä¾‹
#æ„å»ºä¸€ä¸ªå¤šå±‚ç¥ç»ç½‘ç»œï¼ˆ2å±‚=1ä¸ªéšè—å±‚+1ä¸ªå…¨è¿æ¥å±‚ï¼‰
simple_net = nn.Sequential(
    nn.Linear(28*28,30),
    nn.ReLU(),
    nn.Linear(30,1)
)
#å®šä¹‰è®¡ç®—lossçš„å‡½æ•°
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#å®ä¾‹åŒ–
learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)
#è®­ç»ƒ
learn.fit(30,0.1)
```

<img src="D:\Git\a\Path-Records\img\dlcf_0402.png" style="zoom:50%;" />

è®­ç»ƒè¿‡ç¨‹è®°å½•åœ¨`learn.recorder`ä¸­ï¼Œè¾“å‡ºè¡¨å­˜å‚¨åœ¨`values`å±æ€§ä¸­ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§

```python
plt.plot(L(learn.recorder.values).itemgot(2))
'''train_loss:L(learn.recorder.values).itemgot(0)
valid_loss:L(learn.recorder.values).itemgot(1)
batch_accuracy:L(learn.recorder.values).itemgot(2)'''
```

#### 3.9 æœ¯è¯­

| Term | Meaning|
|:---|---|
|ReLU | Function that returns 0 for negative numbers and doesn't change positive numbers.|
|Mini-batch | A small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch).|
|Forward pass | Applying the model to some input and computing the predictions.|
|Loss | A value that represents how well (or badly) our model is doing.|
|Gradient | The derivative of the loss with respect to some parameter of the model.|
|Backward pass | Computing the gradients of the loss with respect to all model parameters.|
|Gradient descent | Taking a step in the directions opposite to the gradients to make the model parameters a little bit better.|
|Learning rate | The size of the step we take when applying SGD to update the parameters of the model.|
|Activations | Numbers that are calculated (both by linear and nonlinear layers) |
|Parameters | Numbers that are randomly initialized, and optimized (that is, the numbers that define the model) |
|Special Tensors | Rank zero: scalar / Rank one: vector / Rank two: matrix |

### 4: Natural Language (NLP) (PDL2022)

#### 4.1 å‘å±•

ï¼ˆ1ï¼‰**ULMFit** (ç”¨çš„RNN): Wikitext(103)Language Model (30%å‡†ç¡®ç‡) â†’ IMDb Language Model â†’ IMDb Classifier

ï¼ˆ2ï¼‰**Transformers** (æ©ç è¯­è¨€å»ºæ¨¡): ç”¨äºæœºå™¨é˜…è¯»ç†è§£ã€å¥å­åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦ç­‰

ï¼ˆ3ï¼‰...

å¯ä»¥çœ‹åˆ°ä¼¼ä¹Transformerè¦æ¯”ULMFité«˜çº§ï¼Œå®é™…ä¸Šä¸¤è€…çš„ç”¨é€”ä¸åŒï¼›å¦å¤–ï¼ŒULMFitèƒ½å¤Ÿé˜…è¯»æ›´é•¿çš„å¥å­ï¼Œå¦‚æœä¸€ä¸ªdocumentåŒ…å«è¶…è¿‡2000ä¸ªå•è¯ï¼Œé‚£ä¹ˆå°±æ›´æ¨èä½¿ç”¨ULMFitè¿›è¡Œåˆ†ç±»ã€‚

#### 4.2 æœ€é‡è¦çš„package

1/ pandas

2/ numpy

3/ matplotlib

4/ pytorch

##### å‚è€ƒä¹¦ï¼š[Python for Data Analysis, 3E About the Open Edition]([Python for Data Analysis, 3E](https://wesmckinney.com/book/))

#### 4.3 Tokenization

A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:

- *Tokenization*: Split each text up into words (or actually, as we'll see, into *tokens*)
- *Numericalization*: Convert each word (or token) into a number.

The details about how this is done actually depend on the particular model we use. So first we'll need to pick a model. There are thousands of models available, but a reasonable starting point for nearly any NLP problem is to use thisï¼š

`microsoft/deberta-v3-small`

```python
tokz.tokenize("A platypus is an ornithorhynchus anatinus.")
'''
['â–A',
 'â–platypus',
 'â–is',
 'â–an',
 'â–or',
 'ni',
 'tho',
 'rhynch',
 'us',
 'â–an',
 'at',
 'inus',
 '.']
'''
```

#### 4.4 è®­ç»ƒé›†ä¸éªŒè¯é›†çš„åˆ’åˆ†

Training set & Validation set

In practice, a random split like we've used here might not be a good idea -- here's what Dr Rachel Thomas has to say about it:

> "*One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a `train_test_split` method, this method takes a random subset of the data, which is a poor choice for many real-world problems.*"

I strongly recommend reading her article [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/) to more fully understand this critical topic.

* éšæœºåˆ’åˆ†æ•°æ®é›†ä¸é€‚ç”¨çš„æƒ…å†µï¼šæ—¶é—´åºåˆ—ã€æ–°äººè„¸ã€æ–°çš„èˆ¹ç­‰ç­‰ï¼›
* cross-validationæ¯”è¾ƒå±é™©ï¼Œé™¤éç”¨åˆ°çš„caseæ˜¯é‚£ç§å¯ä»¥éšæœºæ´—ç‰Œçš„æƒ…å†µï¼ˆéšæœºåˆ†ABCä¸‰ç»„æ•°æ®é›†ï¼ŒABåˆå¹¶åšè®­ç»ƒé›†-CåšéªŒè¯é›†ï¼Œä¸‰ç»„å¾ªç¯ï¼Œæœ€åæ±‚å¹³å‡å€¼ä½œä¸ºæ¨¡å‹çš„performanceï¼‰ï¼›
* æ‰€ä»¥ç”¨ä¸€ä¸ªtest setæµ‹è¯•é›†å»æœ€ç»ˆç¡®è®¤ä¸€ä¸‹æ¨¡å‹çš„å¥½åä¹Ÿè›®é‡è¦çš„ã€‚

#### 4.5 Metrics

In real life, outside of Kaggle, things not easy... As my partner Dr Rachel Thomas notes in [The problem with metrics is a big problem for AI](https://www.fast.ai/2019/09/24/metrics/):

> At their heart, what most current AI approaches do is to optimize metrics. The practice of optimizing metrics is not new nor unique to AI, yet AI can be particularly efficient (even too efficient!) at doing so. This is important to understand, because any risks of optimizing metrics are heightened by AI. While metrics can be useful in their proper place, there are harms when they are unthinkingly applied. Some of the scariest instances of algorithms run amok all result from over-emphasizing metrics. We have to understand this dynamic in order to understand the urgent risks we are facing due to misuse of AI.

* Metricså¾ˆå¤šæ—¶å€™å¹¶ä¸æ˜¯*æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„äº‹æƒ…*ï¼Œå®ƒåªæ˜¯*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*çš„ä¸€ä¸ªä»£ç†ï¼Œå¦‚æˆ‘ä»¬å…³å¿ƒè€å¸ˆçš„æ•™å­¦æ•ˆæœï¼Œmetricsæ˜¯å­¦ç”Ÿçš„åˆ†æ•°ï¼›
* Metricsä¼šè¢«æ•…æ„åœ°ã€ä½œå¼Šåœ°æ‹‰é«˜ï¼Œä½¿å®ƒå¤±å»äº†è¡¡é‡*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*çš„èƒ½åŠ›ï¼Œå¦‚è€å¸ˆäººä¸ºæ‹‰é«˜å­¦ç”Ÿçš„åˆ†æ•°ï¼Œå®ƒä¸å†èƒ½åæ˜ è€å¸ˆçš„æ•™å­¦æ•ˆæœï¼›
* Metricsä¼šæ›´çŸ­è§†ï¼Œæ¯”å¦‚é“¶è¡Œä¸€æ—¦å°†cross-sellingè¿™ä¸ªmetricsä½œä¸ºç›®æ ‡ï¼Œå°±ä¼šå‚¬ç”Ÿå‡ºå„ç§è™šå‡å¼€æˆ·ï¼Œè€Œå®é™…ä¸Šé“¶è¡Œçš„ç›®æ ‡æ˜¯ç»´æŠ¤è‰¯å¥½çš„å®¢æˆ·å…³ç³»ï¼Œåè€…æ‰æ˜¯é•¿è¿œçš„æˆ˜ç•¥ï¼Œæ¯”å¦‚*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*æ˜¯æé«˜è§†é¢‘å½±å“åŠ›ï¼Œç”¨äº†ç‚¹å‡»ç‡ä½œä¸ºMetricsï¼Œå°±æ²¡æœ‰è€ƒè™‘åˆ°ä¸€äº›è§†é¢‘é•¿æœŸæ¥çœ‹å¯¹è¯»è€…çš„å¸®åŠ©å’Œå¡‘é€ ï¼›
* å¾ˆå¤šMetricsæ˜¯åœ¨ä¸€ä¸ªé«˜åº¦æˆç˜¾çš„ç¯å¢ƒæ”¶é›†æ•°æ®çš„ï¼Œæ¯”å¦‚æ•°æ®æ”¶é›†åˆ°å°æœ‹å‹å–œæ¬¢åƒç”œé£Ÿï¼Œç®—æ³•ä¼šè®©é£Ÿç‰©è¶Šæ¥è¶Šç”œï¼Œæ°¸è¿œä¸å¯èƒ½outputå‡ºæœ‰è¥å…»çš„é£Ÿç‰©ï¼›
* å°½ç®¡å¦‚æ­¤Metricsä¾ç„¶å¾ˆæœ‰ç”¨ï¼Œéœ€è¦è€ƒè™‘å¤šä¸ªmetricsæ¥é¿å…ä¸Šè¿°é—®é¢˜ï¼Œä½†æœ€ç»ˆæˆ‘ä»¬è¦åŠªåŠ›å°†å®ƒä»¬æ•´åˆï¼›
* Metricsé€šè¿‡å®šé‡æ–¹å¼è¡¡é‡ç»“æœï¼Œä½†æˆ‘ä»¬ä¾ç„¶éœ€è¦å®šæ€§çš„ä¿¡æ¯æ‰èƒ½è·å¾—å¥½çš„metricsï¼›
* å»è¯¢é—®å·²åœ¨æ­¤å±±ä¸­çš„äººæ°¸è¿œå¯ä»¥foreseeä¸€äº›ä¸è‰¯åæœï¼Œå¦‚è€å¸ˆå¯ä»¥å¾ˆå®¹æ˜“åœ°çŸ¥é“ï¼Œç”¨å­¦ç”Ÿåˆ†æ•°ä½œä¸ºå”¯ä¸€è¡¡é‡æ ‡å‡†ä¼šå¯¼è‡´ä»€ä¹ˆç³Ÿç³•çš„ç»“æœã€‚

#### 4.6 codes

```python
# æ£€æŸ¥æ˜¯å¦ä¸ºkaggleç¯å¢ƒ
import os
iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')
iskaggle

# ä¸‹è½½datasets
from pathlib import Path
if iskaggle:
    path = Path('../input/us-patent-phrase-to-phrase-matching')
    ! pip install -q datasets #åªéœ€è¦è·‘ä¸€æ¬¡å°±åˆ°äº†é¡µé¢ä¸­äº†
    
# ï¼è¡¨ç¤ºåé¢çš„ä¸æ˜¯pythonå‘½ä»¤ï¼Œæ˜¯shellå‘½ä»¤ï¼Œä½†æ˜¯å¦‚æœæƒ³åˆ©ç”¨åˆ°pythonä¸­çš„å‚æ•°ï¼Œå°±ç”¨{}æ¡†èµ·æ¥
!ls {path}
```

```python
#æ•°æ®çš„é¢„å¤„ç†å·¥ä½œ
# å®šä¹‰ä¸€ä¸ªè®­ç»ƒé›†çš„dataframe
import pandas as pd
df = pd.read_csv(path/'train.csv')
df.describe(include='object') #ä¸€ä¸ªéå¸¸é‡è¦çš„dataframeæ–¹æ³•

# æ„å»ºä¸€ä¸ªdf
df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor
df.input.head()
'''
0    TEXT1: A47; TEXT2: abatement of pollution; ANC...
1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...
2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...
3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...
4    TEXT1: A47; TEXT2: forest region; ANC1: abatement
Name: input, dtype: object
'''
df['input'][0], df.input[0]
''TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement''
            
# å®ä¾‹åŒ–ä¸€ä¸ªè®­ç»ƒé›†çš„Dataset
from datasets import Dataset,DatasetDict
ds = Dataset.from_pandas(df)
'''
Dataset({
    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],
    num_rows: 36473
})
å…¶ä¸­inputæ˜¯æ•´ä¸ªstringå¥å­
'''

# é€‰æ‹©ä¸€ä¸ªmodel,å°±æœ‰äº†å’Œè¿™ä¸ªmodelå¯¹åº”çš„vocabularyï¼Œç„¶åå®ä¾‹åŒ–å®ƒçš„tokzå·¥å…·
model_nm = 'microsoft/deberta-v3-small'
from transformers import AutoModelForSequenceClassification,AutoTokenizer
tokz = AutoTokenizer.from_pretrained(model_nm)

# å®šä¹‰ä¸€ä¸ªç”¨æ¥tokzæŸæ®µæ–‡å­—çš„å‡½æ•°ï¼Œå¹¶åº”ç”¨
def tok_func(x): return tokz(x["input"])
toknum_ds = ds.map(tok_func, batched=True) #æ¥è‡ªHuggingFaceçš„datasetsåº“,è¿™ä½¿å¾—å®ƒä¸å†åªæœ‰'input'è¿˜æœ‰'input_ids'
'''
.map(tok_func, batched=True)ï¼šå¯¹ ds ä¸­çš„æ•°æ®åº”ç”¨ tok_func è¿›è¡Œæ˜ å°„ï¼ˆmapï¼‰ï¼Œå¹¶ä¸”å¯ç”¨æ‰¹å¤„ç†ï¼ˆbatched=Trueï¼‰ï¼Œä»¥æé«˜æ•ˆç‡ã€‚
tok_dsï¼šè¿”å›ä¸€ä¸ªæ–°çš„ Datasetï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬å·²ç»è¢« tok_func å¤„ç†è¿‡ï¼ˆé€šå¸¸æ˜¯ Tokenizerï¼‰
'''
#row = toknum_ds[0]
#row['input'], row['input_ids'] 'â‘ ç¬¬ä¸€å¥è¯stringï¼Œâ‘¡ä¸€ä¸²æ•°å­—'
#tokz.vocab['â–of'] #è¿™é‡Œæœ‰ä¸€ä¸ªvocabularyï¼Œtockenâ†’æ•°å­—
#'265'

# å‡†å¤‡ä¸€ä¸ªlablesï¼Œtransformerä¸€å‘éƒ½è®¤ä¸ºlabelså°±è¯´æœ‰ä¸€åˆ—å«åšlabelsï¼Œæ‰€ä»¥å¾—æ”¹åå­—
toknum_ds = toknum_ds.rename_columns({'score':'labels'})

# æŠŠä¹‹å‰å·²ç»æ•°å­—åŒ–äº†çš„Datasetåˆ†ä¸€ä¸‹ï¼Œåˆ†æˆä¸€ä¸ªè®­ç»ƒé›†ä¸€ä¸ªéªŒè¯é›†
dds = toknum_ds.train_test_split(0.25, seed=42)

# å®šä¹‰Metrics
import numpy as np
def corr(x,y): return np.corrcoef(x,y)[0][1]
def corr_d(eval_pred): return {'pearson': corr(*eval_pred)} #eval_predé€šå¸¸æ˜¯åŒ…å«é¢„æµ‹å€¼å’Œå®é™…å€¼çš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œ*è¡¨ç¤ºå°†é¢„æµ‹å€¼å’Œå®é™…å€¼æ‹†è§£æˆä½ç½®å‚æ•°ä¼ é€’ç»™corr() [å¦å¤–ï¼Œè¿™ä¸ª'pearson'æ ‡ç­¾æœ€åä¼šå‡ºç°åœ¨è®­ç»ƒç»“æœé‡Œ]
```

```python
#è®­ç»ƒ
from transformers import TrainingArguments,Trainer
bs = 128
epochs = 4
lr = 8e-5
args = TrainingArguments( #Hugging Faceçš„Transformersåº“æ—¶ï¼Œä¸“é—¨ç”¨äºé›†ä¸­ç®¡ç†å’Œå®šä¹‰è®­ç»ƒä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶æ‰€éœ€è¦çš„æ‰€æœ‰è¶…å‚æ•°å’Œè®¾ç½®
    'outputs',                # è¾“å‡ºç›®å½•ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹å’Œæ—¥å¿—å°†ä¿å­˜åœ¨æ­¤ç›®å½•ä¸­
    learning_rate=lr,         # å­¦ä¹ ç‡ï¼šæ¨¡å‹ä¼˜åŒ–çš„æ­¥é•¿ï¼ˆlr æ˜¯äº‹å…ˆå®šä¹‰å¥½çš„å˜é‡ï¼‰
    warmup_ratio=0.1,         # é¢„çƒ­æ¯”ä¾‹ï¼šå­¦ä¹ ç‡ä» 0 çº¿æ€§å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡çš„æ¯”ä¾‹ï¼ˆ0.1 è¡¨ç¤º 10% çš„è®­ç»ƒæ­¥æ•°ä½œä¸ºé¢„çƒ­é˜¶æ®µï¼‰
    lr_scheduler_type='cosine',  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼šä½¿ç”¨ä½™å¼¦é€€ç«ï¼ˆCosine Annealingï¼‰ç­–ç•¥æ¥é€æ­¥é™ä½å­¦ä¹ ç‡
    fp16=True,                # æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦è®­ç»ƒï¼šå°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æµ®ç‚¹æ•°ç²¾åº¦é™ä¸º 16 ä½ï¼Œå¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜å ç”¨
    evaluation_strategy="epoch",  # è¯„ä¼°ç­–ç•¥ï¼šæ¯ä¸ªè®­ç»ƒè½®ï¼ˆepochï¼‰ç»“æŸåè¿›è¡Œè¯„ä¼°
    per_device_train_batch_size=bs,  # æ¯ä¸ªè®¾å¤‡ï¼ˆGPUï¼‰ä¸Šè®­ç»ƒçš„æ‰¹é‡å¤§å°ï¼ˆ`bs` æ˜¯äº‹å…ˆå®šä¹‰çš„å˜é‡ï¼‰
    per_device_eval_batch_size=bs*2,  # æ¯ä¸ªè®¾å¤‡ä¸Šè¯„ä¼°çš„æ‰¹é‡å¤§å°ï¼Œé€šå¸¸è¯„ä¼°æ—¶æ‰¹é‡å¯ä»¥ç¨å¤§ä¸€äº›
    num_train_epochs=epochs,  # è®­ç»ƒè½®æ•°ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¿›è¡Œçš„å®Œæ•´æ•°æ®é›†è¿­ä»£æ¬¡æ•°ï¼ˆ`epochs` æ˜¯äº‹å…ˆå®šä¹‰çš„å˜é‡ï¼‰
    weight_decay=0.01,        # æƒé‡è¡°å‡ï¼šL2 æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    report_to='none' # ç¦ç”¨æŠ¥å‘Šï¼ˆå¦‚ä¸æŠ¥å‘Šåˆ° TensorBoard æˆ– WandBï¼‰ï¼Œå¦‚æœéœ€è¦æŠ¥å‘Šï¼Œå¯ä»¥è®¾ç½®ä¸º 'tensorboard' æˆ– 'wandb'
)
model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) #å°†ç”¨çš„æ¨¡å‹
trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'], tokenizer=tokz,
                  compute_metrics=corr_d) #å°†modelã€è¶…å‚æ•°ä»¬ã€dataæ•´åˆåœ¨ä¸€èµ·çš„ç±»

trainer.train()
```

```python
#æµ‹è¯•é›†
# å®šä¹‰ä¸€ä¸ªæµ‹è¯•é›†çš„dataframeï¼Œæ„å»ºä¸€ä¸ªeval_dfï¼ŒåŸºäºè¿™å®ä¾‹åŒ–ä¸€ä¸ªeval_dså¹¶å°†å®ƒtokenization
eval_df = pd.read_csv(path/'test.csv')
eval_df.describe()
eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor
eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)

# é¢„æµ‹æµ‹è¯•é›†
preds = trainer.predict(eval_ds).predictions.astype(float)
preds = np.clip(preds, 0, 1) #å°†<0>1çš„æ•°å€¼è§„æ•´åˆ°0~1

#å°†ç»“æœå¯¼å‡ºåˆ°csv
import datasets
submission = datasets.Dataset.from_dict({
    'id': eval_ds['id'],
    'score': preds
})
submission.to_csv('submission.csv', index=False)
```

#### 4.7 è¶…å‚æ•°ï¼šæƒé‡è¡°å‡weight decay

L2 æ­£åˆ™åŒ–é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ªä¸æ¨¡å‹æƒé‡çš„å¹³æ–¹å’Œæˆæ­£æ¯”çš„é¡¹æ¥å®ç°æƒ©ç½šã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæŸå¤±å‡½æ•° `L(w)`ï¼Œè¡¨ç¤ºæ¨¡å‹çš„æŸå¤±ï¼Œå…¶ä¸­ `w` æ˜¯æ¨¡å‹çš„æƒé‡å‚æ•°ï¼Œé‚£ä¹ˆåŠ å…¥ L2 æ­£åˆ™åŒ–åçš„æŸå¤±å‡½æ•° `L2(w)` å°±æ˜¯ï¼š
$$
L2(w)=L(w) + \lambda \sum_{i} w_i^2
$$
å…¶ä¸­ï¼š

- `L(w)` æ˜¯åŸå§‹çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€å‡æ–¹è¯¯å·®ç­‰ï¼‰ã€‚
- `w_i` æ˜¯æ¨¡å‹çš„ç¬¬ `i` ä¸ªæƒé‡ã€‚
- `Î»` æ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„è¶…å‚æ•°ï¼Œæ§åˆ¶ L2 æ­£åˆ™åŒ–çš„å½±å“ã€‚è¾ƒå¤§çš„ `Î»` ä¼šå¯¹æ¨¡å‹çš„è®­ç»ƒäº§ç”Ÿè¾ƒå¤§çš„å½±å“ã€‚

**ä½œç”¨**

- **é™åˆ¶æƒé‡çš„å¤§å°**ï¼šL2 æ­£åˆ™åŒ–é¼“åŠ±æ¨¡å‹çš„æƒé‡ `w` å°½å¯èƒ½å°ï¼Œé¿å…å‡ºç°è¿‡å¤§çš„æƒé‡å€¼ï¼Œè¿™æ ·å¯ä»¥å‡å°‘æ¨¡å‹çš„å¤æ‚åº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- **å¹³æ»‘æ¨¡å‹**ï¼šé€šè¿‡æŠ‘åˆ¶å¤§æƒé‡ï¼ŒL2 æ­£åˆ™åŒ–ä¿ƒä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´åŠ å¹³æ»‘çš„å‡½æ•°ï¼Œè€Œéè¿‡äºå¤æ‚çš„ã€è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„å‡½æ•°ã€‚
- **æ”¹è¿›æ³›åŒ–èƒ½åŠ›**ï¼šé€šè¿‡é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒL2 æ­£åˆ™åŒ–ä½¿å¾—æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ï¼ˆæµ‹è¯•é›†ï¼‰ä¸Šçš„è¡¨ç°æ›´åŠ ç¨³å®šã€‚

é€šä¿—è¯´æ˜ï¼Œå› ä¸ºåœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥äº†weightsçš„å¹³æ–¹å’Œï¼Œä¸ºäº†è®©æŸå¤±å‡½æ•°å˜å°ï¼Œæ¨¡å‹ä¼šå€¾å‘äºå‡å°weightsçš„æ•°å€¼ï¼Œå¯æ˜¯å½“æ•°å€¼ä¸º0çš„æ—¶å€™ï¼Œè¿™ä¸ªæ¨¡å‹çš„é¢„æµ‹åˆä¼šå¾ˆå·®ï¼Œæ‰€ä»¥åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å°±ä¼šæ‰¾åˆ°ä¸€ä¸ªå¾®å¦™çš„å¹³è¡¡ï¼šä¸€äº›å¯¹é¢„æµ‹ä¸å¤ªæœ‰ç”¨çš„ç‰¹å¾å¯¹åº”çš„weightsä¼šè¢«å‹ç¼©åˆ°å¾ˆå°ï¼Œè®©å¯¹é¢„æµ‹æœ‰ç”¨çš„ç‰¹å¾å¯¹åº”çš„weightsè·å¾—è¶³å¤Ÿçš„ç©ºé—´ã€‚

### 4: nlp (fastbook10)

#### 4.1 è‡ªç›‘ç£å­¦ä¹ 

ä½¿ç”¨åµŒå…¥åœ¨è‡ªå˜é‡ä¸­çš„æ ‡ç­¾æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸æ˜¯éœ€è¦å¤–éƒ¨æ ‡ç­¾ã€‚ä¾‹å¦‚ï¼Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚è‡ªç›‘ç£å­¦ä¹ ä¹Ÿå¯ä»¥ç”¨äºå…¶ä»–é¢†åŸŸï¼›ä¾‹å¦‚ï¼Œå‚è§[â€œè‡ªç›‘ç£å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰â€](https://oreil.ly/ECjfJ)ä»¥äº†è§£è§†è§‰åº”ç”¨ã€‚

> åªè¦æœ‰å¯èƒ½ï¼Œå°½å¯èƒ½ä½¿ç”¨ä¸€ä¸ªå·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹ã€‚å³ä¾¿æ˜¯è¯¥æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸæ²¡æœ‰ç»è¿‡è®­ç»ƒï¼Œåªæ˜¯é€šç”¨èŒƒå›´å†…è®­ç»ƒäº†ï¼Œç”¨å®ƒçš„æ—©æœŸå‡ ä¸ªå±‚ä¾ç„¶å¯ä»¥æé«˜æ–°æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œæ•ˆæœã€‚
>
> è‡ªç›‘ç£å­¦ä¹ ï¼šmodelç”¨çš„labelsæ¥è‡ªäºinputsã€‚
>
> Pretrained modelåœ¨é¢„è®­ç»ƒæ—¶çš„é‚£ä¸ªä»»åŠ¡å«åšpretext taskï¼Œè€Œæˆ‘ä»¬è¦å°†å®ƒç”¨åœ¨ç‰¹å®šé¢†åŸŸçš„é‚£ä¸ªä»»åŠ¡å«åšdownstream tasksã€‚
>
> Autoencoderï¼šå°†ä¸€å¼ å›¾å‹ç¼©ï¼Œä¹‹åå†å°†å®ƒå°½å¯èƒ½åœ°è¿˜åŸæˆåŸå›¾ã€‚ä½†å¦‚æœä½ çš„downstream taskæ˜¯ç”Ÿæˆä¸€å¼ æ¯”åŸå›¾æ›´é«˜æ¸…çš„å›¾ç‰‡ï¼Œè¿™ä¸ªæ¨¡å‹å°±ä¸é€‚åˆåšä½ çš„pretrained modelã€‚å¯è§ï¼Œpretext taskå’Œdownstream taskè¦å¥½å¥½åœ°å¯¹åº”ä½œç”¨ã€‚
>
> åˆ«èŠ±å¤ªå¤šæ—¶é—´åœ¨åˆ›å»ºpretrained modelä¸Šï¼Œåªè¦å®ƒåˆç†åœ°å¿«å’Œç®€å•å°±è¡Œã€‚Note also that you can do multiple rounds of self-supervised pretraining and regular pretraining. For instance, you could use one of the above approaches for initial pretraining, and then do segmentation for additional pretraining, and then finally train your downstream task. You could also do multiple tasks at once (multi-task learning) at either or both stages. But of course, do the simplest thing first, and then add complexity only if you determine you really need it!
>
> **Consistency Loss** 
>
> ä¸¾ä¾‹ï¼šæˆ‘ä»¬æœ¬æ¥éœ€è¦10ä¸‡æ¡æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œç°åœ¨ç”¨1ä¸‡æ¡æ•°æ®ï¼Œå¹¶å¯¹è¿™ä¸ªæ•°æ®è¿›è¡Œå¤„ç†ï¼ˆç¿»è½¬ã€æ—‹è½¬ã€è£å‰ªç­‰ã€åŒä¹‰è¯æ›¿æ¢ã€å›è¯‘ç­‰ï¼‰åšæ•°æ®å¢å¼ºï¼Œç„¶åç”¨è¿™1ä¸‡æ¡+å¢å¼ºæ•°æ®è¿›è¡Œè®­ç»ƒã€‚åœ¨è®­ç»ƒä¸­ï¼Œé™¤äº†æ­£å¸¸è®­ç»ƒï¼Œè¿˜ä¼šå»çœ‹æºæ•°æ®å’Œå¢å¼ºæ•°æ®çš„é¢„æµ‹ç»“æœæ˜¯ä¸æ˜¯ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬ä¸€æ ·ï¼Œé‡åŒ–å®ƒå°±å¼•å…¥Consistency Loss
>
> ![](D:\Git\a\Path-Records\img\04-1-1.jpg)

è‡ªç›‘ç£å­¦ä¹ é€šå¸¸ä¸ç”¨äºç›´æ¥è®­ç»ƒçš„æ¨¡å‹ï¼Œè€Œæ˜¯ç”¨äºé¢„è®­ç»ƒç”¨äºè¿ç§»å­¦ä¹ çš„æ¨¡å‹ã€‚

* é€šç”¨è¯­è¨€æ¨¡å‹å¾®è°ƒï¼ˆULMFiTï¼‰æ–¹æ³•ï¼šæœ‰ä¸€ä¸ªåŸºäºç»´åŸºç™¾ç§‘è¯­æ–™åº“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨IMDbçš„è¯­æ–™åº“è¿›è¡Œå¾®è°ƒï¼Œå†è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»

![](D:\Git\a\Path-Records\img\dlcf_1001.png)

* åˆ†è¯æ–¹æ³•ï¼šåŸºäºå•è¯çš„ã€åŸºäºå­è¯çš„å’ŒåŸºäºå­—ç¬¦çš„

#### 4.2 Tokenization & Numericalization

ç”±åˆ†è¯è¿‡ç¨‹åˆ›å»ºçš„åˆ—è¡¨çš„ä¸€ä¸ªå…ƒç´ ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªå•è¯*word tokenization*ï¼Œä¸€ä¸ªå•è¯çš„ä¸€éƒ¨åˆ†ï¼ˆä¸€ä¸ª*å­è¯*ï¼‰*subword tokenization*ï¼Œæˆ–ä¸€ä¸ªå•ä¸ªå­—ç¬¦ã€‚

##### 4.2.1 Word Tokenization

```python
from fastai.text.all import *
path = untar_data(URLs.IMDB)
path.ls()
# è·å–pathä¸­æŒ‡å®šfoldersä¸­çš„files
files = get_text_files(path, folders=['train', 'test', 'unsup'])
txt = files[0].open().read()
txt[:100]
```

```python
# WordTokenizeræ˜¯ä¸€ä¸ªåˆ†è¯å™¨ç±»ï¼Œå®ä¾‹åŒ–WordTokenizerç±»
spacy = WordTokenizer()
# first()ä½œç”¨æ˜¯è¿”å›åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚è¿™é‡Œå®ƒå–çš„æ˜¯spacy([txt])å¤„ç†åè¿”å›çš„ç¬¬ä¸€ä¸ªDocå¯¹è±¡çš„tokenåˆ—è¡¨ã€‚åˆ†è¯å™¨æ¥å—æ–‡æ¡£é›†åˆï¼Œæ‰€ä»¥ç”¨[txt]
toks = first(spacy([txt]))
# æ˜¾ç¤º`collection`çš„å‰`n`ä¸ªé¡¹ç›®ï¼Œä»¥åŠå®Œæ•´çš„å¤§å°â€”â€”è¿™æ˜¯`L`é»˜è®¤ä½¿ç”¨çš„
print(coll_repr(toks,30))
```

```python
# é€šè¿‡Tokenizerç±»å¢åŠ é¢å¤–çš„åŠŸèƒ½
tkn = Tokenizer(spacy)
print(coll_repr(tkn(txt),30))
'''
(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]
'''
```

ğŸ‘†ä¸€äº›ç‰¹æ®Šæ ‡è®°ï¼š

| ä¸»è¦ç‰¹æ®Šæ ‡è®° | ä»£è¡¨                                                         |
| ------------ | ------------------------------------------------------------ |
| xxbos        | æŒ‡ç¤ºæ–‡æœ¬çš„å¼€å§‹ï¼Œæ„æ€æ˜¯â€œæµçš„å¼€å§‹â€ï¼‰ã€‚é€šè¿‡è¯†åˆ«è¿™ä¸ªå¼€å§‹æ ‡è®°ï¼Œæ¨¡å‹å°†èƒ½å¤Ÿå­¦ä¹ éœ€è¦â€œå¿˜è®°â€å…ˆå‰è¯´è¿‡çš„å†…å®¹ï¼Œä¸“æ³¨äºå³å°†å‡ºç°çš„å•è¯ã€‚ |
| xxmaj        | æŒ‡ç¤ºä¸‹ä¸€ä¸ªå•è¯ä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼ˆå› ä¸ºå‡å°vocabularyçš„ä½“é‡ï¼ŒèŠ‚çœè®¡ç®—å’Œå†…å­˜èµ„æºï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å­—æ¯è½¬æ¢ä¸ºå°å†™ï¼‰ |
| xxunk        | æŒ‡ç¤ºä¸‹ä¸€ä¸ªå•è¯æ˜¯æœªçŸ¥çš„                                       |

##### 4.2.2 Subword Tokenization

```python
# è¯»å–200ä¸ªfilesä¸­çš„å¥å­txts
txts = L(o.open().read() for o in files[:200])
# å°†å¥å­txtsæ‹†åˆ†æˆszä¸ªvocabularyï¼Œå¹¶tokenize txt
def subword(sz):
    sp = SubwordTokenizer(vocab_sz=sz)
    sp.setup(txts)
    return ' '.join(first(sp([txt]))[:40])
# åˆ†æˆ1000ä¸ªvocabulary
subword(1000)
'â– J ian g â– X ian â–us es â–the â–comp le x â–back st or y â–of â–L ing â–L ing â–and â–Ma o â–D a o b ing â–to â–st ud y â–Ma o \' s â–" c'
# åˆ†æˆ100ä¸ªvocabularyï¼Œä¸ºäº†èƒ½æ‹†åˆ†ï¼Œå¥½å¤šéƒ½æ‹†æˆå­—æ¯äº†
subword(100)
'â– J i a n g â– X i a n â– u s e s â–the â– c o m p l e x â– b a c k s t o r y â– o f â– L'
```

##### 4.2.3 Numericalization

```python
# å°†txtså‰200æ¡textéƒ½Word Tokenizationï¼ˆä¹Ÿå¯ä»¥subword tokenizationï¼Œæœ¬ä¾‹ç”¨äº†å‰è€…ï¼‰
toks200 = txts[:200].map(tkn)
toks200[0]
'''
è·å¾—äº†ä¸€ä¸ªåˆ†è¯åçš„åˆ—è¡¨ï¼Œåˆ—è¡¨é•¿åº¦ä¸º200
(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to'...]
'''

# ç±»æ¯”ä¸Šé¢çš„subword(),å› ä¸ºè¦æ‰‹åŠ¨å»ºç«‹ä¸ªvocabularyï¼Œè¿™ä¸ªå®ä¾‹åŒ–åä¹Ÿè¦setupä¸€ä¸‹
num = Numericalize()
num.setup(toks200) #åŸºäºåˆ†è¯åçš„ç»“æœè®¾ç½®æ•°å­—æ˜ å°„
nums = num(toks)[:20]
'''
TensorText([   0,    0, 1269,    9, 1270,    0,   14,    0,    0,   12,    0,
               0,   15, 1271,    0,   22,   24,    0,  795,   24])
'''
# å°†æ•°å­—åŒ–çš„å¥å­å†æ˜ å°„å›tokens
' '.join(num.vocab[o] for o in nums)
'''
'xxunk xxunk uses the complex xxunk of xxunk xxunk and xxunk xxunk to study xxunk \'s " xxunk revolution "'
'''
```

`Numericalize`çš„é»˜è®¤å€¼ä¸º`min_freq=3`å’Œ`max_vocab=60000`ã€‚`max_vocab=60000`å¯¼è‡´ fastai ç”¨ç‰¹æ®Šçš„*æœªçŸ¥å•è¯*æ ‡è®°`xxunk`æ›¿æ¢é™¤æœ€å¸¸è§çš„ 60,000 ä¸ªå•è¯ä¹‹å¤–çš„æ‰€æœ‰å•è¯ã€‚è¿™æœ‰åŠ©äºé¿å…è¿‡å¤§çš„åµŒå…¥çŸ©é˜µï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦å¹¶å ç”¨å¤ªå¤šå†…å­˜ï¼Œå¹¶ä¸”è¿˜å¯èƒ½æ„å‘³ç€æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥è®­ç»ƒç¨€æœ‰å•è¯çš„æœ‰ç”¨è¡¨ç¤ºã€‚ç„¶è€Œï¼Œé€šè¿‡è®¾ç½®`min_freq`æ¥å¤„ç†æœ€åä¸€ä¸ªé—®é¢˜æ›´å¥½ï¼›é»˜è®¤å€¼`min_freq=3`æ„å‘³ç€å‡ºç°å°‘äºä¸‰æ¬¡çš„ä»»ä½•å•è¯éƒ½å°†è¢«æ›¿æ¢ä¸º`xxunk`ã€‚

##### 4.2.4 å°†è¿™äº›txtæ”¾è¿›batchesé‡Œé¢ï¼Œå½¢æˆDataLoader

<img src="D:\Git\a\Path-Records\img\04-2-4.jpg" style="zoom:100%;" />

```python
# toks200æ˜¯200æ¡txtåˆ†è¯åçš„ï¼Œnum200å°±æ˜¯200æ¡åˆ†è¯å¥å­æ˜ å°„æˆæ•°å­—çš„
num200 = toks200.map(num)
# DataLoader
dl = LMDataLoader(num200)
x,y = first(dl)
x.shape, y.shape
'(torch.Size([64, 72]), torch.Size([64, 72]))ï¼Œå¯è§DataLoaderå°†streamæ‹†æˆäº†64ä¸ªmini-streamï¼Œæ¯ä¸ªmini-streamæœ‰72ä¸ªtokens'
# xå’Œyåªæ˜¯ç›¸å·®ä¸€ä¸ªtoken
' '.join(num.vocab[o] for o in x[0][:15])
'xxbos xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and'
' '.join(num.vocab[o] for o in y[0][:15])
'xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj'
```

#### 4.3 è®­ç»ƒæ–‡æœ¬åˆ†ç±»å™¨

* ä½¿ç”¨è¿ç§»å­¦ä¹ è®­ç»ƒæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ†ç±»å™¨æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¾®è°ƒåœ¨ Wikipedia ä¸Šé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä»¥é€‚åº” IMDb è¯„è®ºçš„è¯­æ–™åº“ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹æ¥è®­ç»ƒåˆ†ç±»å™¨ã€‚

##### 4.3.1 è¯­è¨€è¯†åˆ«-æ•°æ®åŠ è½½å™¨DataBlock

* **å®ä¾‹æ–¹æ³•**ï¼Œéœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç„¶åæ‰èƒ½è°ƒç”¨çš„æ–¹æ³•ï¼ŒMyClass.instance_method()ä¼šæŠ¥é”™ï¼›**ç±»æ–¹æ³•**å°±ä¸éœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç›´æ¥è°ƒç”¨MyClass.class_method()ä¸ä¼šæŠ¥é”™ï¼Œè€Œä¸”å¯ä»¥è®¿é—®ç±»å˜é‡ï¼›**é™æ€æ–¹æ³•**ä¹Ÿä¸éœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç›´æ¥è°ƒç”¨MyClass.static_method()ä¹Ÿä¸ä¼šæŠ¥é”™ï¼Œä½†æ²¡åŠæ³•è®¿é—®ç±»å˜é‡ã€‚

```python
from fastai.text.all import *
path = untar_data(URLs.IMDB)
# è¿™æ˜¯ä¸Šé¢å…¨éƒ¨æ‰‹åŠ¨ä»£ç çš„æ±‡æ€»--------------------------------------------------------------------------------
files = get_text_files(path, folders=['train', 'test', 'unsup'])
#txt = files[0].open().read()
spacy = WordTokenizer()
#toks = first(spacy([txt]))
tkn = Tokenizer(spacy)
# è¯»å–200ä¸ªfilesä¸­çš„å¥å­txts
txts = L(o.open().read() for o in files[:200])
# å°†å¥å­txtsæ‹†åˆ†æˆszä¸ªvocabularyï¼Œå¹¶tokenize txt
#def subword(sz):
#    sp = SubwordTokenizer(vocab_sz=sz)
#    sp.setup(txts)
#    return ' '.join(first(sp([txt]))[:40])
#tks = tkn(txt)
toks = txts.map(tkn)
num = Numericalize()
num.setup(toks)
#nums = num(toks)[:20]
num = toks.map(num)
dl = LMDataLoader(num) #æ²¡æ³•æŒ‡å®šbatch_size
x,y = first(dl)
# fastaiæœ‰ç°æˆçš„æ–¹æ³•---------------------------------------------------------------------------------------
get_imdb = partial(get_text_files, folders=['train','test','unsup'])
# è¯­è¨€æ¨¡å‹çš„æ•°æ®åŠ è½½å™¨
dls_lm = DataBlock(
    blocks = TextBlock.from_folder(path,is_lm=True),
    get_items = get_imdb,
    splitter = RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
dls_lm.show_batch(max_n=3)
```

|      | text                                                         | text_                                                        |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 0    | xxbos xxmaj it â€™s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard | xxmaj it â€™s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard xxunk |
| 1    | what xxmaj i â€˜ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this | xxmaj i â€˜ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this is |

##### 4.3.2 è¯­è¨€è¯†åˆ«-Fine-tune

* **Embedding**ï¼šåµŒå…¥æ˜¯æŠŠæ–‡å­—è½¬æ¢æˆè®¡ç®—æœºèƒ½ç†è§£çš„æ•°å­—ï¼Œè€Œä¸”è¿™ç§è½¬æ¢ä¸æ˜¯ç®€å•çš„ 1 å¯¹ 1 æ˜ å°„ï¼Œè€Œæ˜¯è®©è¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨æ•°å€¼ç©ºé—´é‡Œä¹Ÿé å¾—æ›´è¿‘ã€‚å¸¸è§çš„ NLP ä»»åŠ¡éƒ½ä¼šç”¨åˆ° Embeddingï¼Œæ¯”å¦‚ï¼š**Word2Vec**ï¼ˆGoogle å¼€å‘çš„è¯å‘é‡æ¨¡å‹ï¼‰ã€**GloVe**ï¼ˆæ–¯å¦ç¦å¼€å‘çš„è¯å‘é‡ï¼‰ã€**FastText**ï¼ˆFacebook å¼€å‘çš„è¯å‘é‡ï¼‰ã€**BERT / GPT**ï¼ˆç°ä»£ NLP æ¨¡å‹çš„åº•å±‚éƒ½ä¼šç”¨æ›´é«˜çº§çš„ Embeddingï¼‰ã€‚

```python
learn = language_model_learner(
    dls_lm,
    AWD_LSTM,
    drop_mult=0.3,      # Dropout ä¹˜æ•°ï¼ˆæ§åˆ¶æ­£åˆ™åŒ–çš„ç¨‹åº¦ï¼‰
    metrics=[accuracy, Perplexity()]     # è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ + å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
).to_fp16()      # å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦ï¼ˆFP16ï¼‰ï¼Œæå‡è®­ç»ƒé€Ÿåº¦
```

* **æŸå¤±å‡½æ•°**ï¼šäº¤å‰ç†µæŸå¤±
* **Perplexity** metricså¸¸ç”¨åœ¨NLPä¸­ï¼Œå®ƒæ˜¯æŸå¤±å‡½æ•°çš„æŒ‡æ•°ï¼ˆå³torch.exp(cross_entropy)ï¼‰

* **Dropout**æ˜¯ä¸€ç§é˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„æ–¹æ³•ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºâ€œä¸¢å¼ƒâ€ï¼ˆè®¾ä¸º 0ï¼‰ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦ä¾èµ–æŸäº›ç‰¹å®šçš„ç‰¹å¾ã€‚å®ƒç±»ä¼¼äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®å¼ºåŒ–ã€‚

###### â‘ **Dropout vs. Weight Decayï¼šåŒºåˆ«å¯¹æ¯”**

| ç‰¹æ€§        | Dropout                                              | Weight Decay (L2 æ­£åˆ™åŒ–)             |
| ---------------- | -------------------------------------------------------- | ---------------------------------------- |
| ä½œç”¨æ–¹å¼       | éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºï¼Œè®©ç½‘ç»œå­¦ä¼šä¸åŒçš„ç‰¹å¾          | é™åˆ¶æƒé‡å¤§å°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ             |
| é€‚ç”¨èŒƒå›´    | æ›´é€‚ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå°¤å…¶æ˜¯ CNNã€RNNã€Transformerï¼‰ | é€‚ç”¨äºå‡ ä¹æ‰€æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹           |
| è®­ç»ƒä¸æµ‹è¯•   | åªåœ¨è®­ç»ƒæ—¶ç”Ÿæ•ˆï¼Œæµ‹è¯•æ—¶å…³é—­                           | è®­ç»ƒå’Œæµ‹è¯•æ—¶éƒ½ç”Ÿæ•ˆ                   |
| æ•°å­¦å…¬å¼     | è®©éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºè®¾ä¸º 0                                 | ç»™æŸå¤±å‡½æ•°å¢åŠ  Î»âˆ‘w^2 æƒ©ç½šé¡¹              |
| ç›´è§‚ç†è§£     | è®©ç¥ç»ç½‘ç»œå˜æˆä¸€ä¸ªå°å‹é›†æˆå­¦ä¹                        | å‡å°‘å¤§æƒé‡ï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ•°æ® |
| å¯¹è®¡ç®—çš„å½±å“ | å¢åŠ è®¡ç®—é‡ï¼Œå› ä¸ºæ¯æ¬¡è®­ç»ƒéƒ½è¦éšæœºä¸¢å¼ƒä¸åŒç¥ç»å…ƒ       | ä¸ä¼šå¢åŠ è®¡ç®—é‡                       |

###### â‘¡**ä»€ä¹ˆæ—¶å€™ç”¨ Dropoutï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨ Weight Decayï¼Ÿ**

è™½ç„¶å®ƒä»¬çš„å®ç°æ–¹å¼ä¸åŒï¼Œä½†ç›®çš„éƒ½æ˜¯ é˜²æ­¢æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

âœ” å¯ä»¥ä¸€èµ·ä½¿ç”¨ï¼

- Dropout ä¸»è¦ä½œç”¨åœ¨ ç½‘ç»œç»“æ„ å±‚é¢ï¼ˆä¸¢å¼ƒç¥ç»å…ƒï¼‰
- Weight Decay ä¸»è¦ä½œç”¨åœ¨ å‚æ•°ä¼˜åŒ– å±‚é¢ï¼ˆçº¦æŸæƒé‡å¤§å°ï¼‰
- ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸ ä¸¤è€…éƒ½ç”¨

| æƒ…å†µ                                   | æ›´é€‚åˆ Dropout    | æ›´é€‚åˆ Weight Decay |
| ------------------------------------------ | --------------------- | ----------------------- |
| æ•°æ®é‡å°ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ                   | âœ…                     | âœ…                       |
| ç¥ç»ç½‘ç»œå¾ˆæ·±ï¼ˆCNNã€LSTMã€Transformerï¼‰ | âœ…                     | âœ…                       |
| å‚æ•°é‡å°‘ï¼ˆå°å‹æ¨¡å‹ï¼Œå¦‚çº¿æ€§å›å½’ï¼‰       | âŒ                     | âœ…                       |
| è¿‡æ‹Ÿåˆä¸¥é‡                             | âœ…ï¼ˆæé«˜ Dropout ç‡ï¼‰  | âœ…ï¼ˆå¢å¤§ Weight Decayï¼‰  |
| æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆRNNï¼‰                    | âœ…ï¼ˆDropout ä¹Ÿèƒ½å¸®å¿™ï¼‰ | âŒ                       |

ğŸ’¡ ç»éªŒæ³•åˆ™ï¼š

- å¤§æ¨¡å‹ï¼ˆCNN/RNNï¼‰ â†’ ä¸¤è€…éƒ½ç”¨ï¼Œdropout=0.3~0.5 + wd=1e-4
- å°æ¨¡å‹ï¼ˆçº¿æ€§å›å½’ï¼‰ â†’ ä¸»è¦ç”¨ Weight Decay
- æ•°æ®é‡ç‰¹åˆ«å° â†’ Dropout å¯ä»¥å°‘ç”¨ï¼Œä½† Weight Decay ä»ç„¶æœ‰æ•ˆ

**è®­ç»ƒ**ï¼š

åƒ`vision_learner`ä¸€æ ·ï¼Œå½“ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆè¿™æ˜¯é»˜è®¤è®¾ç½®ï¼‰æ—¶ï¼Œ`language_model_learner`åœ¨ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨`freeze`ã€‚å› æ­¤è¿™å°†ä»…è®­ç»ƒåµŒå…¥å±‚ï¼Œå…¶ä»–éƒ¨åˆ†çš„æƒé‡æ˜¯è¢«å†»ç»“çš„ã€‚ä¹‹æ‰€ä»¥åªè®­ç»ƒåµŒå…¥å±‚ï¼Œæ˜¯å› ä¸ºåœ¨ IMDb è¯­æ–™åº“ä¸­ï¼Œå¯èƒ½ä¼šæœ‰ä¸€äº›è¯æ±‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯è¡¨ä¸­æ‰¾ä¸åˆ°ï¼Œè¿™äº›è¯çš„åµŒå…¥ï¼ˆembeddingsï¼‰éœ€è¦éšæœºåˆå§‹åŒ–ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œè€Œé¢„è®­ç»ƒæ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†å·²ç»æœ‰äº†è¾ƒå¥½çš„å‚æ•°ï¼Œå› æ­¤æš‚æ—¶ä¸ä¼šè¢«è°ƒæ•´ã€‚

fine_tuneä¸ä¼šä¿å­˜åŠæˆå“æ¨¡å‹ç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨äº†fit_one_cycle

```python
learn.fit_one_cycle(1, 2e-2)
```

###### â‘¢**fit vs. fit_one_cycle å¯¹æ¯”**

| å¯¹æ¯”é¡¹     | fit                      | fit_one_cycle               |
| ---------- | ------------------------ | --------------------------- |
| å­¦ä¹ ç‡è°ƒåº¦ | å›ºå®šå­¦ä¹ ç‡               | åŠ¨æ€è°ƒæ•´ï¼ˆwarm-up + decayï¼‰ |
| åŠ¨é‡è°ƒåº¦   | ä¸å˜                     | è‡ªé€‚åº”è°ƒæ•´                  |
| é€‚ç”¨åœºæ™¯   | å°è§„æ¨¡è®­ç»ƒã€ç®€å•ä»»åŠ¡     | æ·±åº¦å­¦ä¹ ã€å¤§è§„æ¨¡è®­ç»ƒ        |
| ä¼˜ç‚¹       | ç®€å•ã€ç¨³å®š               | æé«˜æ³›åŒ–èƒ½åŠ›ã€æ”¶æ•›æ›´å¿«      |
| ç¼ºç‚¹       | å¯èƒ½è®­ç»ƒæ…¢ï¼Œæ³›åŒ–èƒ½åŠ›ä¸ä½³ | éœ€è¦è°ƒæ•´å‚æ•°ï¼Œç¨å¤æ‚        |

##### 4.3.3 è¯­è¨€è¯†åˆ«-ä¿å­˜æ¨¡å‹

```python
# ä¿å­˜ç»å†1æ¬¡epochçš„æ¨¡å‹çŠ¶æ€
learn.save('1epoch')
# åŠ è½½æ¨¡å‹
learn = learn.load('1epoch')
# åˆå§‹è®­ç»ƒå®Œæˆï¼Œè§£å†»åç»§ç»­å¾®è°ƒæ¨¡å‹
learn.unfreeze()
learn.fit_one_cycle(10,2e-3)
# ä¿å­˜ç¼–ç å™¨encoderï¼šç¼–ç å™¨å°±æ˜¯æˆ‘ä»¬è®­ç»ƒçš„æ‰€æœ‰æ¨¡å‹ï¼Œé™¤äº†æœ€åä¸€å±‚ï¼ˆå®ƒå°†activationsè½¬æ¢æˆé¢„æµ‹tokençš„æ¦‚ç‡ï¼‰
learn.save_encoder('finetuned')
```

æ­¤æ—¶å·²ç»å®Œæˆç¬¬äºŒé˜¶æ®µï¼š

![](D:\Git\a\Path-Records\img\dlcf_1001.png)

```python
# Kaggleä¸­å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¸‹è½½ä¸‹æ¥
path1 = Path('/kaggle/working/')
learn.save(path1/'mymodel')  # ä¿å­˜æ¨¡å‹
learn.load(path1/'mymodel') #åŠ è½½æ¨¡å‹

path2 = Path('/kaggle/working/')
learn.save_encoder(path2/'finetuned')
learn.load_encoder(path2/'finetuned')
```

###### fastaiçš„å¿«æ·æ„é€ å™¨learn

ä¸åŒä»»åŠ¡æœ‰ä¸åŒçš„å¿«æ·æ„é€ å™¨ï¼š

| ä»»åŠ¡                 | å¿«æ·æ–¹æ³•                                            | åº•å±‚ç±»    |
| -------------------- | --------------------------------------------------- | --------- |
| è®¡ç®—æœºè§†è§‰           | `vision_learner`                                    | `Learner` |
| è‡ªç„¶è¯­è¨€å¤„ç†         | `language_model_learner`ã€`text_classifier_learner` | `Learner` |
| è¡¨æ ¼æ•°æ®             | `tabular_learner`                                   | `Learner` |
| ååŒè¿‡æ»¤ï¼ˆæ¨èç³»ç»Ÿï¼‰ | `collab_learner`                                    | `Learner` |

##### 4.3.4 æ–‡æœ¬ç”Ÿæˆï¼ˆä¸åœ¨é˜¶æ®µä¸­ï¼‰

```python
TEXT = "I liked this movie because"
N_WORDS = 40
N_SENTENCES = 2
# temperature=0.75ï¼šæ§åˆ¶éšæœºæ€§ã€‚>1æ›´éšæœºæ›´æœ‰åˆ›é€ åŠ›ä½†å¯èƒ½æ²¡æ„ä¹‰ï¼›<1æ›´ç¡®å®šæ›´åˆç†ä½†å¯èƒ½è¾ƒæ— èŠ
preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)
         for _ in range(N_SENTENCES)]
print("\n".join(preds))
```

##### 4.3.5 æ–‡æœ¬åˆ†ç±»-æ•°æ®åŠ è½½å™¨DataBlock

```python
# åˆ›å»ºæ•°æ®åŠ è½½å™¨
dls_clas = DataBlock(
    blocks=(TextBlock.from_folder(path,vocab=dls_lm.vocab),CategoryBlock),
    '''ä½¿ç”¨ dls_lm.vocab çš„è¯æ±‡è¡¨å¯¹æ–‡æœ¬è¿›è¡Œæ•°å€¼åŒ–ï¼Œä»¥ä¿æŒå’Œ dls_lm ç›¸åŒçš„è¯ç´¢å¼•ç¼–å·ï¼Œè¿™ä¸ªè¯æ±‡è¡¨ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å­—åºåˆ—ï¼Œ1 tokenâ†’1 integer
    é€šè¿‡ä¼ é€’ `is_lm=False`ï¼ˆæˆ–è€…æ ¹æœ¬ä¸ä¼ é€’ `is_lm`ï¼Œå› ä¸ºå®ƒé»˜è®¤ä¸º `False`ï¼‰ï¼Œæˆ‘ä»¬å‘Šè¯‰ `TextBlock` æˆ‘ä»¬æœ‰å¸¸è§„æ ‡è®°çš„æ•°æ®ï¼Œè€Œä¸æ˜¯å°†ä¸‹ä¸€ä¸ªæ ‡è®°ä½œä¸ºæ ‡ç­¾'''
    get_y=parent_label,
    get_items=partial(get_text_files,folders=['train','test']),
    splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path,path=path,bs=128,seq_len=72)
dls_clas.show_batch(max_n=4)
```

```python
# åªä¸ºäº†è¯´æ˜æ€ä¹ˆå¤„ç†åœ¨åˆ†ç±»ç®—æ³•ä¸­texté•¿çŸ­ä¸ä¸€çš„é—®é¢˜
files = get_text_files(path, folders=['train','test','unsup'])
txts = L(o.open().read() for o in files[:200])
spacy = WordTokenizer()
tkn = Tokenizer(spacy)
toks200 = txts.map(tkn)
num = Numericalize()
num.setup(toks200)
nums_samp = toks200[:10].map(num)
nums_samp.map(len)
'(#10) [158,319,181,193,114,145,260,146,252,295]ï¼šå¯è§Lç±»æ¯æ¡å¤§å°ä¸ç­‰'
```

ä½†æ˜¯ï¼ŒPyTorch çš„ DataLoaderéœ€è¦å°†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰é¡¹ç›®æ•´åˆåˆ°ä¸€ä¸ªå¼ é‡ä¸­ï¼Œè€Œä¸€ä¸ªå¼ é‡å…·æœ‰å›ºå®šçš„å½¢çŠ¶ï¼ˆå³ï¼Œæ¯ä¸ªè½´ä¸Šéƒ½æœ‰ç‰¹å®šçš„é•¿åº¦ï¼Œå¹¶ä¸”æ‰€æœ‰é¡¹ç›®å¿…é¡»ä¸€è‡´ï¼‰ï¼Œä¸ºäº†è®©å®ƒä»¬ç›¸ç­‰ï¼Œå°±å¾—å¡«å……ã€‚

* å¡«å……ï¼šæˆ‘ä»¬å°†æ‰©å±•æœ€çŸ­çš„æ–‡æœ¬ä»¥ä½¿å®ƒä»¬éƒ½å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„å¡«å……æ ‡è®°ï¼Œè¯¥æ ‡è®°å°†è¢«æˆ‘ä»¬çš„æ¨¡å‹å¿½ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…å†…å­˜é—®é¢˜å¹¶æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬å°†å¤§è‡´ç›¸åŒé•¿åº¦çš„æ–‡æœ¬æ‰¹é‡å¤„ç†åœ¨ä¸€èµ·ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªepochå‰ï¼ˆå¯¹äºè®­ç»ƒé›†ï¼‰æŒ‰é•¿åº¦å¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼Œåˆ†æˆå‡ ä¸ªbatchesï¼Œä½¿ç”¨æ¯ä¸ªbatchä¸­æœ€å¤§æ–‡æ¡£çš„å¤§å°ä½œä¸ºè¿™ä¸ªbatchçš„ç›®æ ‡å¤§å°ã€‚

* ä½¿ç”¨DataBlock+is_lm=Falseæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬æ“ä½œã€‚

###### DataBlock & DataLoaders

| **ç»„ä»¶**         | **ä½œç”¨**                            |
| ---------------- | ----------------------------------- |
| `DataBlock`      | åªæ˜¯ æ•°æ®å¤„ç†çš„æ¨¡æ¿ï¼Œä¸åŒ…å«æ•°æ®     |
| `.dataloaders()` | å°† `DataBlock` è½¬ä¸º `DataLoaders`   |
| `DataLoaders`    | çœŸæ­£çš„æ•°æ®åŠ è½½å™¨ï¼ŒåŒ…å«è®­ç»ƒ/éªŒè¯æ•°æ® |

###### DataLoadersçš„ä¸»è¦å˜ç§

| DataLoaders ç±»å‹                  | ç”¨é€”è¯´æ˜                                                     |
| --------------------------------- | ------------------------------------------------------------ |
| `ImageDataLoaders`                | å›¾åƒåˆ†ç±»ï¼ˆè‡ªåŠ¨ä»æ–‡ä»¶å¤¹æˆ– CSV åŠ è½½å›¾åƒæ•°æ®ï¼‰                  |
| `SegmentationDataLoaders`         | å›¾åƒåˆ†å‰²ï¼ˆåƒç´ çº§åˆ†ç±»ä»»åŠ¡ï¼Œè¾“å…¥å›¾åƒï¼Œè¾“å‡ºåˆ†å‰²æ©ç ï¼‰           |
| `ObjectDetectionDataLoaders`      | ç›®æ ‡æ£€æµ‹ï¼ˆæ£€æµ‹å›¾åƒä¸­ç›®æ ‡çš„ä½ç½®å’Œç±»åˆ«ï¼Œå¦‚ bounding boxï¼‰      |
| `TextDataLoaders`                 | è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼ˆè‡ªåŠ¨å¤„ç†æ–‡æœ¬æ•°æ®ï¼Œé€‚ç”¨äºè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬åˆ†ç±»ç­‰ï¼‰ |
| `TabularDataLoaders`              | è¡¨æ ¼æ•°æ®ï¼ˆç»“æ„åŒ–æ•°æ®ï¼Œå¦‚ CSVï¼Œç”¨äºåˆ†ç±»æˆ–å›å½’ä»»åŠ¡ï¼‰           |
| `CollabDataLoaders`               | ååŒè¿‡æ»¤ï¼ˆæ¨èç³»ç»Ÿï¼Œç”¨æˆ·-ç‰©å“-è¯„åˆ†æ•°æ®ï¼‰                     |
| `DataLoaders`ï¼ˆåŸºç±»ï¼‰             | é€šç”¨æ•°æ®åŠ è½½å™¨ï¼Œé€‚ç”¨äºè‡ªå®šä¹‰æ•°æ®æˆ–ç‰¹æ®Šä»»åŠ¡                   |
| `TimeseriesDataLoaders`           | æ—¶é—´åºåˆ—æ•°æ®ï¼ˆç”¨äºé¢„æµ‹æœªæ¥åºåˆ—å€¼æˆ–åšåºåˆ—åˆ†ç±»ï¼‰               |
| `MixedDataLoaders` / `MultiBlock` | å¤„ç†æ··åˆè¾“å…¥ç±»å‹ï¼ˆå¦‚å›¾åƒ+è¡¨æ ¼+æ–‡æœ¬çš„å¤šæ¨¡æ€æ•°æ®ï¼‰ï¼Œé€šå¸¸é€šè¿‡ `DataBlock` æ„å»º |

##### 4.3.6 æ–‡æœ¬åˆ†ç±»-Fine-tune

```python
learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,metrics=accuracy).to_fp16()
learn = learn.load_encoder('/kaggle/input/finetuned/pytorch/default/1/finetuned')
```

* é€å±‚è§£å†»gradual unfreezingï¼šå†»ç»“æ¨¡å‹çš„éƒ¨åˆ†å±‚ï¼Œå…ˆè®­ç»ƒæ¨¡å‹çš„æœ€åä¸€å±‚ï¼Œç„¶åé€æ¸è§£å†»å‰é¢çš„å±‚ã€‚ä»æœ€åä¸€å±‚å¼€å§‹è®­ç»ƒï¼Œå› ä¸ºè¿™äº›å±‚å·²ç»åŒ…å«äº†å¤§éƒ¨åˆ†çš„ç‰¹å¾ä¿¡æ¯ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸è§£å†»å‰é¢çš„å±‚ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ã€‚
* é€å±‚è§£å†»çš„åŸå› ï¼šâ‘ é¢„è®­ç»ƒçš„ç‰¹å¾æå–å±‚-åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œé€šå¸¸å·²ç»å­¦åˆ°äº†é€šç”¨çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒè¿™äº›å±‚ï¼Œè€Œæ˜¯å†»ç»“å®ƒä»¬ï¼Œä¸“æ³¨äºè®­ç»ƒæ–°ä»»åŠ¡çš„æœ€åå‡ å±‚ã€‚â‘¡é¿å…è¿‡æ‹Ÿåˆ-å¦‚æœä¸€æ¬¡æ€§è§£å†»æ‰€æœ‰å±‚ï¼Œæ¨¡å‹å¯èƒ½ä¼šå¿«é€Ÿè¿‡æ‹Ÿåˆï¼Œå› ä¸ºè¾ƒå°çš„æ•°æ®é›†æ— æ³•æ”¯æŒæ‰€æœ‰å±‚çš„è®­ç»ƒã€‚é€å±‚è§£å†»æœ‰åŠ©äºé€æ¸é€‚åº”æ–°ä»»åŠ¡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆçš„é£é™©ã€‚â‘¢è®­ç»ƒæ•ˆç‡-å†»ç»“å‰é¢å‡ å±‚åï¼Œè®¡ç®—é‡å‡å°‘ï¼Œè®­ç»ƒé€Ÿåº¦åŠ å¿«ã€‚

```python
learn.fit_one_cycle(1,2e-2) #é»˜è®¤æ¨¡å‹ä¼šå†»ç»“ï¼Œå› æ­¤åªæ˜¯è®­ç»ƒæœ€åä¸€å±‚
# è®­ç»ƒå€’æ•°2å±‚
learn.freeze_to(-2)
learn.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2))
# è®­ç»ƒå€’æ•°3å±‚
learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))
# è®­ç»ƒæ‰€æœ‰å±‚
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
```

#### 4.4 æ€»æµç¨‹codes

```python
# å‡†å¤‡
from fastai.text.all import *
path = untar_data(URLs.IMDB)
# è¯­è¨€è¯†åˆ«æ•°æ®åŠ è½½å™¨
get_imdb = partial(get_text_files, folders=['train','test','unsup'])
dls_lm = DataBlock(
    blocks = TextBlock.from_folder(path,is_lm=True),
    get_items = get_imdb,
    splitter = RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
dls_lm.show_batch(max_n=3)
# è¯­è¨€è¯†åˆ«learn
learn = language_model_learner(
    dls_lm,
    AWD_LSTM,
    drop_mult=0.3,
    metrics=[accuracy, Perplexity()]
).to_fp16()
# è¯­è¨€è¯†åˆ«fine-tuneå’Œä¿å­˜ç¼–ç å™¨
learn.fit_one_cycle(1, 2e-2)
learn.save('1epoch')
learn.unfreeze()
learn.fit_one_cycle(1,2e-3)
learn.save_encoder('finetuned')
# æ–‡æœ¬åˆ†ç±»æ•°æ®åŠ è½½å™¨
dls_clas = DataBlock(
    blocks=(TextBlock.from_folder(path,vocab=dls_lm.vocab),CategoryBlock),
    get_y=parent_label,
    get_items=partial(get_text_files,folders=['train','test']),
    splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path,path=path,bs=128,seq_len=72)
# æ–‡æœ¬åˆ†ç±»learn
learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,metrics=accuracy).to_fp16()
learn = learn.load_encoder('/kaggle/input/finetuned/pytorch/default/1/finetuned')
# æ–‡æœ¬ç”Ÿæˆfine-tune
learn.fit_one_cycle(1,2e-2) 
learn.freeze_to(-2)
learn.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2))
learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
```

## 5: From-scratch model (Tabular) (PDL2022)

### 5.1 æ¸…æ´—æ•°æ®

- ä¸€äº›èµ·æ‰‹codeï¼Œä½ åº”è¯¥å·²ç»ç†Ÿæ‚‰

```python
import os
from pathlib import Path

iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')
if iskaggle: path = Path('../input/titanic')
else:
    path = Path('titanic')
    if not path.exists():
        import zipfile,kaggle
        kaggle.api.competition_download_cli(str(path))
        zipfile.ZipFile(f'{path}.zip').extractall(path)
        
import torch, numpy as np, pandas as pd
np.set_printoptions(linewidth=140)
torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)
pd.set_option('display.width', 140)
```

- æ•°æ®æ¸…æ´—

```python
df = pd.read_csv(path/'train.csv')
#æ£€æŸ¥missing data
df.isna().sum()
 #modeè®¡ç®—ä¼—æ•°
modes = df.mode().iloc[0]
df.fillna(modes, inplace=True)
df.isna().sum()
#ä»¥ä¸Šï¼Œå¤„ç†äº†æ‰€æœ‰çš„missing data
```

#### imputing missing values

å¡«è¡¥ç¼ºå¤±å€¼ï¼Œåœ¨Jeremyçš„ä¾‹å­ä¸­ï¼Œä»–ä½¿ç”¨äº†ä¼—æ•°å»å¡«è¡¥ç¼ºå¤±å€¼ï¼Œä»–è¯´è¿™ä¸ªæ–¹æ³•é€šå¸¸æœ‰ç”¨ã€‚ä¸€èˆ¬å½“æ•°æ®ä¸­åŒ…å«ç¼ºå¤±å€¼ï¼Œä¸å»ºè®®ç›´æ¥åˆ é™¤åˆ—ï¼Œå› ä¸ºå¹¶æ²¡æœ‰å®é™…çš„ç†ç”±åˆ é™¤åˆ—ï¼Œä½†æœ‰å¾ˆå¤šç†ç”±ä¿ç•™åˆ—ã€‚æœ‰å¦ä¸€ç§å¤„ç†æ–¹å¼ï¼Œå°±æ˜¯æ–°å¢ä¸€åˆ—å»è¯´æ˜æŸåˆ—æ˜¯å¦æ˜¯ç¼ºå¤±å€¼ï¼Œè¿™æ ·ä¿¡æ¯å°±å¾—ä»¥ä¿å…¨ã€‚

ç”¨ä¼—æ•°å¡«è¡¥ç¼ºå¤±å€¼æ˜¯æœ€æœ€ç®€å•çš„æ–¹æ³•ï¼Œä¸ºä»€ä¹ˆè¦ç”¨è¿™ä¹ˆç®€å•çš„æ–¹æ³•å‘¢ï¼Ÿå› ä¸ºå¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œç”¨ä»€ä¹ˆæ–¹æ³•å¡«è¡¥ç¼ºå¤±å€¼éƒ½æ²¡æœ‰ä»€ä¹ˆå¤ªå¤§çš„åŒºåˆ«ï¼Œæ‰€ä»¥åœ¨åˆšå¼€å§‹å»ºç«‹ä¸€ä¸ªbaselineçš„æ—¶å€™ï¼Œæ²¡æœ‰å¿…è¦æå¾—å¤ªå¤æ‚ã€‚

```python
#å¤§è‡´reviewä¸€ä¸‹åŒ…å«æ•°å­—çš„æ•°æ®
df.describe(include=(np.number))
df['Fare'].hist();
#å‘ç°Fareæ˜¯ä¸ªé•¿å°¾æ•°æ®ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¸å¤ªå–œæ¬¢é•¿å°¾æ•°æ®ï¼Œä¼šå½±å“æ¨¡å‹æ•ˆæœï¼Œä¸è¿‡æœ‰ä¸ªå‡ ä¹è‚¯å®šæˆæœçš„åŠæ³•ï¼Œå¯ä»¥è®²é•¿å°¾æ•°æ®è½¬æ¢æˆç¦»æ•£æ•°æ®
df['LogFare'] = np.log(df['Fare']+1)
#å‘ç°Pclassçœ‹ä¸Šå»ä¸åƒä¸ªæ­£ç»æ•°æ®ï¼Œåƒä¸ªåˆ†ç±»æ•°æ®ï¼Œç¡®è®¤ä¸€ä¸‹
pclasses = sorted(df.Pclass.unique())

#å¤§è‡´reviewä¸€ä¸‹ä¸æ˜¯æ•°å­—çš„æ•°æ®ï¼šNameã€Sexã€Ticketã€Cabinã€Embarkedï¼Œå¯ä»¥çœ‹åˆ°æœ‰å‡ ä¸ªæ•°æ®å¯ä»¥åˆ†ç±»ä¸ºå‡ ç±»ï¼Œæœ‰æœºä¼šè½¬æ¢æˆå¯ç”¨æ•°æ®
df.describe(include=[object])
#æƒ³ç”¨çš„åˆ†ç±»æ•°æ®æ˜¾ç„¶ä¸èƒ½å°±ç”¨objectè®­ç»ƒï¼Œæˆ‘ä»¬å°†å®ƒè½¬æ¢æˆdummy column
df = pd.get_dummies(df, columns=["Sex","Pclass","Embarked"])
df.columns
'''Index(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',
       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],
      dtype='object')'''
added_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']
df[added_cols].head()

#ç°åœ¨å¯ä»¥æ„å»ºæ•°æ®äº†
from torch import tensor
t_dep = tensor(df.Survived) #output
indep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols #input
t_indep = tensor(df[indep_cols].values, dtype=torch.float) #pytorchéœ€è¦æ•°æ®æ˜¯float
t_indep.shape
len(t_indep.shape) #rank
```

### 5.2 æ„å»ºçº¿æ€§æ¨¡å‹â›”

- åˆå§‹åŒ–parameters

```python
torch.manual_seed(442) #è¿™ä¸ªä¼šä½¿éšæœºç”Ÿæˆæ•°å¯å†ç°ï¼Œä¸è¿‡Jeremyåœ¨å®é™…æ“ä½œä¸­å¹¶ä¸ä¼šä½¿ç”¨å®ƒ
n_coeff = t_indep.shape[1]
coeffs = torch.rand(n_coeff)-0.5 #torch.rand()ç”Ÿæˆ0~1çš„æ•°ï¼Œæ‰€ä»¥å‡0.5
```

- åˆ©ç”¨broadcastingè¿›è¡Œè®¡ç®—å¹¶æ±‚loss

```python
#å½’ä¸€åŒ–
vals,indices = t_indep.max(dim=0)
t_indep = t_indep / vals
#è®¡ç®—preds
preds = (t_indep*coeffs).sum(axis=1)
#è®¡ç®—loss
loss = torch.abs(preds-t_dep).mean()
#å°†ä¸Šè¿°è®¡ç®—å†™æˆå‡½æ•°
def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)
def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()
```

### 5.3 æ¢¯åº¦ä¸‹é™(one step)â›”

```python
#å¼€å¯æ¢¯åº¦è·Ÿè¸ª
coeffs.requires_grad_()
#è®¡ç®—loss
loss = calc_loss(coeffs, t_indep, t_dep)
#åå‘ä¼ æ’­
loss.backward()
coeffs.grad #æŸ¥çœ‹
#æ‰‹åŠ¨
with torch.no_grad(): #æš‚æ—¶å…³é—­PyTorchçš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶
    coeffs.sub_(coeffs.grad * 0.1)
    coeffs.grad.zero_()
    print(calc_loss(coeffs, t_indep, t_dep))
```

### 5.4 è®­ç»ƒæ¨¡å‹

- æ¸…æ´—å¥½æ•°æ®ï¼Œå¹¶å½’ä¸€åŒ–
- æ‹†åˆ†æ•°æ®ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†

```python
from fastai.data.transforms import RandomSplitter
trn_split,val_split=RandomSplitter(seed=42)(df) #trn_split/val_splitæ˜¯dfçš„rank-0å¤§å°çš„index
'(#10) [788,525,821,253,374,98,215,313,281,305]ä¸€å…±æœ‰713rowsï¼Œéšæœºé€‰å–rowçš„æ•°'
trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]
trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]
len(trn_indep),len(val_indep)
```

- å†™æ›´æ–°æ¢¯åº¦çš„å‡½æ•°ï¼ˆæ€è·¯å¯ä»¥å‚è€ƒ5.3 withä¸‹é¢çš„éƒ¨åˆ†ï¼‰

```python
def update_coeffs(coeffs, lr):
    coeffs.sub_(coeffs.grad * lr)
    coeffs.grad.zero_()
```

- å†™è®­ç»ƒä¸€ä¸ªepochçš„å‡½æ•°ï¼ˆæ€è·¯å¯ä»¥å‚è€ƒ5.2loss/5.3ï¼‰

```python
#5.2æ„å»ºçš„æ¨¡å‹æŒªåˆ°è¿™é‡Œï¼ˆé¢„æµ‹å’Œlossï¼‰
def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)
def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()
#5.3å†™ä¸€ä¸ªepoch
def one_epoch(coeffs, lr):
    loss = calc_loss(coeffs, trn_indep, trn_dep) #å¼•ç”¨å‡½æ•°
    loss.backward()
    with torch.no_grad(): update_coeffs(coeffs, lr) #å¼•ç”¨å‡½æ•°
    print(f"{loss:.3f}", end="; ")
```

- åˆå§‹åŒ–å‚æ•°ï¼ˆå‚è€ƒ5.2/5.3ï¼‰

```python
def init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()
```

- å†™è®­ç»ƒnä¸ªepochçš„å‡½æ•°ï¼Œè¾“å‡ºlosså˜åŒ–ï¼Œè¿”å›å‚æ•°

```python
def train_model(epochs=30, lr=0.01):
    torch.manual_seed(442) #ä¸ºäº†å¯å¤ç°
    coeffs = init_coeffs()
    for i in range(epocks): one_epoch(coeffs, lr=lr)
    return coeffs
```

- è®­ç»ƒ

```python
coeffs = train_model(18, lr=0.02)
```

- æ˜¾ç¤ºå‚æ•°

```python
def show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))
show_coeffs()
```

### 5.5 è¡¡é‡å‡†ç¡®åº¦

```python
preds = calc_preds(coeffs, val_indep)
results = val_dep.bool()==(preds>0.5)
results.float().mean()

#ä¹Ÿå†™æˆå‡½æ•°
def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs,val_indep)>0.5)).float().mean()
acc(coeffs)
```

### 5.6 ä½¿ç”¨æ¿€æ´»å‡½æ•°

æ›´æ–°äº†ä¸€ä¸‹predsçš„è®¡ç®—å‡½æ•°ï¼ˆå‚è€ƒ5.2/5.4ï¼‰

```python
def calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))
```

å…¶å®ƒæ²¡ä»€ä¹ˆå˜åŒ–ï¼Œä½†ä¸ºäº†æ–¹ä¾¿åˆ—å‡º

```python
coeffs = train_model(lr=100)
acc(coeffs)
show_coeffs()
```

å¦‚æœä¸ç”¨æ¿€æ´»å‡½æ•°ä¼šå¯¼è‡´å¾ˆå¤šæ•°å­—æº¢å‡ºåˆ°0~1ä¹‹å¤–ï¼Œæœ€ç»ˆä¼šç‰¹åˆ«å½±å“è®­ç»ƒæ•ˆæœã€‚æ¯”å¦‚å¦‚æœä¸ç”¨æ¿€æ´»å‡½æ•°ï¼Œä¾ç„¶lr=100ï¼Œé‚£ä¹ˆæœ€åä¼šå¯¼è‡´æ¨¡å‹ä¸æ”¶æ•›ï¼Œåä¹‹æ¨¡å‹æ”¶æ•›å¾—å¾ˆå¥½ï¼Œaccuracyæé«˜äº†å¾ˆå¤šã€‚

### 5.7 æµ‹è¯•é›†

```python
tst_df = pd.read_csv(path/'test.csv')
tst_df['Fare'] = tst_df.Fare.fillna(0)
tst_df.fillna(modes, inplace=True)
tst_df['LogFare'] = np.log(tst_df['Fare']+1)
tst_df = pd.get_dummies(tst_df, columns=["Sex","Pclass","Embarked"])
tst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)
tst_indep = tst_indep / vals
tst_df['Survived'] = (calc_preds(coeffs, tst_indep)>0.5).int()
sub_df = tst_df[['PassengerId','Survived']]
sub_df.to_csv('sub.csv', index=False)
!head sub.csv
```

### 5.8 matrixè½¬æ¢

è¿˜æ˜¯5.6æåˆ°çš„predsçš„è®¡ç®—å‡½æ•°ï¼Œå†æ›´æ–°ä¸€ä¸‹ï¼Œç”¨æ›´åŠ çŸ©é˜µçš„æ–¹æ³•è®¡ç®—

```python
def calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)
```

åŒæ ·çš„ï¼Œ5.4ä¸­coeffséšæœºç”Ÿæˆä¹Ÿè¦ç”ŸæˆçŸ©é˜µ

```python
def init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()
```

åŒæ ·çš„ï¼Œ5.4ä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å› å˜é‡ï¼Œä¹Ÿè½¬æˆçŸ©é˜µï¼Œè¿™ä¸ªå¿…é¡»è¦è½¬æ¢æˆçŸ©é˜µï¼Œè¿™æ ·æ‰èƒ½å¾—åˆ°n*1çš„çŸ©é˜µå†æ±‚å¹³å‡ï¼Œå¦åˆ™åœ¨è®¡ç®—lossçš„æ—¶å€™çŸ©é˜µ-rank1çš„tensorä¼šè¢«å¹¿æ’­æˆn\*nçš„çŸ©é˜µ

```python
trn_dep = trn_dep[:,None]
val_dep = val_dep[:,None]
```

### 5.9 ç¥ç»ç½‘ç»œ

- æ„å»ºæ¨¡å‹å‰å‚æ•°çš„å½¢å¼å·²ç»æƒ³å¥½äº†ï¼Œæ‰€ä»¥å…ˆåˆå§‹åŒ–å‚æ•°

```python
def init_coeffs(n_hidden=20):
    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden
    layer2 = torch.rand(n_hidden, 1)-0.3
    const = torch.rand(1)[0]
    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()
```

- æ„å»ºæ¨¡å‹çš„å‡½æ•°

```python
import torch.nn.functional as F
def calc_preds(coeffs, indeps):
    l1,l2,const = coeffs
    res = F.relu(indeps@l1)
    res = res@l2 + const
    return torch.sigmoid(res)
```

- è®¡ç®—lossçš„å‡½æ•°

```python
def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()
```

- æ›´æ–°å‚æ•°çš„å‡½æ•°

```python
def update_coeffs(coeffs, lr):
    for layer in coeffs:
        layer.sub_(layer.grad * lr)
        layer.grad.zero_()
```

- 1 epochçš„å‡½æ•°

```python
def one_epoch(coeffs, lr):
    loss = calc_loss(coeffs, trn_indep, trn_dep) #å¼•ç”¨å‡½æ•°
    loss.backward()
    with torch.no_grad(): update_coeffs(coeffs, lr) #å¼•ç”¨å‡½æ•°
    print(f"{loss:.3f}", end="; ")
```

- è®­ç»ƒæ¨¡å‹çš„å‡½æ•°

```python
def train_model(epochs=30, lr=0.01):
    torch.manual_seed(442) #ä¸ºäº†å¯å¤ç°
    coeffs = init_coeffs()
    for i in range(epocks): one_epoch(coeffs, lr=lr)
    return coeffs
```

- è·‘èµ·æ¥

```python
coeffs = train_model(lr=20)
```

- accuracyçš„å‡½æ•°

```python
def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs,val_indep)>0.5)).float().mean()
```

- è®¡ç®—accuracy

```python
acc(coeffs)
```

### 5.10 æ·±åº¦å­¦ä¹ 

- æ•°æ®å‡†å¤‡ä¸å˜
- æ„å»ºæ¨¡å‹å‰å‚æ•°çš„å½¢å¼å·²ç»æƒ³å¥½äº†ï¼Œæ‰€ä»¥å…ˆåˆå§‹åŒ–å‚æ•°

```python
#æ„å»ºä¸€ä¸ªå¤šå±‚ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬2ä¸ªéšè—å±‚å’Œ1ä¸ªè¾“å‡ºå±‚ï¼Œæ¯ä¸ªéšè—å±‚çš„è¾“å‡ºéƒ½æ˜¯10ä¸ª
def init_coeffs():
    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want
    sizes = [n_coeff] + hiddens + [1]
    n = len(sizes)
    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)] #å…ƒç´ ä¸ºtensorçš„list
    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)] #å…ƒç´ ä¸ºtensorçš„list
    for l in layers+consts: l.requires_grad_() #listä¸­çš„å…ƒç´ ç®€å•å †å 
    return layers,consts
```

- æ„å»ºæ¨¡å‹

```python
import torch.nn.functional as F
def calc_preds(coeffs, indeps):
    layers,consts = coeffs
    n = len(layers)
    res = indeps #è¾“å…¥ä½œä¸ºç»“æœçš„åˆå§‹åŒ–
    for i,l in enumerate(layers):
        res = res@l + consts[i] #è¿™ä¸ªæ˜¯tensorçš„ç›¸åŠ ä¼šè‡ªåŠ¨å¹¿æ’­
        if i!=n-1: res = F.relu(res) #é™¤äº†è¾“å‡ºå±‚ï¼Œå…¶å®ƒå±‚éƒ½ç”¨reluåšæ¿€æ´»å‡½æ•°
    return torch.sigmoid(res)
```

- è®¡ç®—lossçš„å‡½æ•°ï¼ˆæŠ„5.9ï¼‰

```python
def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()
```

- æ›´æ–°å‚æ•°çš„å‡½æ•°

```python
def update_coeffs(coeffs, lr):
    layers,consts = coeffs
    for layer in layers+consts:
        layer.sub_(layer.grad * lr)
        layer.grad.zero_()
```

- 1 epochçš„å‡½æ•°ï¼ˆæŠ„5.9ï¼‰

```python
def one_epoch(coeffs, lr):
    loss = calc_loss(coeffs, trn_indep, trn_dep) #å¼•ç”¨å‡½æ•°
    loss.backward()
    with torch.no_grad(): update_coeffs(coeffs, lr) #å¼•ç”¨å‡½æ•°
    print(f"{loss:.3f}", end="; ")
```

- è®­ç»ƒæ¨¡å‹çš„å‡½æ•°ï¼ˆæŠ„5.9ï¼‰

```python
def train_model(epochs=30, lr=0.01):
    torch.manual_seed(442) #ä¸ºäº†å¯å¤ç°
    coeffs = init_coeffs()
    for i in range(epocks): one_epoch(coeffs, lr=lr)
    return coeffs
```

- è·‘èµ·æ¥ï¼ˆæŠ„5.9ï¼Œä½†lråšäº†è°ƒæ•´ï¼Œå› ä¸ºå‚æ•°å¤šäº†ï¼‰

```python
coeffs = train_model(lr=2)
```

- accuracyçš„å‡½æ•°ï¼ˆæŠ„5.9ï¼‰

```python
def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs,val_indep)>0.5)).float().mean()
```

- è®¡ç®—accuracyï¼ˆæŠ„5.9ï¼‰

```python
acc(coeffs)
```

### 5.11 ç”¨frameworkæ„å»ºæ·±åº¦å­¦ä¹ 

#### 5.11.1 æ•°æ®å‡†å¤‡

- ä¸5.1å†…å®¹å¯¹æ¯”

```python
#æ•°æ®ä¸‹è½½
from pathlib import Path
import os

iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')
if iskaggle:
    path = Path('../input/titanic')
    !pip install -Uqq fastai
else:
    import zipfile,kaggle
    path = Path('titanic')
    if not path.exists():
        kaggle.api.competition_download_cli(str(path))
        zipfile.ZipFile(f'{path}.zip').extractall(path)
        
#è®¾ç½®æ ¼å¼
from fastai.tabular.all import * #éšå¼å¯¼å…¥äº†pandas as pd
pd.options.display.float_format = '{:.2f}'.format
set_seed(42)

#æ•°æ®å¤„ç†
df = pd.read_csv(path/'train.csv')
def add_features(df):
    df['LogFare'] = np.log1p(df['Fare'])
    df['Deck'] = df.Cabin.str[0].map(dict(A="ABC", B="ABC", C="ABC", D="DE", E="DE", F="FG", G="FG"))
    df['Family'] = df.SibSp+df.Parch
    df['Alone'] = df.Family==0
    df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')
    df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]
    df['Title'] = df.Title.map(dict(Mr="Mr",Miss="Miss",Mrs="Mrs",Master="Master"))
add_features(df)
#'Age', 'SibSp', 'Parch', 'LogFare'ï¼Œ'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'
#ç”ŸæˆDataLoadersï¼Œé‡Œé¢åŒ…å«äº†è®­ç»ƒé›†å’ŒéªŒè¯é›†
splits = RandomSplitter(seed=42)(df)
dls = TabularPandas(
    df, splits=splits,
    procs = [Categorify, FillMissing, Normalize],
    cat_names=["Sex","Pclass","Embarked","Deck", "Title"],
    cont_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Alone', 'TicketFreq', 'Family'],
    y_names="Survived", y_block = CategoryBlock(),
).dataloaders(path=".")
```

#### 5.11.2 æ¨¡å‹è®­ç»ƒ

- ç”¨äº†ç°æˆçš„å…¨è¿æ¥æ¨¡å‹

```python
#éšè—å±‚æ˜¯ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯è¾“å…¥è‡ªå˜é‡ä¸ªæ•°*10ï¼Œä¸€ä¸ªæ˜¯10*10ï¼Œè¾“å‡ºå±‚æ˜¯10*1
learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])
learn.lr_find(suggest_funcs=(slide, valley))
#Jeremyè¯´lré€‰å–slideå’Œvalleyä¹‹é—´çš„æ•°ä¼šå¾ˆå¥½
```

![](D:\Git\a\Path-Records\img\05-1-1.png)

```python
learn.fit(16, lr=0.03)
```

#### 5.11.3 æµ‹è¯•é›†

- è®­ç»ƒå¥½çš„æ¨¡å‹ç”¨å…¨æ–°çš„æµ‹è¯•é›†å»æµ‹è¯•æ³›åŒ–èƒ½åŠ›

```python
#trainæ–‡ä»¶ä¸­çš„Fareæ˜¯æ²¡æœ‰ç©ºçš„ï¼Œä½†æ˜¯testä¸­çš„æœ‰ç©ºï¼Œæ‰€ä»¥å¾—å…ˆè¡¥ä¸Šï¼Œå†åšå’Œtrainä¸­åŒæ ·çš„æ“ä½œ
tst_df = pd.read_csv(path/'test.csv')
tst_df['Fare'] = tst_df.Fare.fillna(0)
add_features(tst_df)

tst_dl = learn.dls.test_dl(tst_df)
preds,_ = learn.get_preds(dl=tst_dl)

tst_dl = learn.dls.test_dl(tst_df)
preds,_ = learn.get_preds(dl=tst_dl)
tst_df['Survived'] = (preds[:,1]>0.5).int()
sub_df = tst_df[['PassengerId','Survived']]
sub_df.to_csv('sub.csv', index=False)
!head sub.csv
```

#### 5.11.4 æ•´ä½“è¿è¡Œå¤šæ¬¡

- ç”±äºæ¯æ¬¡å‚æ•°éƒ½æ˜¯éšæœºç”Ÿæˆçš„ï¼Œæ‰€ä»¥éšæœºç”Ÿæˆå‚æ•°çš„è´¨é‡ä¹Ÿä¼šå½±å“æ¨¡å‹æœ€ç»ˆçš„è®­ç»ƒæ•ˆæœï¼Œå› æ­¤æˆ‘ä»¬å¤šæ¬¡ç”Ÿæˆå‚æ•°è®­ç»ƒæ¨¡å‹ï¼Œå–å¹³å‡å€¼

```python
def ensemble():
    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])
    with learn.no_bar(),learn.no_logging(): learn.fit(16, lr=0.03)
    return learn.get_preds(dl=tst_dl)[0] #[0]æ˜¯predsï¼Œæ˜¯ä¸€ä¸ªn*2çš„tensorï¼Œ[1]æ˜¯targets
learns = [ensemble() for _ in range(5)] #ç”Ÿæˆä¸€ä¸ªlistï¼Œè¿™ä¸ªlistä¸­åŒ…å«5ä¸ªtensor
ens_preds = torch.stack(learns).mean(0) #torch.stack(learns).shape = [5, 418, 2]ï¼Œens_preds.shape = [418, 2]
```

- æ±‚å¹³å‡å€¼æœ‰å¥½å‡ ç§æ–¹æ³•ï¼šâ‘ ç›´æ¥æ±‚5æ¬¡æ¦‚ç‡çš„å¹³å‡å€¼ï¼Œå†01åŒ–ï¼›â‘¡å…ˆ01åŒ–å†æ±‚5æ¬¡å¹³å‡å€¼ï¼Œå†01åŒ–ï¼›â‘¢å…ˆ01åŒ–ç„¶åå–ä¼—æ•°ã€‚ä¸€èˆ¬æƒ…å†µä¸‹â‘ å’Œâ‘¡çš„æ•ˆæœç›¸å¯¹æ¯”è¾ƒå¥½ï¼Œå¶å°”â‘¢æ¯”è¾ƒå¥½ã€‚

```python
tst_df['Survived'] = (ens_preds[:,1]>0.5).int()
sub_df = tst_df[['PassengerId','Survived']]
sub_df.to_csv('ens_sub.csv', index=False)
```

## 5: Tabular (fastbook9)

### 5.1 åˆ†ç±»è‡ªå˜é‡çš„å¤„ç†â€”â€”åˆ†ç±»åµŒå…¥

- åµŒå…¥å±‚Embedding = ç‹¬çƒ­ç¼–ç one-hot + çº¿æ€§å±‚ï¼ˆæ²¡æœ‰biasï¼‰

- Embeddingæœ¬èº«æ˜¯ä¸€ä¸ªrank-2çš„çŸ©é˜µ

- ä¸¾ä¾‹ï¼š

  - ç‹¬çƒ­ç¼–ç +çº¿æ€§å±‚

    å‡è®¾ä½ æœ‰ 5 ä¸ªç±»åˆ«ï¼Œæƒ³è¦å°†å…¶æ˜ å°„æˆä¸€ä¸ªé•¿åº¦ä¸º 3 çš„å‘é‡ã€‚ä½ å¯ä»¥è¿™æ ·åšï¼šâ‘ ç‹¬çƒ­ç¼–ç ï¼ˆone-hotï¼‰ä¸€ä¸ªç±»åˆ«ï¼š

    ```
    ç±»åˆ« 2 â†’ [0, 1, 0, 0, 0]
    ```

    â‘¡ç„¶åé€šè¿‡çº¿æ€§å±‚ï¼ˆæƒé‡çŸ©é˜µ `W.shape = (5, 3)`ï¼‰ï¼š

    ```
    è¾“å‡º = one_hot_vector @ W
    ```

    è¿™ä¸ªæœ¬è´¨å°±æ˜¯ç”¨ one-hot é€‰ä¸­çŸ©é˜µ `W` ä¸­çš„æŸä¸€è¡Œã€‚

  - åµŒå…¥å±‚

    åµŒå…¥å±‚ä¹Ÿæ˜¯ä¸€ä¸ªæŸ¥è¡¨æ“ä½œï¼Œ`Embedding(num_embeddings=5, embedding_dim=3)` æœ¬è´¨ä¸Šç»´æŠ¤ä¸€ä¸ªå½¢çŠ¶ä¸º `(5, 3)` çš„æƒé‡çŸ©é˜µï¼Œæ¯æ¬¡æ ¹æ®ç±»åˆ«ç´¢å¼•ï¼Œç›´æ¥å–å‡ºå¯¹åº”è¡Œä½œä¸ºè¾“å‡ºã€‚æ‰€ä»¥å®ƒåšçš„äº‹æƒ…æ˜¯ï¼š

    ```
    embedding = nn.Embedding(5, 3)
    output = embedding(torch.tensor([2]))  # å°±æ˜¯è¿”å› embedding.weight[2]
    ```

![](D:\Git\a\Path-Records\img\dlcf_0901.png)

- è¿™æœ¬ä¹¦ä¸¾ä¾‹äº†ä¸€ç¯‡è®ºæ–‡ï¼Œè®ºæ–‡æ˜¯é€šè¿‡å„ç§æ•°æ®é¢„æµ‹é”€é‡ï¼Œå…¶ä¸­ä¸€ä¸ªè‡ªå˜é‡å°±æ˜¯åŸå¸‚ï¼Œè®­ç»ƒå¥½ä¹‹åä½œè€…ç”»å‡ºäº†åµŒå…¥çŸ©é˜µï¼ˆä¸‹å›¾ï¼‰ï¼Œå¯ä»¥çœ‹åˆ°embeddingè‡ªåŠ¨å­¦ä¹ åˆ°äº†åŸå¸‚çš„åœ°ç†ä½ç½®ã€‚äº‹å®ä¸Šï¼Œåœ¨è®­ç»ƒä¹‹åï¼Œå•†åº—åµŒå…¥ä¹‹é—´çš„è·ç¦»å’Œå®é™…çš„åœ°ç†ä½ç½®è·ç¦»éå¸¸ç›¸è¿‘ã€‚

![](D:\Git\a\Path-Records\img\dlcf_0902.png)

- ç‹¬çƒ­dummyå’ŒåµŒå…¥çš„å·®å¼‚ï¼šç±»åˆ«å¤šã€ä½¿ç”¨æ·±åº¦å­¦ä¹  â†’ å¼ºçƒˆå»ºè®®ä½¿ç”¨åµŒå…¥ï¼ˆembeddingï¼‰

| å¯¹æ¯”ç»´åº¦            | ç‹¬çƒ­ç¼–ç ï¼ˆOne-hot Encodingï¼‰           | åµŒå…¥ï¼ˆEmbeddingï¼‰                                    |
| ------------------- | -------------------------------------- | ---------------------------------------------------- |
| **ç»´åº¦å¤§å°**        | é«˜ç»´ç¨€ç–ï¼ˆæ¯ä¸ªç±»åˆ«ä¸€ä¸ªç»´åº¦ï¼‰           | ä½ç»´ç¨ å¯†ï¼ˆé€šå¸¸å‡ ç»´~å‡ åç»´ï¼‰                          |
| **å†…å­˜/è®¡ç®—æ•ˆç‡**   | å å†…å­˜å¤šï¼Œè®¡ç®—æ…¢                       | å å†…å­˜å°‘ï¼Œè®¡ç®—å¿«                                     |
| **ç±»åˆ«ç›¸ä¼¼æ€§è¡¨è¾¾**  | æ— æ³•è¡¨è¾¾ç±»åˆ«é—´å…³ç³»ï¼ˆæ¯ä¸ªç±»åˆ«å½¼æ­¤ç‹¬ç«‹ï¼‰ | å¯ä»¥è¡¨è¾¾ç±»åˆ«é—´ç›¸ä¼¼æ€§ï¼ˆè·ç¦»è¶Šè¿‘è¶Šç›¸ä¼¼ï¼‰               |
| **æ˜¯å¦å¯è®­ç»ƒ**      | å¦ï¼Œå›ºå®šä¸å¯å­¦ä¹                        | æ˜¯ï¼ŒåµŒå…¥å‘é‡æ˜¯æ¨¡å‹å‚æ•°ï¼Œ**å¯å­¦ä¹ **                   |
| **æ¨¡å‹æ³›åŒ–èƒ½åŠ›**    | å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œç‰¹åˆ«æ˜¯ç±»åˆ«å¾ˆå¤šæ—¶           | æ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼Œèƒ½ä»ç›¸ä¼¼ç±»åˆ«ä¸­å€ŸåŠ›                     |
| **é€‚åˆçš„æ¨¡å‹ç±»å‹**  | çº¿æ€§æ¨¡å‹ã€æ ‘æ¨¡å‹ç­‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹     | æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯ç¥ç»ç½‘ç»œï¼‰                       |
| **å¯è§£é‡Šæ€§/å¯è§†åŒ–** | ä¸å…·å¤‡å¯è§†åŒ–è¯­ä¹‰ç»“æ„çš„èƒ½åŠ›             | åµŒå…¥å‘é‡å¯é™ç»´å¯è§†åŒ–ï¼ˆå¦‚ç”¨ t-SNEã€PCA å±•ç¤ºèšç±»ç»“æ„ï¼‰ |
| **ç±»åˆ«æ•°é‡é€‚åº”æ€§**  | ç±»åˆ«å°‘ï¼ˆ<10ï¼‰è¾ƒé€‚åˆ                    | ç±»åˆ«å¤šï¼ˆå‡ åä¸ªåˆ°å‡ åƒä¸ªï¼‰æ›´é€‚åˆ                       |
| **è®­ç»ƒé€Ÿåº¦å½±å“**    | è®­ç»ƒæ…¢ï¼Œå°¤å…¶æ˜¯é«˜ç»´æ•°æ®                 | è®­ç»ƒå¿«ï¼Œå‚æ•°å°‘ï¼Œä¼˜åŒ–ç©ºé—´å°                           |

### 5.2 Beyond Deep Learning

ç°ä»£æœºå™¨å­¦ä¹ å¯ä»¥å½’ç»“ä¸ºå‡ ç§å¹¿æ³›é€‚ç”¨çš„å…³é”®æŠ€æœ¯ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç»å¤§å¤šæ•°æ•°æ®é›†æœ€é€‚åˆç”¨ä¸¤ç§æ–¹æ³•å»ºæ¨¡ï¼š

+   å†³ç­–æ ‘é›†æˆï¼ˆå³éšæœºæ£®æ—å’Œæ¢¯åº¦æå‡æœºï¼‰ï¼Œä¸»è¦ç”¨äºç»“æ„åŒ–æ•°æ®ï¼ˆæ¯”å¦‚å¤§å¤šæ•°å…¬å¸æ•°æ®åº“è¡¨ä¸­å¯èƒ½æ‰¾åˆ°çš„æ•°æ®ï¼‰è®­ç»ƒå¿«ï¼Œè§£å®æ€§å¼ºï¼Œæœ‰å·¥å…·å’Œæ–¹æ³•å¯ä»¥å›ç­”ç›¸å…³é—®é¢˜ï¼Œæ¯”å¦‚ï¼šæ•°æ®é›†ä¸­å“ªäº›åˆ—å¯¹ä½ çš„é¢„æµ‹æœ€é‡è¦ï¼Ÿå®ƒä»¬ä¸å› å˜é‡æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿå®ƒä»¬å¦‚ä½•ç›¸äº’ä½œç”¨ï¼Ÿå“ªäº›ç‰¹å®šç‰¹å¾å¯¹æŸä¸ªç‰¹å®šè§‚å¯Ÿæœ€é‡è¦ï¼Ÿ

+   ä½¿ç”¨ SGD å­¦ä¹ çš„å¤šå±‚ç¥ç»ç½‘ç»œï¼ˆå³æµ…å±‚å’Œ/æˆ–æ·±åº¦å­¦ä¹ ï¼‰ï¼Œä¸»è¦ç”¨äºéç»“æ„åŒ–æ•°æ®ï¼ˆæ¯”å¦‚éŸ³é¢‘ã€å›¾åƒå’Œè‡ªç„¶è¯­è¨€ï¼‰

è¿™ä¸€å‡†åˆ™çš„ä¾‹å¤–æƒ…å†µæ˜¯å½“æ•°æ®é›†ç¬¦åˆä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€æ—¶ï¼š

+   æœ‰ä¸€äº›é«˜åŸºæ•°åˆ†ç±»å˜é‡éå¸¸é‡è¦ï¼ˆâ€œåŸºæ•°â€æŒ‡ä»£è¡¨ç¤ºç±»åˆ«çš„ç¦»æ•£çº§åˆ«çš„æ•°é‡ï¼Œå› æ­¤é«˜åŸºæ•°åˆ†ç±»å˜é‡æ˜¯æŒ‡åƒé‚®æ”¿ç¼–ç è¿™æ ·å¯èƒ½æœ‰æ•°åƒä¸ªå¯èƒ½çº§åˆ«çš„å˜é‡ï¼‰ã€‚

+   æœ‰ä¸€äº›åŒ…å«æœ€å¥½ç”¨ç¥ç»ç½‘ç»œç†è§£çš„æ•°æ®çš„åˆ—ï¼Œæ¯”å¦‚çº¯æ–‡æœ¬æ•°æ®ã€‚

åœ¨å®è·µä¸­ï¼Œäº‹æƒ…å¾€å¾€æ²¡æœ‰é‚£ä¹ˆæ˜ç¡®ï¼Œé€šå¸¸ä¼šæœ‰é«˜åŸºæ•°å’Œä½åŸºæ•°åˆ†ç±»å˜é‡ä»¥åŠè¿ç»­å˜é‡çš„æ··åˆã€‚å¾ˆæ˜æ˜¾æˆ‘ä»¬éœ€è¦å°†**å†³ç­–æ ‘**é›†æˆæ·»åŠ åˆ°æˆ‘ä»¬çš„å»ºæ¨¡å·¥å…·ç®±ä¸­ï¼

è¦ç”¨åˆ°å†³ç­–æ ‘ï¼Œæˆ‘ä»¬éœ€è¦å‡ ä¸ªpackageï¼šScikit-learn æ˜¯ä¸€ä¸ªæµè¡Œçš„åº“ï¼Œç”¨äºåˆ›å»ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨çš„æ–¹æ³•ä¸åŒ…æ‹¬æ·±åº¦å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€äº›è¡¨æ ¼æ•°æ®å¤„ç†å’ŒæŸ¥è¯¢ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ Pandas åº“ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜éœ€è¦ NumPyï¼Œå› ä¸ºè¿™æ˜¯ sklearn å’Œ Pandas éƒ½ä¾èµ–çš„ä¸»è¦æ•°å€¼ç¼–ç¨‹åº“ã€‚

### 5.3 æ•°æ®é›†

#### 5.3.1 å¤„ç†æ•°æ®é›†

```python
from fastai.tabular.all import *
path = Path('../input/bluebook-for-bulldozers')
path.ls(file_type='text')
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
df = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)
df.columns
#ä¸€äº›åˆ†ç±»æ•°æ®å¯èƒ½åšçš„æ•´ç†
df.ProductSize.unique() #æŸ¥çœ‹éƒ½æœ‰ä»€ä¹ˆå€¼
sizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'
df['ProductSize'] = df['ProductSize'].astype('category') #è½¬æ¢æˆpandasçš„åˆ†ç±»ç±»å‹
df['ProductSize'] = df['ProductSize'].cat.set_categories(sizes, ordered=True) #æ’åº
df.ProductSize.unique() #æ’å¥½åºäº†
#å› å˜é‡
df['SalePrice']=np.log(df['SalePrice'])

#æ—¥æœŸç±»æ•°æ®çš„å¤„ç†
df = add_datepart(df, 'saledate') #å°†saledateè½¬æ¢æˆå„ç§æ—¥æœŸè¡¨è¾¾
df_test = pd.read_csv(path/'Test.csv', low_memory=False)
df_test = add_datepart(df_test, 'saledate')
' '.join(o for o in df.columns if o.startswith('sale'))
```

- è®­ç»ƒå†³ç­–æ ‘çš„åŸºæœ¬æ­¥éª¤å¯ä»¥å¾ˆå®¹æ˜“åœ°å†™ä¸‹æ¥ï¼š

  1ã€ä¾æ¬¡å¾ªç¯æ•°æ®é›†çš„æ¯ä¸€åˆ—ã€‚

  2ã€å¯¹äºæ¯ä¸€åˆ—ï¼Œä¾æ¬¡å¾ªç¯è¯¥åˆ—çš„æ¯ä¸ªå¯èƒ½çº§åˆ«ã€‚

  3ã€å°è¯•å°†æ•°æ®åˆ†æˆä¸¤ç»„ï¼ŒåŸºäºå®ƒä»¬æ˜¯å¦å¤§äºæˆ–å°äºè¯¥å€¼ï¼ˆæˆ–è€…å¦‚æœå®ƒæ˜¯ä¸€ä¸ªåˆ†ç±»å˜é‡ï¼Œåˆ™åŸºäºå®ƒä»¬æ˜¯å¦ç­‰äºæˆ–ä¸ç­‰äºè¯¥åˆ†ç±»å˜é‡çš„æ°´å¹³ï¼‰ã€‚

  4ã€æ‰¾åˆ°è¿™ä¸¤ç»„ä¸­æ¯ç»„çš„å¹³å‡é”€å”®ä»·æ ¼ï¼Œå¹¶æŸ¥çœ‹è¿™ä¸è¯¥ç»„ä¸­æ¯ä¸ªè®¾å¤‡çš„å®é™…é”€å”®ä»·æ ¼æœ‰å¤šæ¥è¿‘ã€‚å°†è¿™è§†ä¸ºä¸€ä¸ªéå¸¸ç®€å•çš„â€œæ¨¡å‹â€ï¼Œå…¶ä¸­æˆ‘ä»¬çš„é¢„æµ‹åªæ˜¯è¯¥é¡¹ç»„çš„å¹³å‡é”€å”®ä»·æ ¼ã€‚

  5ã€åœ¨å¾ªç¯éå†æ‰€æœ‰åˆ—å’Œæ¯ä¸ªå¯èƒ½çš„çº§åˆ«åï¼Œé€‰æ‹©ä½¿ç”¨è¯¥ç®€å•æ¨¡å‹ç»™å‡ºæœ€ä½³é¢„æµ‹çš„åˆ†å‰²ç‚¹ã€‚

  6ã€ç°åœ¨æˆ‘ä»¬çš„æ•°æ®æœ‰ä¸¤ç»„ï¼ŒåŸºäºè¿™ä¸ªé€‰å®šçš„åˆ†å‰²ã€‚å°†æ¯ä¸ªç»„è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡è¿”å›åˆ°æ­¥éª¤ 1 ä¸ºæ¯ä¸ªç»„æ‰¾åˆ°æœ€ä½³åˆ†å‰²ã€‚7ã€é€’å½’åœ°ç»§ç»­è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æ¯ä¸ªç»„è¾¾åˆ°æŸä¸ªåœæ­¢æ ‡å‡†-ä¾‹å¦‚ï¼Œå½“ç»„ä¸­åªæœ‰ 20 ä¸ªé¡¹ç›®æ—¶åœæ­¢è¿›ä¸€æ­¥åˆ†å‰²ã€‚

- æˆ‘ä»¬å¯ä»¥èŠ‚çœä¸€äº›æ—¶é—´ï¼Œä½¿ç”¨å†…ç½®åœ¨ sklearn ä¸­çš„å®ç°ã€‚ä¸ºæ­¤ï¼Œè¦åšä¸€äº›æ•°æ®å‡†å¤‡ã€‚

#### 5.3.2 å¤„ç†å­—ç¬¦ä¸²-ä½¿ç”¨ TabularPandas å’Œ TabularProc

- å¤„ç†å­—ç¬¦ä¸²å’Œç¼ºå¤±æ•°æ®

```python
#æ‹†åˆ†æµ‹è¯•é›†å’ŒéªŒè¯é›†id
cond = (df.saleYear<2011) | (df.saleMonth<10)
train_idx = np.where( cond)[0]
valid_idx = np.where(~cond)[0]
splits = (list(train_idx),list(valid_idx))

#è¯†åˆ«ä¸€äº›è¿ç»­è‡ªå˜é‡ã€åˆ†ç±»è‡ªå˜é‡å’Œå› å˜é‡ï¼Œå¹¶ä½¿å¾—å®ƒä»¬å…·æœ‰æ•°å­—æ˜¯å±æ€§
procs = [Categorify, FillMissing]
dep_var = 'SalePrice'
cont,cat = cont_cat_split(df, 1, dep_var=dep_var) #contè¿”å›çš„æ˜¯è¿ç»­è‡ªå˜é‡çš„ç‰¹å¾å
to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) #æ‹†åˆ†æˆtrainå’Œvalidäº†
#len(to.train),len(to.valid)
#to.show(3) #å¯ä»¥çœ‹åˆ°toæ²¡æœ‰saledateï¼Œå˜æˆäº†å¾ˆå¤šæ—¥æœŸæ‹†åˆ†ï¼›salepriceåœ¨æœ€å
#to.items.head(3) #å˜é‡éƒ½ä¸æ˜¯å­—ç¬¦ä¸²äº†ï¼Œè€Œæ˜¯æ•°å­—

#ä¿å­˜
'''
from fastcore.foundation import Path
from fastcore.xtras import save_pickle, load_pickle
path_out = Path('../working')
save_pickle(path_out/'to.pkl', to)
to_loaded = load_pickle(path_out/'to.pkl')
'''
```

- TabularPandasï¼šæ˜¯ fastai åº“ä¸­ç”¨äºå¤„ç†è¡¨æ ¼æ•°æ®ï¼ˆtabular dataï¼‰çš„ä¸€ä¸ªå…³é”®ç±»ï¼Œç”¨äºå°è£…å¹¶é¢„å¤„ç†è¡¨æ ¼æ•°æ®ï¼Œä»¥ä¾¿ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ƒç›¸å½“äºæŠŠ pandas.DataFrame åŒ…è£…æˆäº†ä¸€ä¸ªå¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒçš„ç»“æ„ï¼ˆï¼‰ã€‚

| åŠŸèƒ½                          | è¯´æ˜                                                         |
| ----------------------------- | ------------------------------------------------------------ |
| **é¢„å¤„ç†æ•°æ®**                | åŒ…æ‹¬ç±»åˆ«ç¼–ç ï¼ˆCategorifyï¼‰ã€ç¼ºå¤±å€¼å¡«è¡¥ï¼ˆFillMissingï¼‰ã€æ•°å€¼æ ‡å‡†åŒ–ï¼ˆNormalizeï¼‰ç­‰ |
| **åˆ’åˆ†æ•°æ®é›†**                | è®­ç»ƒé›†ã€éªŒè¯é›†ç­‰                                             |
| **æ”¯æŒ fastai çš„ DataLoader** | å¯ä»¥ç”¨ `.dataloaders()` ç›´æ¥è½¬æˆå¯è®­ç»ƒçš„ DataLoader          |
| **å°è£…å¤„ç†æµç¨‹**              | åŒ…å« `procs`ï¼ˆé¢„å¤„ç†æµç¨‹ï¼‰ã€`cont_names`ï¼ˆè¿ç»­å˜é‡ï¼‰ã€`cat_names`ï¼ˆåˆ†ç±»å˜é‡ï¼‰ç­‰å‚æ•° |

### 5.4 å†³ç­–æ ‘

å†³ç­–æ ‘é€šå¸¸æƒ…å†µä¸‹æ˜¯äºŒåˆ†æ³•çš„ï¼›

è€Œä¸”åŒä¸€ä¸ªç‰¹å¾åœ¨åŒä¸€æ£µæ ‘ä¸Šå¯èƒ½å‡ºç°åœ¨ä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸ŠèŠ‚ç‚¹ä¸Šï¼Œå°¤å…¶æ˜¯å½“ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³ç³»è¾ƒå¼ºæ—¶ï¼›

é€‰æ‹©èŠ‚ç‚¹ï¼šâ‘ éå†æ‰€æœ‰ç‰¹å¾ï¼Œâ‘¡å¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œå°è¯•æ‰€æœ‰å¯èƒ½çš„åˆ‡åˆ†ç‚¹ï¼ˆæ•°å€¼ç‰¹å¾å°è¯•é˜ˆå€¼ã€åˆ†ç±»ç‰¹å¾å°è¯•å­é›†ï¼‰ï¼Œâ‘¢è®¡ç®—æ¯ä¸ªåˆ‡åˆ†æ–¹å¼ä¸‹çš„â€œä¸çº¯åº¦æŒ‡æ ‡â€ï¼ˆå¦‚åŸºå°¼æŒ‡æ•°ï¼‰ï¼Œâ‘£é€‰æ‹©èƒ½å¤Ÿå¸¦æ¥æœ€å¤§çº¯åº¦æå‡ï¼ˆå³æœ€å¤§ä¿¡æ¯å¢ç›Šæˆ–æœ€å°åŸºå°¼æŒ‡æ•°ï¼‰çš„ç‰¹å¾å’Œåˆ‡åˆ†ç‚¹ï¼Œâ‘¤åˆ›å»ºè¯¥èŠ‚ç‚¹å¹¶å‘ä¸‹ç»§ç»­åˆ†è£‚ï¼ˆé€’å½’ï¼‰ï¼›

å‰ªæï¼šæœ€å¤§æ·±åº¦ï¼ˆæ ‘çš„å±‚ï¼‰ï¼ŒèŠ‚ç‚¹æ ·æœ¬æ•°ï¼Œå­èŠ‚ç‚¹æ ·æœ¬æ•°ï¼ŒèŠ‚ç‚¹å®Œå…¨çº¯å‡€ï¼Œæ²¡æœ‰ç‰¹å¾å†åˆ’åˆ†ï¼Œåˆ’åˆ†ä¸èƒ½å¸¦æ¥è¶³å¤Ÿå¢ç›Šï¼Œæœ€å¤§å¶å­èŠ‚ç‚¹æ ‘ã€‚

#### 5.4.1 åˆ›å»ºå†³ç­–æ ‘

- è¯•ä¸€è¯•å†³ç­–æ ‘

```python
#æ ¹æ®TabularPandasæ„å»ºè‡ªå˜é‡å’Œå› å˜é‡
xs,y = to.train.xs,to.train.y
valid_xs,valid_y = to.valid.xs,to.valid.y

#è®­ç»ƒå†³ç­–æ ‘
from sklearn.tree import DecisionTreeRegressor
m = DecisionTreeRegressor(max_leaf_nodes=4)
m.fit(xs, y);

#ç»˜åˆ¶å†³ç­–æ ‘
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
plot_tree(m, feature_names=xs.columns, filled=True, precision=2)
plt.show()
!pip install -U dtreeviz
import dtreeviz
samp_idx = np.random.permutation(len(y))[:500]
viz = dtreeviz.model(
    m,                                # ä½ çš„è®­ç»ƒå¥½çš„æ¨¡å‹
    X_train=xs.iloc[samp_idx],              # ç‰¹å¾å˜é‡
    y_train=y.iloc[samp_idx],               # ç›®æ ‡å˜é‡
    feature_names=xs.columns.tolist(),      # åˆ—å
    target_name=dep_var                    # å­—ç¬¦ä¸²å½¢å¼çš„ç›®æ ‡åˆ—åï¼Œæ¯”å¦‚ "SalePrice"
)
viz.view(orientation="LR", scale=1.6, label_fontsize=8, fontname="DejaVu Sans")

#ä¿®æ­£ä¸€äº›æ˜¾ç„¶çš„é—®é¢˜ï¼Œç›®çš„æ˜¯ä½¿å¾—å›¾åƒæ›´åŠ æ¸…æ™°
xs.loc[xs['YearMade']<1900, 'YearMade'] = 1950
valid_xs.loc[valid_xs['YearMade']<1900, 'YearMade'] = 1950
m = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)
viz = dtreeviz.model(
    m,
    X_train=xs.iloc[samp_idx],
    y_train=y.iloc[samp_idx],
    feature_names=xs.columns.tolist(),
    target_name=dep_var
)
viz.view(orientation="LR", scale=1.6, label_fontsize=8, fontname="DejaVu Sans")
```

![](D:\Git\a\Path-Records\img\dlcf_09in02.png)

![](D:\Git\a\Path-Records\img\dlcf_09in03.png)

- æ·±åŒ–å†³ç­–æ ‘

```python
#ç–¯ç‹‚åˆ†å‰ï¼Œä¸è®¾ç½®ä»»ä½•åœæ­¢æ¡ä»¶
m = DecisionTreeRegressor()
m.fit(xs, y);
def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)
def m_rmse(m, xs, y): return r_mse(m.predict(xs), y)
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)
'(0.0, 0.335804)è¿‡æ‹Ÿåˆäº†'

#æŸ¥çœ‹å†³ç­–æ ‘å¶æ•°é‡
m.get_n_leaves(), len(xs)
'(324555, 404710)å¶å­èŠ‚ç‚¹ä¸ªæ•°ç‰¹åˆ«å¤šï¼Œåˆ†å‰å¤ªè¿‡äº†'

#è®¾ç½®æ¯ä¸ªå¶å­ä¸Šé¢æœ€å°‘çš„æ ·æœ¬æ•°é‡
m = DecisionTreeRegressor(min_samples_leaf=25)
m.fit(to.train.xs, to.train.y)
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)
'(0.248564, 0.32344)å¥½ä¸€ç‚¹äº†'
m.get_n_leaves(), len(xs)
'(12397, 404710)'
```

#### 5.4.2 å†³ç­–æ ‘ä¸­çš„åˆ†ç±»å˜é‡

- Pandas æœ‰ä¸€ä¸ª`get_dummies`æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œå®é™…ä¸Šå¹¶æ²¡æœ‰è¯æ®è¡¨æ˜è¿™ç§æ–¹æ³•ä¼šæ”¹å–„æœ€ç»ˆç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°½å¯èƒ½é¿å…ä½¿ç”¨å®ƒï¼Œå› ä¸ºå®ƒç¡®å®ä¼šä½¿æ‚¨çš„æ•°æ®é›†æ›´éš¾å¤„ç†ã€‚åªè¦å°†å®ƒå½“æˆæ­£å¸¸å˜é‡ä¸€èµ·å¤„ç†å³å¯ã€‚ï¼ˆä¸è¿‡åœ¨æ·±åº¦å­¦ä¹ æ—¶å°±ä¼šç”¨åˆ°ï¼‰

### 5.5 éšæœºæ£®æ—

- bagging

1.  éšæœºé€‰æ‹©æ•°æ®çš„å­é›†ï¼ˆå³â€œå­¦ä¹ é›†çš„è‡ªåŠ©å¤åˆ¶â€ï¼‰ã€‚
1.  ä½¿ç”¨è¿™ä¸ªå­é›†è®­ç»ƒæ¨¡å‹ã€‚
1.  ä¿å­˜è¯¥æ¨¡å‹ï¼Œç„¶åè¿”å›åˆ°æ­¥éª¤ 1 å‡ æ¬¡ã€‚
1.  è¿™å°†ä¸ºæ‚¨æä¾›å¤šä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡å‹ã€‚è¦è¿›è¡Œé¢„æµ‹ï¼Œè¯·ä½¿ç”¨æ‰€æœ‰æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œç„¶åå–æ¯ä¸ªæ¨¡å‹é¢„æµ‹çš„å¹³å‡å€¼ã€‚

#### 5.5.1 åˆ›å»ºéšæœºæ£®æ—

```python
from sklearn.ensemble import RandomForestRegressor #å› å˜é‡æ˜¯è¿ç»­çš„
def rf(xs, y, n_estimators=40, max_samples=200_000,
       max_features=0.5, min_samples_leaf=5, **kwargs):
    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,
        max_samples=max_samples, max_features=max_features,
        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)
# å¹¶è¡Œä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒåŠ é€Ÿè®­ç»ƒ
# æ ‘çš„æ•°é‡=40
# æ¯æ£µæ ‘ç”¨çš„æ ·æœ¬é‡=200000
# æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„æœ€å¤§ç‰¹å¾æ¯”ä¾‹=0.5
# æœ€å°å¶å­æ ·æœ¬æ•°=5
# å¯ç”¨ OOB ä¼°è®¡ï¼Œç”¨äºæ¨¡å‹è¯„ä¼°
m = rf(xs, y);
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)
'(0.170992, 0.233527)'
```

éšæœºæ£®æ—æœ€é‡è¦çš„ç‰¹æ€§ä¹‹ä¸€æ˜¯å®ƒå¯¹è¶…å‚æ•°é€‰æ‹©ä¸å¤ªæ•æ„Ÿ

#### 5.5.2 è¢‹å¤–è¯¯å·®

åœ¨éšæœºæ£®æ—ä¸­ï¼Œæ¯æ£µæ ‘çš„è®­ç»ƒæ•°æ®æ˜¯ä»åŸå§‹è®­ç»ƒé›†æœ‰æ”¾å›é‡‡æ ·ï¼ˆbootstrap samplingï¼‰å¾—åˆ°çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š

- æ¯æ£µæ ‘éƒ½ç”¨çš„æ˜¯è®­ç»ƒé›†çš„ä¸€ä¸ªâ€œæœ‰æ”¾å›æŠ½æ ·â€çš„å­é›†ï¼ˆçº¦63%çš„åŸå§‹æ ·æœ¬ï¼‰ã€‚
- å‰©ä¸‹æœªè¢«æŠ½ä¸­çš„çº¦37%æ ·æœ¬å°±å«åšâ€œè¢‹å¤–æ ·æœ¬â€ï¼ˆOut-Of-Bag samplesï¼‰ã€‚

1. å¯¹æ¯ä¸€ä¸ªæ ·æœ¬ xix_ixiï¼š
   - æ‰¾å‡ºé‚£äº›æ²¡æœ‰ä½¿ç”¨å®ƒè®­ç»ƒçš„æ ‘ï¼ˆä¹Ÿå°±æ˜¯å®ƒæ˜¯è¿™äº›æ ‘çš„è¢‹å¤–æ ·æœ¬ï¼‰ã€‚
   - ç”¨è¿™äº›æ ‘å¯¹ xix_ixi è¿›è¡Œé¢„æµ‹ã€‚
   - å–è¿™äº›é¢„æµ‹çš„å¹³å‡å€¼ï¼ˆå›å½’ï¼‰æˆ–æŠ•ç¥¨ï¼ˆåˆ†ç±»ï¼‰ä½œä¸º xix_ixi çš„æœ€ç»ˆé¢„æµ‹ã€‚
2. æŠŠæ‰€æœ‰æ ·æœ¬çš„çœŸå®å€¼å’Œè¢‹å¤–é¢„æµ‹å€¼å¯¹æ¯”ï¼Œè®¡ç®—æ•´ä½“è¯¯å·®ï¼Œä½œä¸º **OOBè¯¯å·®ä¼°è®¡**ã€‚

```python
r_mse(m.oob_prediction_, y)
'0.21091'
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„ OOB é”™è¯¯è¿œä½äºéªŒè¯é›†é”™è¯¯ï¼ˆ0.233527ï¼‰ã€‚è¿™æ„å‘³ç€é™¤äº†æ­£å¸¸çš„æ³›åŒ–é”™è¯¯ä¹‹å¤–ï¼Œè¿˜æœ‰å…¶ä»–åŸå› å¯¼è‡´äº†è¯¥é”™è¯¯ã€‚

### 5.6 æ¨¡å‹è§£é‡Š

å¯¹äºè¡¨æ ¼æ•°æ®ï¼Œæ¨¡å‹è§£é‡Šå°¤ä¸ºé‡è¦ã€‚å¯¹äºç»™å®šçš„æ¨¡å‹ï¼Œæˆ‘ä»¬æœ€æœ‰å…´è¶£çš„æ˜¯ä»¥ä¸‹å†…å®¹ï¼š

+   æˆ‘ä»¬å¯¹ä½¿ç”¨ç‰¹å®šæ•°æ®è¡Œè¿›è¡Œçš„é¢„æµ‹æœ‰å¤šè‡ªä¿¡ï¼Ÿâ€”â€”é¢„æµ‹ç½®ä¿¡åº¦

+   å¯¹äºä½¿ç”¨ç‰¹å®šæ•°æ®è¡Œè¿›è¡Œé¢„æµ‹ï¼Œæœ€é‡è¦çš„å› ç´ æ˜¯ä»€ä¹ˆï¼Œå®ƒä»¬å¦‚ä½•å½±å“è¯¥é¢„æµ‹ï¼Ÿâ€”â€”ç‰¹å¾é‡è¦æ€§+æ ‘è§£é‡Šå™¨

+   å“ªäº›åˆ—æ˜¯æœ€å¼ºçš„é¢„æµ‹å› å­ï¼Œå“ªäº›å¯ä»¥å¿½ç•¥ï¼Ÿâ€”â€”ç‰¹å¾é‡è¦æ€§

+   å“ªäº›åˆ—åœ¨é¢„æµ‹ç›®çš„ä¸Šå®é™…ä¸Šæ˜¯å¤šä½™çš„ï¼Ÿâ€”â€”ç‰¹å¾é‡è¦æ€§

+   å½“æˆ‘ä»¬æ”¹å˜è¿™äº›åˆ—æ—¶ï¼Œé¢„æµ‹ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿâ€”â€”éƒ¨åˆ†ä¾èµ–

#### 5.6.1 æ ‘æ–¹å·®â€”â€”é¢„æµ‹ç½®ä¿¡åº¦

ä½¿ç”¨æ ‘ä¹‹é—´é¢„æµ‹çš„æ ‡å‡†å·®ï¼Œè€Œä¸ä»…ä»…æ˜¯å‡å€¼ï¼Œè¿™å‘Šè¯‰æˆ‘ä»¬é¢„æµ‹çš„*ç›¸å¯¹*ç½®ä¿¡åº¦ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬ä¼šæ›´è°¨æ…åœ°ä½¿ç”¨æ ‘ç»™å‡ºéå¸¸ä¸åŒç»“æœçš„è¡Œçš„ç»“æœï¼ˆæ›´é«˜çš„æ ‡å‡†å·®ï¼‰ï¼Œè€Œä¸æ˜¯åœ¨æ ‘æ›´ä¸€è‡´çš„æƒ…å†µä¸‹ä½¿ç”¨ç»“æœï¼ˆæ›´ä½çš„æ ‡å‡†å·®ï¼‰ã€‚å¦‚ä¸‹ï¼š

```python
#ç½®ä¿¡åº¦
preds = np.stack([t.predict(valid_xs) for t in m.estimators_])
print(preds.shape)
preds_std = preds.std(0)
preds_std[:5]
'''
array([0.30618819, 0.13729507, 0.09630049, 0.25064758, 0.12353961])
ç¬¬ä¸€ä¸ªç½®ä¿¡åº¦ä½ï¼Œç¬¬ä¸‰ä¸ªç½®ä¿¡åº¦é«˜
'''
```

#### 5.6.2 ç‰¹å¾é‡è¦æ€§

##### ï¼ˆ1ï¼‰ç‰¹å¾è´¡çŒ®åº¦

**â€æˆ‘ä»¬å¯ä»¥ç›´æ¥ä» sklearn çš„éšæœºæ£®æ—ä¸­è·å–è¿™äº›ä¿¡æ¯ï¼Œæ–¹æ³•æ˜¯æŸ¥çœ‹`feature_importances_`å±æ€§**ã€‚å¦‚ä¸‹ï¼š

```python
#é‡è¦æ€§
def rf_feat_importance(m, df):
    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}
                       ).sort_values('imp', ascending=False)
fi = rf_feat_importance(m, xs)
#è¡¨æ ¼å½¢å¼
fi[:10]
#ç»˜å›¾å½¢å¼
def plot_fi(fi):
    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)
plot_fi(fi[:30]);
```

![](D:\Git\a\Path-Records\img\dlcf_09in05.png)

ç‰¹å¾é‡è¦æ€§çš„è®¡ç®—ï¼š

- åŸºäºèŠ‚ç‚¹çº¯åº¦å¢ç›Šï¼ˆGini impurity / MSE å‡å°‘ï¼‰ï¼ˆè¿™æ˜¯ Scikit-learn ä¸­é»˜è®¤çš„æ–¹æ³•ï¼Œä¸Šå›¾ï¼‰

  å¦‚æœä¸€ä¸ªç‰¹å¾åœ¨åˆ†è£‚èŠ‚ç‚¹æ—¶å¸¦æ¥äº†è¾ƒå¤§çš„çº¯åº¦æå‡ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰æˆ–æ–¹å·®å‡å°‘ï¼ˆå›å½’ä»»åŠ¡ï¼‰ï¼Œè¯´æ˜å®ƒå¯¹æ¨¡å‹æ›´é‡è¦ã€‚

  - åˆ†ç±»ä»»åŠ¡ä¸­ï¼šç‰¹å¾é‡è¦æ€§ = è¯¥ç‰¹å¾åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šå¸¦æ¥çš„çº¯åº¦æå‡çš„æ€»å’Œï¼ˆåŠ æƒå¹³å‡ï¼‰
  - å›å½’ä»»åŠ¡ä¸­ï¼šç‰¹å¾é‡è¦æ€§ = æ¯æ¬¡ä½¿ç”¨è¯¥ç‰¹å¾åˆ†è£‚æ—¶ï¼Œè®­ç»ƒè¯¯å·®ï¼ˆMSEï¼‰ä¸‹é™çš„æ€»å’Œï¼ˆåŠ æƒï¼‰
  - åŠ æƒè¯´æ˜ï¼šæ¯æ¬¡åˆ†è£‚å¯¹ impurity çš„æ”¹å–„ï¼Œä¼šæ ¹æ®å½“å‰èŠ‚ç‚¹çš„æ ·æœ¬æ•°é‡è¿›è¡ŒåŠ æƒï¼ˆæ ·æœ¬å¤šåˆ™æ›´é‡è¦ï¼‰ã€‚

- åŸºäºâ€œè¢‹å¤–è¯¯å·®å¢åŠ â€è®¡ç®—ï¼ˆPermutation Importanceï¼Œæ‰“ä¹±æ³•ï¼‰ï¼ˆè¿™ä¸ªæ–¹æ³•ä¸æ˜¯é»˜è®¤çš„ï¼Œä½†æ›´ç¨³å®šã€æ›´å…·è§£é‡Šæ€§ï¼‰

  - ç”¨åŸå§‹æ•°æ®è®­ç»ƒå¥½éšæœºæ£®æ—ã€‚
  - è®°å½•è¢‹å¤–ï¼ˆOOBï¼‰æ ·æœ¬çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚
  - å¯¹æŸä¸ªç‰¹å¾åˆ—éšæœºæ‰“ä¹±é¡ºåºï¼ˆç ´åå…¶ä¸ç›®æ ‡çš„å…³è”ï¼‰ã€‚
  - å†æ¬¡è®¡ç®—è¢‹å¤–æ ·æœ¬çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚
  - é‡è¦æ€§ = å‡†ç¡®ç‡ä¸‹é™å¹…åº¦

- SHAPï¼ˆå¤–éƒ¨åº“-åŒ…å«ä½†ä¸é™äºæ ‘æ¨¡å‹å¦‚ï¼šXGBoostã€éšæœºæ£®æ—RFï¼‰

  - å‡è®¾åªæœ‰ 3 ä¸ªç‰¹å¾ï¼šAã€Bã€Cï¼Œæˆ‘ä»¬è¦è®¡ç®—ç‰¹å¾ A çš„ SHAP å€¼ï¼Œéœ€è¦åšï¼šåˆ—ä¸¾æ‰€æœ‰åŒ…å« A çš„æ’åˆ—ç»„åˆï¼ˆæ¯”å¦‚ A, BA, CA, BCA, etc.ï¼‰ï¼›çœ‹ A åŠ å…¥æ¯ä¸ªç»„åˆæ—¶ï¼Œé¢„æµ‹å€¼æ”¹å˜äº†å¤šå°‘ï¼Œæ¯”å¦‚ï¼šé¢„æµ‹(BC) = 100, é¢„æµ‹(ABC) = 130 â†’ A çš„è¾¹é™…è´¡çŒ® = 30ï¼›å¯¹æ‰€æœ‰ç»„åˆçš„è¾¹é™…è´¡çŒ®æ±‚å¹³å‡ï¼Œè¿™æ ·ä½ å°±å¾—åˆ°äº† A çš„ Shapley å€¼ã€‚

  - è´¡çŒ®å¤§â‰ é¢„æµ‹å‡†ï¼ŒSHAP è¡¡é‡çš„æ˜¯å½±å“åŠ›è€Œä¸æ˜¯æ­£ç¡®æ€§ã€‚å®ƒå›ç­”çš„é—®é¢˜æ˜¯ï¼šâ€œå¦‚æœæ²¡æœ‰è¿™ä¸ªç‰¹å¾ï¼Œè¿™æ¬¡é¢„æµ‹ä¼šæœ‰å¤šå¤§ä¸åŒï¼Ÿâ€

  - æ­£å€¼ SHAPï¼šè¯¥ç‰¹å¾å°†é¢„æµ‹ç»“æœæ¨é«˜äº†ã€‚

    è´Ÿå€¼ SHAPï¼šè¯¥ç‰¹å¾å°†é¢„æµ‹ç»“æœå‹ä½äº†ã€‚

    ç»å¯¹å€¼å¤§ï¼šä»£è¡¨è¯¥ç‰¹å¾å¯¹å½“å‰é¢„æµ‹çš„å½±å“è¶Šå¼ºã€‚

    ç»å¯¹å€¼å°æˆ–0ï¼šä»£è¡¨è¯¥ç‰¹å¾å¯¹å½“å‰é¢„æµ‹å½±å“å¾ˆå¼±ï¼Œç”šè‡³å‡ ä¹æ²¡å‚ä¸ã€‚

##### ï¼ˆ2ï¼‰é€‰æ‹©é‡è¦ç‰¹å¾

**â€æœ‰äº†ç‰¹å¾çš„é‡è¦æ€§ä¹‹åï¼Œä¸ºäº†é¿å…ä½¿ç”¨å¤ªå¤šç‰¹å¾è¿›è¡Œæ‹Ÿåˆï¼Œå¯ä»¥é€‰æ‹©æ¯”è¾ƒé‡è¦çš„ç‰¹å¾é‡æ–°æ„å»ºéšæœºæ£®æ—**ï¼š

```python
to_keep = fi[fi.imp>0.005].cols
len(to_keep) '21'
xs_imp = xs[to_keep]
valid_xs_imp = valid_xs[to_keep]
m = rf(xs_imp,y)
m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)
'(0.181079, 0.230649)'
'ä¸ç”¨æ‰€æœ‰ç‰¹å¾çš„ç»“æœ(0.170992, 0.233527)ç›¸æ¯”ï¼Œå¥½åƒä¹Ÿæ²¡æœ‰æ€§èƒ½ä¸‹é™å¤šå°‘ï¼Œä½†æ˜¯å¯ä»¥å‡å°‘å¾ˆå¤šè®¡ç®—é‡å’Œç‰¹å¾éœ€æ±‚'
plot_fi(rf_feat_importance(m, xs_imp));
```

![](D:\Git\a\Path-Records\img\dlcf_09in06.png)

##### ï¼ˆ3ï¼‰è¯†åˆ«ç‰¹å¾é—´ç›¸ä¼¼åº¦

**â€å»é™¤å†—ä½™ç‰¹å¾ï¼šæœ‰ä¸€äº›ç‰¹å¾ç‰¹åˆ«ç›¸ä¼¼ï¼Œæ¯”å¦‚ProductGroupå’ŒProductGroupDescï¼Œåº”è¯¥å»é™¤è¿™äº›å†—ä½™çš„ç‰¹å¾ã€‚**

```python
#ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§
import seaborn as sns
from scipy.cluster.hierarchy import linkage
from scipy.spatial.distance import squareform

def cluster_columns(df):
    corr = df.corr()  # è®¡ç®—ç›¸å…³çŸ©é˜µ
    d = 1 - corr.abs()  # è®¡ç®—è·ç¦»çŸ©é˜µï¼ˆè¶Šç›¸å…³è·ç¦»è¶Šå°ï¼‰
    dist_linkage = linkage(squareform(d), method='average')  # å±‚æ¬¡èšç±»
    sns.clustermap(corr, row_linkage=dist_linkage, col_linkage=dist_linkage,
                   cmap='coolwarm', center=0, figsize=(10,10))
    plt.show()
cluster_columns(xs_imp)
```

![](D:\Git\a\Path-Records\img\dlcf_09in07.png)

å¯ä»¥çœ‹åˆ°ï¼Œçº¢è‰²çš„éƒ¨åˆ†å°±æ˜¯ç–¯ç‹‚ç›¸å¹²çš„ç‰¹å¾ï¼Œé‚£ä¹ˆæŠŠè¿™äº›å¯†åˆ‡ç›¸å…³çš„ç‰¹å¾ã€‚

##### ï¼ˆ4ï¼‰æ£€æŸ¥æ¨¡å‹æ•ˆæœ

**â€è®©æˆ‘ä»¬å°è¯•åˆ é™¤ä¸€äº›è¿™äº›å¯†åˆ‡ç›¸å…³ç‰¹å¾ï¼Œçœ‹çœ‹æ¨¡å‹æ˜¯å¦å¯ä»¥ç®€åŒ–è€Œä¸å½±å“å‡†ç¡®æ€§ã€‚**

```python
#æŸ¥çœ‹å»é™¤å…±çº¿ç‰¹å¾çš„æ•ˆæœ
def get_oob(df):
    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,
        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)
    m.fit(df, y)
    return m.oob_score_  #å¯¹æ‹Ÿåˆéšæœºæ£®æ—æ¥è¯´ï¼Œobb_score_æ˜¯RÂ²å†³å®šç³»æ•°ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½
get_oob(xs_imp) #åŸºçº¿
'0.8761015614269996'
{c:get_oob(xs_imp.drop(c, axis=1)) for c in (
    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',
    'fiModelDesc', 'fiBaseModel',
    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}
'''
{'saleYear': 0.8759547835051612,
 'saleElapsed': 0.8731632755222668,
 'ProductGroupDesc': 0.8773314254721,
 'ProductGroup': 0.8774958058240945,
 'fiModelDesc': 0.8755700975380052,
 'fiBaseModel': 0.8757558713831249,
 'Hydraulics_Flow': 0.877585297419066,
 'Grouser_Tracks': 0.8773646824594399,
 'Coupler_System': 0.877924411741774}
'''
to_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']
get_oob(xs_imp.drop(to_drop, axis=1)) #æ›´æ–°
'0.8745177203238274'
xs_final = xs_imp.drop(to_drop, axis=1)
valid_xs_final = valid_xs_imp.drop(to_drop, axis=1)
m = rf(xs_final, y)
m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)
'(0.182724, 0.233131)ï¼›åœ¨å»é™¤å…±çº¿ç‰¹å¾å‰æ˜¯(0.180774, 0.230802)ï¼›ç”¨æ‰€æœ‰ç‰¹å¾çš„ç»“æœ(0.170992, 0.233527)'
'''
Index(['YearMade', 'ProductSize', 'Coupler_System', 'fiProductClassDesc',
       'ModelID', 'saleElapsed', 'Hydraulics_Flow', 'fiSecondaryDesc',
       'Enclosure', 'ProductGroup', 'fiModelDesc', 'SalesID', 'MachineID',
       'Hydraulics', 'fiModelDescriptor', 'Drive_System'],
      dtype='object')
'''
```

| å±æ€§            | æ˜¯å¦åŸºäºè¢‹å¤–æ ·æœ¬ | ç”¨é€”                                         | ä¸¾ä¾‹                                    |
| --------------- | ---------------- | -------------------------------------------- | --------------------------------------- |
| oob_prediction_ | æ˜¯               | è®°å½•æ¯ä¸ªæ ·æœ¬åœ¨æœªè¢«ç”¨äºè®­ç»ƒçš„æ ‘ä¸­å¾—åˆ°çš„é¢„æµ‹å€¼ | ä¸€ä¸ªæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ ·æœ¬çš„ OOB é¢„æµ‹ |
| oob_score_      | æ˜¯               | è¡¡é‡æ•´ä½“æ¨¡å‹æ€§èƒ½ï¼Œåœ¨è¢‹å¤–æ ·æœ¬ä¸Šçš„å¾—åˆ†         | é€šå¸¸æ˜¯å›å½’ä¸­çš„ RÂ² æˆ–åˆ†ç±»å‡†ç¡®ç‡          |

#### 5.6.3 éƒ¨åˆ†ä¾èµ–-PDPå›¾

éƒ¨åˆ†ä¾èµ–å›¾è¯•å›¾å›ç­”è¿™ä¸ªé—®é¢˜ï¼šå¦‚æœä¸€è¡Œé™¤äº†å…³æ³¨çš„ç‰¹å¾ä¹‹å¤–æ²¡æœ‰å˜åŒ–ï¼Œå®ƒä¼šå¦‚ä½•å½±å“å› å˜é‡ï¼Ÿ

å·²çŸ¥'YearMade', 'ProductSize'æ˜¯éå¸¸é‡è¦çš„ç‰¹å¾ï¼Œå…ˆçœ‹ä¸€ä¸‹è¿™ä¸¤ä¸ªç‰¹å¾çš„æƒ…å†µï¼š

```python
p = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()
c = to.classes['ProductSize']
plt.yticks(range(len(c)), c);
ax = valid_xs_final['YearMade'].hist()
```

ç„¶åç»˜åˆ¶åä¾èµ–å›¾PDPå›¾ï¼Œä»¥YearMadeä¸ºä¾‹ï¼Œæˆ‘ä»¬å°†YearMadeåˆ—ä¸­çš„æ¯ä¸ªå€¼æ›¿æ¢ä¸º 1950ï¼Œç„¶åè®¡ç®—æ¯ä¸ªæ‹å–å“çš„é¢„æµ‹é”€å”®ä»·æ ¼ï¼Œå¹¶å¯¹æ‰€æœ‰æ‹å–å“è¿›è¡Œå¹³å‡ã€‚ç„¶åæˆ‘ä»¬å¯¹ 1951ã€1952 ç­‰å¹´ä»½åšåŒæ ·çš„æ“ä½œï¼Œç›´åˆ°æˆ‘ä»¬çš„æœ€ç»ˆå¹´ä»½ 2011ï¼š

```python
from sklearn.inspection import PartialDependenceDisplay
PartialDependenceDisplay.from_estimator(m,valid_xs_final,features=['YearMade','ProductSize'],kind='average').figure_.set_size_inches(12,4)
```

![](D:\Git\a\Path-Records\img\dlcf_09in10.png)

YearMadeè¿˜æ˜¯è›®åˆç†çš„ï¼Œ90å¹´åæ‰æ˜¯æ•°æ®ä¸»è¦é›†ä¸­çš„ä½ç½®ï¼Œæ¯”è¾ƒç¬¦åˆå¸¸è¯†ï¼›ProductSizeçš„éƒ¨åˆ†å›¾æœ‰ç‚¹ä»¤äººæ‹…å¿§ã€‚å®ƒæ˜¾ç¤ºæˆ‘ä»¬çœ‹åˆ°çš„æœ€ç»ˆç»„ï¼Œå³ç¼ºå¤±å€¼ï¼Œä»·æ ¼æœ€ä½ã€‚è¦åœ¨å®è·µä¸­ä½¿ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾å‡ºä¸ºä»€ä¹ˆå®ƒç»å¸¸ç¼ºå¤±ä»¥åŠè¿™æ„å‘³ç€ä»€ä¹ˆã€‚ç¼ºå¤±å€¼æœ‰æ—¶å¯ä»¥æ˜¯æœ‰ç”¨çš„é¢„æµ‹å› å­-è¿™å®Œå…¨å–å†³äºå¯¼è‡´å®ƒä»¬ç¼ºå¤±çš„åŸå› ã€‚ç„¶è€Œï¼Œæœ‰æ—¶å®ƒä»¬å¯èƒ½è¡¨æ˜æ•°æ®æ³„æ¼ã€‚

- **æ•°æ®æ³„éœ²**

å…³äºæ•°æ®æŒ–æ˜é—®é¢˜çš„ç›®æ ‡çš„ä¿¡æ¯å¼•å…¥ï¼Œè¿™äº›ä¿¡æ¯ä¸åº”è¯¥åˆæ³•åœ°ä»ä¸­æŒ–æ˜å‡ºæ¥ã€‚æ³„æ¼çš„ä¸€ä¸ªå¾®ä¸è¶³é“çš„ä¾‹å­æ˜¯ä¸€ä¸ªæ¨¡å‹å°†ç›®æ ‡æœ¬èº«ç”¨ä½œè¾“å…¥ï¼Œå› æ­¤å¾—å‡ºä¾‹å¦‚â€œé›¨å¤©ä¸‹é›¨â€çš„ç»“è®ºã€‚å®é™…ä¸Šï¼Œå¼•å…¥è¿™ç§éæ³•ä¿¡æ¯æ˜¯æ— æ„çš„ï¼Œå¹¶ä¸”ç”±æ•°æ®æ”¶é›†ã€èšåˆå’Œå‡†å¤‡è¿‡ç¨‹ä¿ƒæˆã€‚

è¯†åˆ«æ•°æ®æ³„æ¼æœ€å®ç”¨å’Œç®€å•æ–¹æ³•ï¼Œå³æ„å»ºæ¨¡å‹ï¼Œç„¶åæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

â‘  æ£€æŸ¥æ¨¡å‹çš„å‡†ç¡®æ€§æ˜¯å¦è¿‡äºå®Œç¾ã€‚

â‘¡ å¯»æ‰¾åœ¨å®è·µä¸­ä¸åˆç†çš„é‡è¦é¢„æµ‹å› å­ã€‚

â‘¢ å¯»æ‰¾åœ¨å®è·µä¸­ä¸åˆç†çš„éƒ¨åˆ†ä¾èµ–å›¾ç»“æœã€‚

- **é€šå¸¸å…ˆæ„å»ºæ¨¡å‹ï¼Œç„¶åè¿›è¡Œæ•°æ®æ¸…ç†æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œè€Œä¸æ˜¯åè¿‡æ¥ã€‚æ¨¡å‹å¯ä»¥å¸®åŠ©æ‚¨è¯†åˆ«æ½œåœ¨çš„æ•°æ®é—®é¢˜ã€‚**

å®ƒè¿˜å¯ä»¥å¸®åŠ©æ‚¨ç¡®å®šå“ªäº›å› ç´ å½±å“ç‰¹å®šé¢„æµ‹ï¼Œä½¿ç”¨æ ‘è§£é‡Šå™¨ã€‚

#### 5.6.4 æ ‘è§£é‡Šå™¨

å¯¹äºé¢„æµ‹ç‰¹å®šæ•°æ®è¡Œï¼Œæœ€é‡è¦çš„å› ç´ æ˜¯ä»€ä¹ˆï¼Œå®ƒä»¬å¦‚ä½•å½±å“è¯¥é¢„æµ‹ï¼Ÿæˆ‘ä»¬éœ€è¦ä½¿ç”¨*treeinterpreter*åº“ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨*waterfallcharts*åº“æ¥ç»˜åˆ¶ç»“æœå›¾è¡¨ã€‚

å‡è®¾æˆ‘ä»¬æ­£åœ¨æŸ¥çœ‹æ‹å–ä¸­çš„ç‰¹å®šç‰©å“ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯èƒ½é¢„æµ‹è¿™ä¸ªç‰©å“ä¼šéå¸¸æ˜‚è´µï¼Œæˆ‘ä»¬æƒ³çŸ¥é“åŸå› ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å–å‡ºé‚£ä¸€è¡Œæ•°æ®å¹¶å°†å…¶é€šè¿‡ç¬¬ä¸€æ£µå†³ç­–æ ‘ï¼ŒæŸ¥çœ‹æ ‘ä¸­æ¯ä¸ªç‚¹å¤„ä½¿ç”¨çš„åˆ†å‰²ã€‚å¯¹äºæ¯ä¸ªåˆ†å‰²ï¼Œæˆ‘ä»¬æ‰¾åˆ°ç›¸å¯¹äºæ ‘çš„çˆ¶èŠ‚ç‚¹çš„å¢åŠ æˆ–å‡å°‘ã€‚æˆ‘ä»¬å¯¹æ¯æ£µæ ‘éƒ½è¿™æ ·åšï¼Œå¹¶å°†æ¯ä¸ªåˆ†å‰²å˜é‡çš„é‡è¦æ€§å˜åŒ–ç›¸åŠ ã€‚

- é¦–å…ˆè®¡ç®—contributionsï¼Œå…¶å®å°±æ˜¯æœ‰ä¸ªåŸºç¡€å€¼biasï¼Œç‰¹å¾1å¢åŠ x1ï¼Œç‰¹å¾2å¢åŠ x2...æœ€ååŠ åœ¨ä¸€èµ·å¾—åˆ°prediction

```python
!pip install treeinterpreter
from treeinterpreter import treeinterpreter
row = valid_xs_final.iloc[:5]
prediction,bias,contributions = treeinterpreter.predict(m, row.values)
#predictionåªæ˜¯éšæœºæ£®æ—çš„é¢„æµ‹ã€‚biasæ˜¯æ¨¡å‹é¢„æµ‹çš„åŸºç¡€å€¼ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•ç‰¹å¾ä¿¡æ¯æ—¶çš„å¹³å‡é¢„æµ‹å€¼
#contributionsï¼šå®ƒå‘Šè¯‰æˆ‘ä»¬ç”±äºæ¯ä¸ªç‹¬ç«‹å˜é‡çš„å˜åŒ–è€Œå¯¼è‡´çš„é¢„æµ‹æ€»å˜åŒ–
#å¯¹äºæŸä¸€è¡Œï¼Œcontributions+bias=prediction
```

- é‚£ä¹ˆæ¯ä¸ªç‰¹å¾ä½¿å¾—predictionå‘ç”Ÿäº†æ€æ ·çš„å˜åŒ–å‘¢ï¼Ÿç”»ç€‘å¸ƒå›¾

```python
import plotly.graph_objects as go
fig = go.Figure(go.Waterfall(
    name = "aaaaa", orientation = "v",
    x = valid_xs_final.columns,
    textposition = "outside",
    increasing=dict(marker=dict(color="#4C72B0")),   # è“è‰²ï¼šå¢åŠ 
    decreasing=dict(marker=dict(color="#DD8452")),   # æ©™æ£•è‰²ï¼šå‡å°‘
    totals=dict(marker=dict(color="#6C757D")),
    text = contributions[0].round(3),
    y = contributions[0],
    connector = {"line":{"color":"black", "width":1}}
))
fig.add_shape(
    type='line',
    x0=0, x1=1, xref='paper',
    y0=0.08, y1=0.08,
    line=dict(color='gray', dash='dash')
)
fig.update_layout(
    #title="ç€‘å¸ƒå›¾",
    #xanchor='center',
    #title_font=dict(size=14, family="DejaVu Sans", color='black'),
    #font=dict(size=12, family="DejaVu Sans", color='black'),
    plot_bgcolor='white',
    showlegend=False,
    paper_bgcolor='white'
)
fig.update_xaxes(
    #title_text="ç‰¹å¾",
    title_font=dict(size=12),
    tickfont=dict(size=10),
    showline=True,
    linewidth=1,
    linecolor='black')
fig.update_yaxes(title_text="Contribution to Prediction",
    title_font=dict(size=12),
    tickfont=dict(size=10),
    showline=True,
    linewidth=1,
    linecolor='black',
    zeroline=True,
    zerolinecolor='gray',
    zerolinewidth=0.5,
    range=[-0.5, 0.1])
fig.show()
```

![](D:\Git\a\Path-Records\img\dlcf_09in11.png)

è¿™ç§ä¿¡æ¯åœ¨ç”Ÿäº§ä¸­æœ€æœ‰ç”¨ï¼Œè€Œä¸æ˜¯åœ¨æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å®ƒä¸ºæ•°æ®äº§å“çš„ç”¨æˆ·æä¾›æœ‰å…³é¢„æµ‹èƒŒåçš„åŸºæœ¬æ¨ç†çš„æœ‰ç”¨ä¿¡æ¯ã€‚

### 5.7 å¤–æ¨å’Œç¥ç»ç½‘ç»œ

#### 5.7.1 å¤–æ¨é—®é¢˜

![](D:\Git\a\Path-Records\img\dlcf_09in13.png)

å¦‚å›¾ï¼Œç”¨å‰é¢éƒ¨åˆ†åšéšæœºæ£®æ—é¢„æµ‹åé¢éƒ¨åˆ†ï¼Œå°±ä¼šå‘ç°æœ‰å¾ˆå¤§çš„é—®é¢˜ï¼Œè¿™å°±æ˜¯éšæœºæ£®æ—æ— æ³•å¯¹å…¶æœªè§è¿‡çš„æ•°æ®ç±»å‹è¿›è¡Œå¤–æ¨ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬çš„éªŒè¯é›†ä¸åŒ…å«åŸŸå¤–æ•°æ®ã€‚

#### 5.7.2 æŸ¥æ‰¾åŸŸå¤–æ•°æ®

æˆ‘ä»¬å°è¯•é¢„æµ‹ä¸€è¡Œæ˜¯åœ¨éªŒè¯é›†è¿˜æ˜¯è®­ç»ƒé›†ä¸­ï¼Œæ¥éªŒè¯æµ‹è¯•é›†æ˜¯å¦ä¸è®­ç»ƒæ•°æ®ä»¥ç›¸åŒæ–¹å¼åˆ†å¸ƒã€‚

```python
df_dom = pd.concat([xs_final, valid_xs_final])
is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))
m = rf(df_dom, is_valid)
rf_feat_importance(m, df_dom)
```

|      | cols        | imp      |
| ---- | ----------- | -------- |
| 5    | saleElapsed | 0.859446 |
| 9    | SalesID     | 0.119325 |
| 13   | MachineID   | 0.014259 |
| 0    | YearMade    | 0.001793 |
| 8    | fiModelDesc | 0.001740 |
| 11   | Enclosure   | 0.000657 |

è¿™æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ä¹‹é—´æœ‰ä¸‰åˆ—æ˜¾ç€ä¸åŒï¼šsaleElapsedã€SalesID å’Œ MachineIDã€‚ç°åœ¨ä¾æ¬¡å°†å®ƒä»¬å»æ‰ï¼Œçœ‹çœ‹å¯¹æ¨¡å‹çš„å½±å“ï¼š

```python
m = rf(xs_final, y)
print('orig', m_rmse(m, valid_xs_final, valid_y))
for c in ('SalesID','saleElapsed','MachineID'):
    m = rf(xs_final.drop(c,axis=1), y)
    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))
'''
orig 0.23322
SalesID 0.230832
saleElapsed 0.236452
MachineID 0.231546
'''
```

å»æ‰SalesIDå’ŒMachineIDï¼Œé‡æ–°è®­ç»ƒæ¨¡å‹ï¼š

```python
time_vars = ['SalesID','MachineID']
xs_final_time = xs_final.drop(time_vars, axis=1)
valid_xs_time = valid_xs_final.drop(time_vars, axis=1)
m = rf(xs_final_time, y)
m_rmse(m, valid_xs_time, valid_y)
'0.231307æ¨¡å‹è¡¨ç°æé«˜äº†'
```

- æˆ‘ä»¬å»ºè®®å¯¹æ‰€æœ‰æ•°æ®é›†å°è¯•æ„å»ºä¸€ä¸ªä»¥ is_valid ä¸ºå› å˜é‡çš„æ¨¡å‹ï¼Œå°±åƒæˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„é‚£æ ·ã€‚å®ƒé€šå¸¸å¯ä»¥æ­ç¤ºæ‚¨å¯èƒ½ä¼šå¿½ç•¥çš„å¾®å¦™çš„é¢†åŸŸè½¬ç§»é—®é¢˜ã€‚

- é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸ä½¿ç”¨æ—§æ•°æ®ï¼Œä»¥å…æ—¶ä»£å˜åŒ–å¯¼è‡´é¢„æµ‹ä¸åŒï¼šè¿™è¡¨æ˜æ‚¨ä¸åº”è¯¥æ€»æ˜¯ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ï¼›æœ‰æ—¶å€™å­é›†å¯èƒ½æ›´å¥½ã€‚

```python
filt = xs['saleYear']>2004
xs_filt = xs_final_time[filt]
y_filt = y[filt]
m = rf(xs_filt, y_filt)
m_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)
'(0.177078, 0.229417)ï¼Œç›¸æ¯”äºå»é™¤å…±çº¿æ€§ä¹‹å(0.182724, 0.233131)ï¼›åœ¨å»é™¤å…±çº¿ç‰¹å¾å‰æ˜¯(0.180774, 0.230802)ï¼›ç”¨æ‰€æœ‰ç‰¹å¾çš„ç»“æœ(0.170992, 0.233527)æé«˜ä¸”è¾“å…¥æ›´å°‘äº†'
```

#### 5.7.3 ä½¿ç”¨ç¥ç»ç½‘ç»œ

åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œå¤„ç†åˆ†ç±»å˜é‡çš„ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨åµŒå…¥ã€‚ä¸ºäº†åˆ›å»ºåµŒå…¥ï¼Œfastai éœ€è¦ç¡®å®šå“ªäº›åˆ—åº”è¯¥è¢«è§†ä¸ºåˆ†ç±»å˜é‡ã€‚åµŒå…¥å¤§å°å¤§äº 10,000 é€šå¸¸åªåº”åœ¨æµ‹è¯•æ˜¯å¦æœ‰æ›´å¥½çš„æ–¹æ³•æ¥åˆ†ç»„å˜é‡ä¹‹åä½¿ç”¨ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ 9,000 ä½œä¸ºæˆ‘ä»¬çš„ max_card å€¼ã€‚

```python
#æ•°æ®å‡†å¤‡
df_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)
df_nn['ProductSize'] = df_nn['ProductSize'].astype('category')
df_nn['ProductSize'] = df_nn['ProductSize'].cat.set_categories(sizes, ordered=True)
df_nn[dep_var] = np.log(df_nn[dep_var])
df_nn = add_datepart(df_nn, 'saledate')
df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]
cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)
#max_cardæœ€å¤§åˆ†ç±»æ•°ï¼ˆcardinalityï¼‰ï¼Œfastai é»˜è®¤æ˜¯ 20ï¼Œè¶…è¿‡è¿™ä¸ªå€¼å°±è®¤ä¸ºæ˜¯è¿ç»­å˜é‡ï¼Œè¿™é‡Œè®¾ç½®ä¸º 9000 è¡¨ç¤ºå‡ ä¹ä¸é™åˆ¶
#åŒä¿é™©ï¼ŒsaleElapsedä¸€å®šä¸èƒ½æ˜¯åˆ†ç±»å˜é‡ï¼Œè¦å°†å®ƒä½œä¸ºè¿ç»­å˜é‡
#cont_nn.append('saleElapsed')
#cat_nn.remove('saleElapsed')
df_nn_final[cat_nn].nunique() #å”¯ä¸€å€¼çš„ä¸ªæ•°
'''
YearMade                73
ProductSize              6
Coupler_System           2
fiProductClassDesc      74
ModelID               5281
fiSecondaryDesc        177
Hydraulics_Flow          3
fiModelDesc           5059
Enclosure                6
ProductGroup             6
fiModelDescriptor      140
Hydraulics              12
Tire_Size               17
Drive_System             4
dtype: int64
ModelIDå’ŒfiModelDescå¯èƒ½ç›¸ä¼¼å†—ä½™ï¼Œåˆ é™¤å…¶ä¸­ä¸€ä¸ªçœ‹å¯¹éšæœºæ£®æ—çš„å½±å“
'''
xs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)
valid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)
m2 = rf(xs_filt2, y_filt)
m_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)
cat_nn.remove('fiModelDescriptor')
'ä¸çŸ¥é“ä¸ºå•¥æœ€åæ˜¯fiModelDescriptorï¼Œ(0.178941, 0.23032)ï¼Œå’Œä¹‹å‰(0.177078, 0.229417)å·®å¼‚ä¸å¤§ï¼Œå¯ä»¥æŠŠå®ƒå»æ‰'
procs_nn = [Categorify, FillMissing, Normalize]
to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,
                      splits=splits, y_names=dep_var)
dls = to_nn.dataloaders(1024)
```

è¿™æ®µæ˜¯å¯¹æ¯”è€Œå·²

```python
#éšæœºæ£®æ—
procs = [Categorify, FillMissing]
dep_var = 'SalePrice'
cont,cat = cont_cat_split(df, 1, dep_var=dep_var) #contè¿”å›çš„æ˜¯è¿ç»­è‡ªå˜é‡çš„ç‰¹å¾å
to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) #æ‹†åˆ†æˆtrainå’Œvalidäº†
#ç¥ç»ç½‘ç»œ
procs_nn = [Categorify, FillMissing, Normalize]
cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)
to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var)
dls = to_nn.dataloaders(1024)
```

æ­£å¦‚æˆ‘ä»¬è®¨è®ºè¿‡çš„ï¼Œä¸ºå›å½’æ¨¡å‹è®¾ç½®y_rangeæ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬æ‰¾åˆ°æˆ‘ä»¬å› å˜é‡çš„æœ€å°å€¼å’Œæœ€å¤§å€¼

```python
y = to_nn.train.y
y.min(),y.max()
'(8.465899467468262, 11.863582611083984)'
from fastai.tabular.all import *
learn = tabular_learner(dls, y_range=(8,12), layers=[500,250], n_out=1, loss_func=F.mse_loss)
learn.lr_find()
'SuggestedLRs(valley=0.0014454397605732083)'
```

![](D:\Git\a\Path-Records\img\dlcf_09in14.png)

```python
learn.fit_one_cycle(5, 1e-2)
preds,targs = learn.get_preds()
r_mse(preds,targs)
'0.22728,å¯¹æ¯”éšæœºæ£®æ—(0.178941, 0.23032)ï¼Œå¥½ä¸€äº›'
learn.save('nn')
```

### 5.8 æé«˜æ¨¡å‹è¡¨ç°çš„æ–¹æ³•

#### 5.8.1 é›†æˆEnsembling

åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªéå¸¸ä¸åŒçš„æ¨¡å‹ï¼Œä½¿ç”¨éå¸¸ä¸åŒçš„ç®—æ³•è¿›è¡Œè®­ç»ƒï¼šä¸€ä¸ªæ˜¯éšæœºæ£®æ—ï¼Œä¸€ä¸ªæ˜¯ç¥ç»ç½‘ç»œã€‚å¯ä»¥åˆç†åœ°æœŸæœ›æ¯ä¸ªæ¨¡å‹äº§ç”Ÿçš„é”™è¯¯ç±»å‹ä¼šæœ‰å¾ˆå¤§ä¸åŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæœŸæœ›å®ƒä»¬çš„é¢„æµ‹å¹³å‡å€¼ä¼šæ¯”ä»»ä½•ä¸€ä¸ªå•ç‹¬çš„é¢„æµ‹éƒ½è¦å¥½ã€‚

```python
rf_preds = m.predict(valid_xs_time)
ens_preds = (to_np(preds.squeeze()) + rf_preds) /2
r_mse(ens_preds,valid_y)
'0.22291'
```

#### 5.8.2 æå‡Boosting

æˆ‘ä»¬æ·»åŠ æ¨¡å‹è€Œä¸æ˜¯å¯¹å®ƒä»¬è¿›è¡Œå¹³å‡ã€‚ä»¥ä¸‹æ˜¯æå‡çš„å·¥ä½œåŸç†ï¼š

1.  è®­ç»ƒä¸€ä¸ªæ¬ æ‹Ÿåˆæ•°æ®é›†çš„å°æ¨¡å‹ã€‚

1.  è®¡ç®—è¯¥æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸­çš„é¢„æµ‹ã€‚

1.  ä»ç›®æ ‡ä¸­å‡å»é¢„æµ‹å€¼ï¼›è¿™äº›è¢«ç§°ä¸º*æ®‹å·®*ï¼Œä»£è¡¨äº†è®­ç»ƒé›†ä¸­æ¯ä¸ªç‚¹çš„è¯¯å·®ã€‚

1.  å›åˆ°ç¬¬ 1 æ­¥ï¼Œä½†æ˜¯ä¸è¦ä½¿ç”¨åŸå§‹ç›®æ ‡ï¼Œè€Œæ˜¯ä½¿ç”¨æ®‹å·®ä½œä¸ºè®­ç»ƒçš„ç›®æ ‡ã€‚

1.  ç»§ç»­è¿™æ ·åšï¼Œç›´åˆ°è¾¾åˆ°åœæ­¢æ ‡å‡†ï¼Œæ¯”å¦‚æœ€å¤§æ ‘çš„æ•°é‡ï¼Œæˆ–è€…è§‚å¯Ÿåˆ°éªŒè¯é›†é”™è¯¯å˜å¾—æ›´ç³Ÿã€‚

ä½¿ç”¨æå‡æ ‘é›†æˆè¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬è®¡ç®—æ¯æ£µæ ‘çš„é¢„æµ‹ï¼Œç„¶åå°†å®ƒä»¬å…¨éƒ¨åŠ åœ¨ä¸€èµ·ã€‚æœ‰è®¸å¤šéµå¾ªè¿™ç§åŸºæœ¬æ–¹æ³•çš„æ¨¡å‹ï¼Œä»¥åŠè®¸å¤šç›¸åŒæ¨¡å‹çš„åç§°ã€‚æ¢¯åº¦æå‡æœºï¼ˆGBMsï¼‰å’Œæ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGBDTsï¼‰æ˜¯æ‚¨æœ€æœ‰å¯èƒ½é‡åˆ°çš„æœ¯è¯­ï¼Œæˆ–è€…æ‚¨å¯èƒ½ä¼šçœ‹åˆ°å®ç°è¿™äº›æ¨¡å‹çš„ç‰¹å®šåº“çš„åç§°ï¼›åœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œ**XGBoost (eXtreme Gradient Boosting)**æ˜¯æœ€å—æ¬¢è¿çš„ã€‚

ä½†è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨éšæœºæ£®æ—ä¸­æ›´å¤šæ ‘å¯ä»¥é™ä½è¿‡æ‹Ÿåˆé£é™©ï¼Œä½†åœ¨æå‡é›†æˆä¸­ï¼Œæ‹¥æœ‰æ›´å¤šæ ‘ï¼Œè®­ç»ƒé”™è¯¯å°±ä¼šå˜å¾—æ›´å¥½ï¼Œæœ€ç»ˆæ‚¨å°†åœ¨éªŒè¯é›†ä¸Šçœ‹åˆ°è¿‡æ‹Ÿåˆã€‚

#### 5.8.3 ç¥ç»ç½‘ç»œå­¦ä¹ çš„åµŒå…¥

æˆ‘ä»¬åœ¨æœ¬ç« å¼€å¤´æåˆ°çš„å®ä½“åµŒå…¥è®ºæ–‡çš„æ‘˜è¦ä¸­æŒ‡å‡ºï¼šâ€œä»è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸­è·å¾—çš„åµŒå…¥åœ¨ä½œä¸ºè¾“å…¥ç‰¹å¾æ—¶æ˜¾è‘—æé«˜äº†æ‰€æœ‰æµ‹è¯•çš„æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚â€

![](D:\Git\a\Path-Records\img\dlcf_0908.png)

è¿™äº›åµŒå…¥ç”šè‡³ä¸éœ€è¦ä¸ºç»„ç»‡ä¸­çš„æ¯ä¸ªæ¨¡å‹æˆ–ä»»åŠ¡å•ç‹¬å­¦ä¹ ã€‚ç›¸åï¼Œä¸€æ—¦ä¸ºç‰¹å®šä»»åŠ¡çš„åˆ—å­¦ä¹ äº†ä¸€ç»„åµŒå…¥ï¼Œå®ƒä»¬å¯ä»¥å­˜å‚¨åœ¨ä¸€ä¸ªä¸­å¿ƒä½ç½®ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹ä¸­é‡å¤ä½¿ç”¨ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬ä»ä¸å…¶ä»–å¤§å…¬å¸çš„ä»ä¸šè€…çš„ç§ä¸‹äº¤æµä¸­å¾—çŸ¥ï¼Œè¿™åœ¨è®¸å¤šåœ°æ–¹å·²ç»å‘ç”Ÿäº†ã€‚

### 5.9 ç»“è®º

+   *éšæœºæ£®æ—*æ˜¯æœ€å®¹æ˜“è®­ç»ƒçš„ï¼Œå› ä¸ºå®ƒä»¬å¯¹è¶…å‚æ•°é€‰æ‹©éå¸¸æœ‰éŸ§æ€§ï¼Œéœ€è¦å¾ˆå°‘çš„é¢„å¤„ç†ã€‚å®ƒä»¬è®­ç»ƒé€Ÿåº¦å¿«ï¼Œå¦‚æœæœ‰è¶³å¤Ÿçš„æ ‘ï¼Œå°±ä¸ä¼šè¿‡æ‹Ÿåˆã€‚ä½†æ˜¯å®ƒä»¬å¯èƒ½ä¼šç¨å¾®ä¸å¤Ÿå‡†ç¡®ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤–æ¨çš„æƒ…å†µä¸‹ï¼Œæ¯”å¦‚é¢„æµ‹æœªæ¥çš„æ—¶é—´æ®µã€‚

+   æ¢¯åº¦æå‡æœºç†è®ºä¸Šè®­ç»ƒé€Ÿåº¦ä¸éšæœºæ£®æ—ä¸€æ ·å¿«ï¼Œä½†å®é™…ä¸Šæ‚¨å°†ä¸å¾—ä¸å°è¯•å¾ˆå¤šè¶…å‚æ•°ã€‚å®ƒä»¬å¯èƒ½ä¼šè¿‡æ‹Ÿåˆï¼Œä½†é€šå¸¸æ¯”éšæœºæ£®æ—ç¨å¾®å‡†ç¡®ä¸€äº›ã€‚

+   *ç¥ç»ç½‘ç»œ*éœ€è¦æœ€é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œå¹¶éœ€è¦é¢å¤–çš„é¢„å¤„ç†ï¼Œæ¯”å¦‚å½’ä¸€åŒ–ï¼›è¿™ç§å½’ä¸€åŒ–ä¹Ÿéœ€è¦åœ¨æ¨æ–­æ—¶ä½¿ç”¨ã€‚å®ƒä»¬å¯ä»¥æä¾›å¾ˆå¥½çš„ç»“æœå¹¶å¾ˆå¥½åœ°å¤–æ¨ï¼Œä½†åªæœ‰åœ¨æ‚¨å°å¿ƒå¤„ç†è¶…å‚æ•°å¹¶æ³¨æ„é¿å…è¿‡æ‹Ÿåˆæ—¶æ‰èƒ½å®ç°ã€‚

## 6: Random Forests

### 6.1 Binaryçš„æ ‡å‡†

ä»ä»¥5.1çš„æ•°æ®ä¸ºä¾‹ï¼Œè®²æ•°æ®åˆ’åˆ†æˆä¸¤ä¸ªé˜µè¥ï¼Œè®¡ç®—çš„æ˜¯:

```python
def _side_score(side, y):  #sideæ˜¯åˆ’åˆ†åœ¨å·¦æ‰‹é˜µè¥è¿˜æ˜¯å³æ‰‹é˜µè¥ï¼ˆä»¥å·¦æ‰‹é˜µè¥ä¸ºä¾‹ï¼‰
    tot = side.sum()  #å·¦æ‰‹é˜µè¥æœ‰å‡ æ¡æ•°æ®
    if tot<=1: return 0
    return y[side].std()*tot  #å·¦æ‰‹é˜µè¥çš„å› å˜é‡ä¹‹é—´çš„æ ‡å‡†å·®*å› å˜é‡çš„ä¸ªæ•°

def score(col, y, split):  #å·¦æ‰‹é˜µè¥å’Œå³æ‰‹é˜µè¥çš„åŠ åœ¨ä¸€èµ·
    lhs = col<=split
    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)

score(trn_xs["Sex"], trn_y, 0.5)
```

åƒè¿™ç§é€‰æ‹©ä¸€ä¸ªå‚æ•°åˆ’åˆ†ä¸€æ¬¡ï¼Œå°†æ•°æ®åˆ†æˆä¸¤ç±»çš„ç®—æ³•ï¼Œå«åš1Rã€‚

### 6.2 Scikit-learn

```python
from sklearn.tree import DecisionTreeClassifier, export_graphviz
m = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);
import graphviz
def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):
    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,
                      special_characters=True, rotate=False, precision=precision, **kwargs)
    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))
draw_tree(m, trn_xs, size=10)
```

![](D:\Git\a\Path-Records\img\06-1-1.png)

6.1æ˜¯åˆ’åˆ†é˜µè¥çš„ä¸€ç§æ–¹æ³•ï¼Œè€Œginiæ˜¯å¦å¤–ä¸€ç§æ–¹æ³•ï¼ˆé»˜è®¤ï¼‰ï¼š

```python
def gini(cond):
    act = df.loc[cond, dep]
    return 1 - act.mean()**2 - (1-act).mean()**2
gini(df.Sex=='female'), gini(df.Sex=='male')
```

ç”¨å†³ç­–æ ‘åšä¸€ä¸ªbaselineæ˜¯ä¸ªå¥½ä¸»æ„ï¼Œå› ä¸ºå®ƒå¾ˆéš¾mess up thingsï¼Œæ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°åº•çº¿æ˜¯ä»€ä¹ˆï¼Œå†é€šè¿‡å…¶å®ƒæ–¹æ³•å»æå‡ã€‚

#### 6.2.1 åˆ†ç±»å˜é‡çš„å¤„ç†

* scikit-learnï¼šè¦æ±‚ç”¨æˆ·è‡ªå·±å¯¹ç±»åˆ«å˜é‡åš one-hot æˆ–å…¶ä»–ç¼–ç ã€‚å®ƒä¸ä¼šè‡ªåŠ¨å†³å®šé¡ºåºã€‚å¦‚æœæ‰‹åŠ¨ç®€å•æ˜ å°„ï¼Œå°±ä¼šå¼•å…¥ä¸€ä¸ªå‡é¡ºåºï¼Œè¦è­¦æƒ•ã€‚

* LightGBM / CatBoost / XGBoostï¼šæ”¯æŒç›´æ¥è¾“å…¥ categorical ç‰¹å¾ï¼š

  LightGBM ä¼šæ ¹æ®ç±»åˆ«çš„ç›®æ ‡ç»Ÿè®¡ï¼ˆå¦‚æ¯ä¸ªç±»åˆ«çš„å‡å€¼ç›®æ ‡ï¼‰è‡ªåŠ¨å­¦ä¸€ä¸ªæœ€ä¼˜æ’åºï¼Œå†å°è¯•åˆ†è£‚ã€‚

  CatBoost ç”¨çš„æ˜¯ä¸€ç§ç‰¹æ®Šçš„ target encodingï¼ˆç»“åˆéšæœºæ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ã€‚

  XGBoost ä¸€èˆ¬ä¹Ÿéœ€è¦ one-hotï¼Œä½†åœ¨æ–°ç‰ˆé‡Œé€æ¸æ”¯æŒç›´æ¥ç±»åˆ«ç‰¹å¾ï¼Œç±»ä¼¼LightGBMã€‚

æ‰€ä»¥ï¼š

* å†³ç­–æ ‘æœ¬è´¨ä¸Šä¸éœ€è¦ç±»åˆ«çš„æ•°å­—é¡ºåºï¼Œå®ƒä¼šå°è¯•ä¸åŒçš„â€œç±»åˆ«é›†åˆåˆ’åˆ†â€ã€‚

* å¦‚æœä½ åªæ˜¯æŠŠç±»åˆ«â€œç¡¬ç¼–ç æˆæ•°å­—â€ï¼ŒæŸäº›å®ç°ä¼šé”™è¯¯åœ°å½“ä½œè¿ç»­å˜é‡å¤„ç†ï¼Œäº§ç”Ÿè™šå‡çš„é¡ºåºã€‚

* æ‰€ä»¥ä¸€èˆ¬æ¨è one-hot æˆ–è€…ç”¨ æ”¯æŒåŸç”Ÿåˆ†ç±»å˜é‡çš„åº“ï¼ˆLightGBM, CatBoostï¼‰ã€‚

### 6.3 Random forest - bagging

```python
from sklearn.ensemble import RandomForestClassifier #å› å˜é‡æ˜¯åˆ†ç±»çš„

rf = RandomForestClassifier(100, min_samples_leaf=5)
rf.fit(trn_xs, trn_y);
mean_absolute_error(val_y, rf.predict(val_xs))

pd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');
# å¦‚5.6.2ï¼Œå¯ä»¥çœ‹ç‰¹å¾é‡è¦æ€§
```

### 6.4 è¿‡æ‹Ÿåˆ

åœ¨éšæœºæ£®æ—ä¸­ï¼Œè®­ç»ƒç‰¹åˆ«å¤šçš„æ ‘å¹¶ä¸ä¼šè¿‡æ‹Ÿåˆï¼Œç›¸åæ ‘ä¸å¤Ÿå¤šæ‰ä¼šè¿‡æ‹Ÿåˆï¼Œå½“ç„¶æ¯æ£µæ ‘è®­ç»ƒå¾—è¿‡æ·±ä¹Ÿä¼šè¿‡æ‹Ÿåˆã€‚

### 6.5 Gradient Boosting

ä½ å¯ä»¥æŠŠ Gradient Boosting æƒ³æˆæ˜¯ä¸€ä¸ªâ€œè€å¸ˆ+å­¦ç”Ÿâ€åå¤ä¿®æ­£çš„è¿‡ç¨‹ï¼š

1. ç¬¬ä¸€æ£µæ ‘åšä¸€ä¸ªç²—ç•¥é¢„æµ‹ã€‚
2. çœ‹çœ‹é¢„æµ‹å’ŒçœŸå®ç­”æ¡ˆçš„è¯¯å·®ï¼ˆæ®‹å·®ï¼‰ã€‚
3. ä¸‹ä¸€æ£µæ ‘å­¦ä¹ è¿™äº›è¯¯å·®ï¼Œè¯•ç€ä¿®æ­£å®ƒã€‚
4. ä¸€æ£µä¸€æ£µåŠ ä¸‹å»ï¼Œæ¯æ¬¡éƒ½åœ¨â€œè¡¥è¯¾â€ï¼Œè®©æ•´ä½“é¢„æµ‹è¶Šæ¥è¶Šå¥½ã€‚

**ä¼˜ç‚¹**ï¼š

- æ‹Ÿåˆèƒ½åŠ›å¼ºï¼Œç»å¸¸æ˜¯ Kaggle æ¯”èµ›å† å†›å¸¸ç”¨æ¨¡å‹ã€‚
- å¯ä»¥å¤„ç†å›å½’ã€åˆ†ç±»ã€æ’åºé—®é¢˜ã€‚

**ç¼ºç‚¹**ï¼š

- ä¸²è¡Œè®­ç»ƒï¼Œé€Ÿåº¦æ¯”éšæœºæ£®æ—æ…¢ã€‚

- å¯¹å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ ‘æ·±åº¦ã€è¿­ä»£æ¬¡æ•°ï¼‰æ¯”è¾ƒæ•æ„Ÿï¼Œéœ€è¦è°ƒå‚ã€‚


**åˆ†ç±»**

- sklearn.ensemble.GradientBoostingï¼šåŸºç¡€ç‰ˆå®ç°ã€‚
- XGBoostï¼šæ”¹è¿›ç‰ˆï¼ˆæ”¯æŒäºŒé˜¶å¯¼ã€ç¨€ç–å¤„ç†ã€å¹¶è¡Œä¼˜åŒ–ï¼‰ã€‚
- LightGBMï¼šè¿›ä¸€æ­¥åŠ é€Ÿï¼ˆåŸºäºç›´æ–¹å›¾åˆ†è£‚ï¼Œå¤§æ•°æ®æ›´å¿«ï¼‰ã€‚
- CatBoostï¼šä¸“é—¨ä¼˜åŒ–ç±»åˆ«å˜é‡ã€‚

**æ­¥éª¤**

* ä¸¥æ ¼æ¥è¯´ï¼Œç¬¬ 0 æ­¥è¿˜æ²¡æœ‰æ ‘ï¼Œåªæœ‰ä¸€ä¸ªå¸¸æ•°æ¨¡å‹ï¼›

* ç¬¬ä¸€æ£µæ ‘çš„ä»»åŠ¡å°±æ˜¯å»â€œä¿®æ­£â€è¿™ä¸ªåŸºå‡†é¢„æµ‹

  * è®¡ç®—æ®‹å·®ï¼ˆå›å½’é—®é¢˜ï¼‰
  * ç”¨ä¸€æ£µå°å›å½’æ ‘å»æ‹Ÿåˆè¿™äº›æ®‹å·®ã€‚
    - è¿™æ£µæ ‘å¾€å¾€æ˜¯æµ…æ ‘ï¼ˆæ¯”å¦‚æ·±åº¦=3~5ï¼‰ï¼Œä¸æ˜¯å¤æ‚çš„å¤§æ ‘ã€‚
    - å®ƒä¼šå°½é‡åˆ†å‰²æ•°æ®ï¼Œä½¿å¾—ä¸åŒåŒºåŸŸçš„æ®‹å·®å°½é‡å¹³å‡ã€‚

  * æ›´æ–°æ¨¡å‹

* é€¼è¿‘çœŸå®å‡½æ•°ã€‚

| ç®—æ³•                            | æ ¸å¿ƒç‰¹ç‚¹                                                     | ä¼˜åŠ¿                                                       | åŠ£åŠ¿                                                  | é€‚ç”¨åœºæ™¯                                             |
| ------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ----------------------------------------------------- | ---------------------------------------------------- |
| **Gradient Boosting (sklearn)** | æœ€åŸºç¡€çš„å®ç°ï¼Œé€æ£µæ ‘æ‹Ÿåˆæ®‹å·®                                 | åŸç†ç›´è§‚ï¼Œå®¹æ˜“ç†è§£ï¼Œé€‚åˆæ•™å­¦å’Œå°è§„æ¨¡å®éªŒ                   | é€Ÿåº¦æ…¢ï¼Œä¸èƒ½å¹¶è¡Œï¼Œä¸æ”¯æŒç¨€ç–è¾“å…¥ï¼Œå¯¹å¤§æ•°æ®ä¸å‹å¥½      | å°æ•°æ®é›†ï¼Œæ•™å­¦/éªŒè¯ç†è®º                              |
| **XGBoost**                     | äºŒé˜¶å¯¼ä¼˜åŒ– + æ­£åˆ™åŒ– + å‰ªæï¼›æ”¯æŒå¹¶è¡Œå’Œç¨€ç–å¤„ç†               | ç²¾åº¦é«˜ï¼Œé²æ£’æ€§å¼ºï¼Œå·¥ä¸šç•Œåº”ç”¨å¹¿ï¼›æœ‰å¾ˆå¤šå‚æ•°å¯è°ƒ             | å†…å­˜æ¶ˆè€—è¾ƒå¤§ï¼Œè°ƒå‚å¤æ‚                                | ä¸­å¤§å‹æ•°æ®é›†ï¼Œç»“æ„åŒ–è¡¨æ ¼ç±»ä»»åŠ¡                       |
| **LightGBM**                    | åŸºäºç›´æ–¹å›¾åˆ†è£‚ï¼ˆHistogramï¼‰+ Leaf-wise ç­–ç•¥ï¼›æ”¯æŒå¤§è§„æ¨¡æ•°æ®  | è®­ç»ƒé€Ÿåº¦å¿«ï¼Œå†…å­˜å ç”¨ä½ï¼Œé€‚åˆè¶…å¤§è§„æ¨¡æ•°æ®ï¼›é»˜è®¤æ•ˆæœå¥½       | å¯¹å°æ•°æ®å¯èƒ½è¿‡æ‹Ÿåˆï¼›å¯¹ç±»åˆ«ç‰¹å¾æ”¯æŒä¸å¦‚ CatBoost ç¨³å®š  | å¤§æ•°æ®é›†ï¼Œé«˜ç»´ç¨€ç–ç‰¹å¾åœºæ™¯ï¼ˆé‡‘èã€æ¨èã€å¹¿å‘Šï¼‰       |
| **CatBoost**                    | å†…ç½®ç±»åˆ«ç‰¹å¾ç¼–ç ï¼ˆåŸºäºç»Ÿè®¡æ–¹æ³•ï¼‰ï¼Œé¿å…æ‰‹åŠ¨ one-hotï¼›å¯¹ç±»åˆ«å˜é‡ä¼˜åŒ–æå¥½ | å¯¹ç±»åˆ«ç‰¹å¾å¤„ç†æ•ˆæœæœ€ä½³ï¼›å‡ ä¹æ— éœ€å¤æ‚è°ƒå‚ï¼›é˜²æ­¢è¿‡æ‹Ÿåˆèƒ½åŠ›å¼º | é€Ÿåº¦æ¯” LightGBM ç¨æ…¢ï¼›æ–‡æ¡£/ç”Ÿæ€æ¯” XGBoost/LightGBM å°‘ | ç±»åˆ«å‹ç‰¹å¾å æ¯”å¾ˆé«˜çš„åœºæ™¯ï¼ˆé‡‘èé£æ§ã€ç”µå•†ã€ç¤¾äº¤ç½‘ç»œï¼‰ |

### 6.6 Kaggle competition

ä¸€ä¸ªå›¾åƒåˆ†ç±»é—®é¢˜ï¼Œæ²¡ä»€ä¹ˆç¬”è®°å¯åšã€‚

Kaggleçš„CPUåªæœ‰2ä¸ªï¼Œå¯¹ç°ä»£å®é™…ä¸Šæ˜¯å¾ˆå°‘çš„ï¼Œå› æ­¤å¾ˆå¤šæ—¶å€™ï¼Œåœ¨ä¸Šé¢è·‘ä¸œè¥¿æ˜¯å¾ˆæ…¢çš„ï¼Œéœ€è¦æ³¨æ„å°½å¯èƒ½é€šè¿‡ä»£ç æå‡æ•ˆç‡ã€‚

[The best vision models for fine-tuning](https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning)

#### 6.6.1 TTA

##### **æ¦‚å¿µ**

è®­ç»ƒæ—¶çš„æ•°æ®å¢å¼º (Data Augmentation)ï¼š
 åœ¨è®­ç»ƒæ—¶å¯¹å›¾ç‰‡è¿›è¡Œéšæœºæ—‹è½¬ã€è£å‰ªã€ç¿»è½¬ã€é¢œè‰²æŠ–åŠ¨ç­‰æ“ä½œï¼Œå¢åŠ æ ·æœ¬å¤šæ ·æ€§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

æµ‹è¯•æ—¶å¢å¼º (TestTimeAugmentation, TTA)ï¼š
 åœ¨æ¨ç†é˜¶æ®µï¼ˆæµ‹è¯•/é¢„æµ‹æ—¶ï¼‰ï¼Œå¯¹åŒä¸€å¼ æµ‹è¯•å›¾ç‰‡åšå¤šç§å¢å¼ºï¼ˆä¾‹å¦‚ç¿»è½¬ã€ç¼©æ”¾ï¼‰ï¼Œè®©æ¨¡å‹å¤šæ¬¡é¢„æµ‹ï¼Œç„¶åæŠŠè¿™äº›é¢„æµ‹ç»“æœåšå¹³å‡ï¼ˆæˆ–æŠ•ç¥¨ï¼‰ï¼Œå¾—åˆ°æœ€ç»ˆé¢„æµ‹ã€‚ï¼ˆåœ¨fastaiä¸­ï¼Œé»˜è®¤åš4ç§å¢å¼ºï¼‰

##### **ä¼˜ç¼ºç‚¹**

âœ…ä¼˜ç‚¹ï¼š

- æå‡é¢„æµ‹ç¨³å®šæ€§ï¼Œå‡å°‘å¶ç„¶æ€§é”™è¯¯
- å¯¹äº æ•°æ®åˆ†å¸ƒä¸å‡è¡¡ã€æ ·æœ¬æœ‰é™çš„æƒ…å†µç‰¹åˆ«æœ‰æ•ˆ
- å¸¸è§äº Kaggle ç«èµ›ã€åŒ»å­¦å½±åƒã€é¥æ„Ÿè¯†åˆ« ç­‰

âš ï¸ ç¼ºç‚¹ï¼š

- æ¨ç†é€Ÿåº¦å˜æ…¢ï¼ˆéœ€è¦å¯¹æ¯å¼ å›¾ç‰‡é¢„æµ‹å¤šæ¬¡ï¼‰
- æ•ˆæœæå‡æœ‰é™ï¼ˆé€šå¸¸æå‡ 0.2% ~ 1%ï¼‰
- å¦‚æœå¢å¼ºæ“ä½œé€‰å¾—ä¸å¥½ï¼Œå¯èƒ½åè€Œå¼•å…¥å™ªå£°

```python
#ä¸€äº›ä¸å®Œæ•´çš„èŒƒä¾‹
def train(arch, item, batch, epochs=5):
    dls = ImageDataLoaders.from_folder(trn_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch)
    learn = vision_learner(dls, arch, metrics=error_rate).to_fp16()
    learn.fine_tune(epochs, 0.01)
    return learn
trn_path = path/'train_images'
learn = train(arch, epochs=12,
              item=Resize((480, 360), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),
              batch=aug_transforms(size=(256,192), min_scale=0.75))
tta_preds,targs = learn.tta(dl=learn.dls.valid)
error_rate(tta_preds, targs)
```

#####  **Tabular æ•°æ®å¢å¼ºçš„å¸¸è§æ–¹æ³•**

å›¾åƒã€è¯­éŸ³ã€æ–‡æœ¬ â†’ æœ‰è‡ªç„¶çš„â€œç»“æ„â€å’Œâ€œç©ºé—´â€ï¼Œåšå¹³ç§»ã€æ—‹è½¬ã€å™ªå£°ã€é®æŒ¡ç­‰å¢å¼ºä¸ä¼šæ”¹å˜è¯­ä¹‰ã€‚

è¡¨æ ¼æ•°æ® â†’ æ¯ä¸ªç‰¹å¾åˆ—éƒ½æœ‰ä¸åŒå«ä¹‰ï¼ˆå¹´é¾„ã€æ”¶å…¥ã€åœ°åŒºã€ç±»åˆ«ç¼–ç ç­‰ï¼‰ï¼Œç›²ç›®å¢åŠ å™ªå£°å¯èƒ½ç ´åè¯­ä¹‰ï¼ˆæ¯”å¦‚å¹´é¾„ = -5 å²ï¼‰ã€‚å› æ­¤ï¼Œè¡¨æ ¼å¢å¼ºå¿…é¡»ç»“åˆé¢†åŸŸçŸ¥è¯†ï¼Œä¸èƒ½éšæ„â€œæ‰°åŠ¨â€ã€‚

(1) ç®€å•å™ªå£°æ³•

åœ¨è¿ç»­ç‰¹å¾ä¸ŠåŠ å°çš„éšæœºå™ªå£°ï¼š

```
df['age_aug'] = df['age'] + np.random.normal(0, 1, len(df))
```

æ³¨æ„ä¸èƒ½æ”¹å˜ç±»åˆ«å‹ç‰¹å¾ï¼ˆå¦‚æ€§åˆ«ã€åœ°åŒºï¼‰ã€‚
(2) SMOTEï¼ˆè¿‡é‡‡æ ·ï¼‰

é€‚ç”¨åœºæ™¯ï¼šç±»åˆ«ä¸å¹³è¡¡ï¼ˆäºŒåˆ†ç±»/å¤šåˆ†ç±»é—®é¢˜ï¼‰ã€‚

åŸç†ï¼šåœ¨å°‘æ•°ç±»æ ·æœ¬ä¹‹é—´æ’å€¼ï¼Œåˆæˆâ€œæ–°æ ·æœ¬â€ã€‚

å¸¸ç”¨åº“ï¼š`imblearn`

```
from imblearn.over_sampling import SMOTE
X_res, y_res = SMOTE().fit_resample(X, y)
```

(3) éšæœºäº¤æ¢ï¼ˆSwap Noiseï¼‰

åœ¨åŒä¸€åˆ—å†…ï¼Œéšæœºäº¤æ¢ä¸åŒæ ·æœ¬çš„å€¼ï¼Œä¿æŒåˆ†å¸ƒä¸€è‡´ã€‚

- ä¾‹å¦‚éšæœºæ‰“ä¹±â€œåœ°åŒºâ€è¿™ä¸€åˆ—ï¼Œä½†ä¸æ”¹å˜æ€»ä½“åˆ†å¸ƒã€‚


(4) åˆæˆæ•°æ®ï¼ˆGAN / VAEï¼‰[ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œæœ€å¥½åˆ¤åˆ«å™¨è¾“å‡ºæ¥è¿‘0.5]

ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ **CTGAN, TabGAN, VAE**ï¼‰ç”Ÿæˆç¬¦åˆåŸå§‹åˆ†å¸ƒçš„æ–°æ ·æœ¬ã€‚

å¸¸ç”¨äºé‡‘èã€åŒ»ç–—åœºæ™¯ã€‚

```
from ctgan import CTGANSynthesizer
ctgan = CTGANSynthesizer()
ctgan.fit(df, discrete_columns=['gender', 'region'])
samples = ctgan.sample(1000)
```

(5) ç‰¹å¾ç»„åˆå¢å¼ºï¼ˆFeature Engineeringï¼‰

å¢åŠ äº¤äº’ç‰¹å¾ï¼Œä¾‹å¦‚ï¼š

- `income_per_age = income / age`
- `region_income_rank`
  è¿™æ›´åƒæ˜¯â€œå¢å¼ºç‰¹å¾ç©ºé—´â€ï¼Œä¹Ÿå¯ä»¥çœ‹ä½œä¸€ç§æ•°æ®å¢å¼ºã€‚
  **TTA åœ¨ Tabular æ•°æ®é‡Œçš„æ„ä¹‰**

åœ¨å›¾åƒé‡Œï¼ŒTTA æ˜¯å¯¹åŒä¸€æ¡æ•°æ®åšå¤šç§å¢å¼ºå†ç»¼åˆé¢„æµ‹ã€‚åœ¨è¡¨æ ¼é‡Œï¼Œè¿™ä¹ˆåšä¸å¤ªå¸¸è§ï¼Œä½†å¯ä»¥ï¼š

- å¯¹åŒä¸€æ¡æ ·æœ¬åŠ ä¸åŒçš„å™ªå£°ï¼Œå¾—åˆ°å¤šä¸ªâ€œé‚»å±…æ ·æœ¬â€ï¼Œå†å¹³å‡é¢„æµ‹ç»“æœ â†’ æé«˜é²æ£’æ€§ã€‚
- åœ¨ Kaggle è¡¨æ ¼ç«èµ›é‡Œï¼Œæœ‰äººç”¨è¿‡ç±»ä¼¼æ–¹æ³•ï¼Œç§°ä¸º Tabular TTAã€‚

## 7: Collaborative filtering

### 7.1 å°å†…å­˜è·‘å¤§æ¨¡å‹

#### 7.1.1 å†…å­˜

æŸ¥çœ‹è·‘ä¸€ä¸ªæ¨¡å‹ä¼šç”¨åˆ°å¤šå°‘GPUï¼š

```python
import gc
def report_gpu():
    print(torch.cuda.list_gpu_processes())
    gc.collect()  # è§¦å‘Pythonåƒåœ¾å›æ”¶ï¼ˆæ¸…ç†æœªå¼•ç”¨çš„å¯¹è±¡ï¼‰
    torch.cuda.empty_cache() # é‡Šæ”¾PyTorchç¼“å­˜çš„æ˜¾å­˜ï¼ˆå¯è¿˜ç»™CUDAï¼Œä½†ä¸æ˜¯è¿˜ç»™ç³»ç»Ÿï¼‰
train('convnext_small_in22k', 128, epochs=1, accum=4, finetune=False)
report_gpu()
'''
GPU:0
process       3248 uses    11838.000 MB GPU memory
'''
```

ä¸ºäº†å…ˆç®€æ˜“åœ°è¯•éªŒä¸€ä¸‹ï¼Œå¯ä»¥å…ˆç”¨ä¸€ä¸ªæ¯”è¾ƒå°çš„è¾“å…¥è¯•ä¸€è¯•çœ‹ç”¨å¤šå°‘gpu

#### 7.1.2 æ¢¯åº¦ç´¯ç§¯

Gradient Accumulationï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰

##### åŸç†

è®¾å®šç›®æ ‡ batch size ä¸º `B`ï¼Œä½†æ˜¾å­˜åªèƒ½æ”¾ä¸‹ `b`ã€‚
 ğŸ‘‰ æ¯”å¦‚ç›®æ ‡æƒ³è¦ B=128ï¼Œä½†ä¸€æ¬¡åªèƒ½æ”¾ b=32ã€‚

é‚£ä¹ˆå°±åˆ†æˆå¤šæ¬¡å‰å‘/åå‘ä¼ æ’­ï¼š

- æ¯æ¬¡è¾“å…¥ b=32æ ·æœ¬ï¼Œç®—æ¢¯åº¦ï¼Œä½†ä¸æ›´æ–°å‚æ•°ï¼ˆä¸åš optimizer.step()ï¼‰ã€‚
- æŠŠæ¢¯åº¦ç´¯ç§¯åœ¨æ¨¡å‹å‚æ•°ä¸Šã€‚

å½“ç´¯è®¡äº† accum_steps = B/b = 128/32 = 4 æ¬¡ mini-batch åï¼š

- å†æ‰§è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ï¼ˆ`optimizer.step()`ï¼‰ã€‚
- ç„¶åæŠŠæ¢¯åº¦æ¸…é›¶ï¼ˆ`optimizer.zero_grad()`ï¼‰ã€‚

##### ä¼˜ç¼ºç‚¹

ä¼˜ç‚¹

- ç”¨è¾ƒå°æ˜¾å­˜æ¨¡æ‹Ÿå¤§ batch è®­ç»ƒ
- æ›´ç¨³å®šçš„æ¢¯åº¦æ›´æ–°
- å¯ä»¥åˆ©ç”¨å¤§ batch size çš„ä¼˜åŠ¿ï¼ˆæ›´å¹³æ»‘çš„ loss æ›²çº¿ï¼‰

ç¼ºç‚¹

- è®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢ï¼ˆå› ä¸ºä¸€æ¬¡æ›´æ–°è¦å¤šæ¬¡ forward/backwardï¼Œä½†Jeremyè®¤ä¸ºå¹¶ä¸æ˜¾è‘—ï¼‰
- å¹¶ä¸æ˜¯æ‰€æœ‰ä¼˜åŒ–å™¨éƒ½å¯¹å¤§ batch size æ”¶æ•›å¾—æ›´å¥½ï¼ˆæ¯”å¦‚ AdamW çš„æ•ˆæœæœ‰æ—¶å·®åˆ«ä¸å¤§ï¼‰

##### ä»£ç 

```python
def train(arch, size, item=Resize(480, method='squish'), accum=1, finetune=True, epochs=12):
    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item,
        batch_tfms=aug_transforms(size=size, min_scale=0.75), bs=64//accum)
    cbs = GradientAccumulation(64) if accum else []
    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()
    if finetune:
        learn.fine_tune(epochs, 0.01)
        return learn.tta(dl=dls.test_dl(tst_files))
    else:
        learn.unfreeze()
        learn.fit_one_cycle(epochs, 0.01)
```

##### å…¶å®ƒ

å¯¹åŒä¸€ä¸ªç»“æ„ï¼Œæ— è®ºåšä¸åšæ¢¯åº¦ç´¯ç§¯ï¼Œæœ€ä½³çš„lrä¸å—å½±å“ï¼Œlråªä¸batch sizeæœ‰å…³ã€‚

### 7.2 é›†æˆEnsembling

Tabularï¼š5.8.1

### 7.3 Multi-outputs

#### 7.3.1 æ„å»ºdls

æ— æ³•å†ç”¨ä¸Šå±‚å‡½æ•°ImageDataLoadersï¼Œè¦ä½¿ç”¨ä¸‹å±‚å‡½æ•°DataBlockï¼š

```python
dls = DataBlock(
    blocks=(ImageBlock,CategoryBlock,CategoryBlock), #è‡ªå˜é‡ã€å› å˜é‡ã€å› å˜é‡
    n_inp=1, #è‡ªå˜é‡æ•°é‡
    get_items=get_image_files,
    get_y = [parent_label,get_variety], #å› å˜é‡è·å–æ–¹å¼
    splitter=RandomSplitter(0.2, seed=42),
    item_tfms=Resize(192, method='squish'),
    batch_tfms=aug_transforms(size=128, min_scale=0.75)
).dataloaders(trn_path)
```

#### 7.3.2 Error rate & Loss

```python
def disease_err(inp,disease,variety): return error_rate(inp,disease)
def disease_loss(inp,disease,variety): return F.cross_entropy(inp,disease)
arch = 'convnext_small_in22k'
learn = vision_learner(dls, arch, loss_func=disease_loss, metrics=disease_err, n_out=10).to_fp16()
lr = 0.01
'è™½ç„¶dlsæ„å»ºäº†ä¸¤ä¸ªå› å˜é‡ï¼Œå¯æ˜¯ç”±äºlosså’Œmetricsçš„è®¾ç½®ï¼Œæœ€åå…¶å®æ˜¯1ä¸ªè¾“å‡º'
```

```python
def disease_loss(inp,disease,variety): return F.cross_entropy(inp[:,:10],disease)
def variety_loss(inp,disease,variety): return F.cross_entropy(inp[:,10:],variety)
def combine_loss(inp,disease,variety): return disease_loss(inp,disease,variety)+variety_loss(inp,disease,variety)

def disease_err(inp,disease,variety): return error_rate(inp[:,:10],disease)
def variety_err(inp,disease,variety): return error_rate(inp[:,10:],variety)
err_metrics = (disease_err,variety_err)

all_metrics = err_metrics+(disease_loss,variety_loss)
learn = vision_learner(dls, arch, loss_func=combine_loss, metrics=all_metrics, n_out=20).to_fp16()
'é€šè¿‡ä¿®æ­£losså’Œmetricsï¼Œå°±èƒ½è¾“å‡º2ä¸ªoutputsï¼Œå…¶å®ä¸»è¦æ˜¯ä¿®æ­£metrics'
```

##### cross_entropy

ä¸¾ä¾‹åˆ†ç±»ç®—æ³•ï¼Œoutputè¾“å‡ºä¸º3ä¸ªç±»åˆ«ï¼ŒçŒ«ã€ç‹—ã€é©¬ï¼š

- è¾“å‡ºä¸º[x, x, x]å½¢å¼ï¼›

- å†…éƒ¨3ä¸ªå®æ•°ï¼ˆlogitsï¼‰è½¬åŒ–ä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œä¸‰è€…åŠ å’Œä¸º1ã€‚å…¬å¼å¦‚ä¸‹ï¼š

$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}, \quad i=1,2,\dots,K
$$

- æ•°æ®å®é™…ä¸Šæœ‰çœŸå®æ ‡ç­¾å¦‚[0, 0, 1]ï¼Œè¿™ç›¸å½“ä¸€ä¸ªone-hotç¼–ç ï¼Œäº¤å‰ç†µä¸ºï¼š
  $$
  H(y,y')=âˆ’{\sum_{i}â€‹y_iâ€‹log(y'_iâ€‹)}
  $$
  å› ä¸ºyæ˜¯one-hotç¼–ç ï¼Œåªæœ‰çœŸçš„é‚£ä¸€é¡¹æ˜¯1ï¼Œå…¶å®ƒçš„æ˜¯0ï¼Œæ‰€ä»¥å…¶å®å…¬å¼åŒ–ç®€ä¸ºï¼š
  $$
  H(y,y'â€‹)=âˆ’log(y'_{true}â€‹)
  $$

ä»¥ä¸Šå°±æ˜¯Jeremyçš„æ–‡ä»¶ä¸­è®¡ç®—äº¤å‰ç†µçš„æ–¹æ³•ã€‚

è¿˜æœ‰å¦ä¸€ç§æ–¹æ³•æ˜¯äºŒåˆ†ç±»æ³•ï¼Œè¾“å‡º $$[z_0,z_1]$$ï¼Œè¡¨ç¤ºâ€œè´Ÿç±»â€å’Œâ€œæ­£ç±»â€çš„åˆ†æ•°ï¼š

- softmaxï¼š
  $$
  y'_0â€‹=\frac{e^{z_0}}{e^{z_0}+e^{z_1}}â€‹â€‹,y'_1â€‹=\frac{e^{z_1}}{e^{z_0}+e^{z_1}}
  $$

- è®¡ç®—äº¤å‰ç†µï¼š
  $$
  L=âˆ’[y_0â€‹log(y'_0â€‹)+y_1â€‹log(y'_1â€‹)]
  $$

- éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå¦‚æœæ˜¯äºŒåˆ†ç±»è¾“å‡ºä¸º$$z$$ä¸€ä¸ªæ•°ï¼Œä¹Ÿå¯ä»¥ç”Ÿæˆäº¤å‰ç†µçš„lossï¼Œå…ˆsigmoidå°†$$z$$å‹ç¼©åˆ°0~1ï¼Œç„¶åï¼š
  $$
  L=âˆ’[ylog(y'â€‹)+(1âˆ’y)log(1âˆ’y'â€‹)]
  $$
  è¿™ä¸ªå’Œä¸Šé¢ä¸¤æ­¥çš„æœ¬è´¨æ˜¯ä¸€æ ·çš„

#### 7.3.3 å…¶å®ƒè¯´æ˜

æˆ‘ä»¬æƒ³åˆ†ç±»æ°´ç¨»çš„ç—…å®³ï¼Œå¦‚æœæˆ‘ä»¬æ—¢åˆ†ç±»æ°´ç¨»ç—…å®³åˆåˆ†ç±»æ°´ç¨»ç§ç±»ï¼Œé‚£ä¹ˆï¼š

ï¼ˆ1ï¼‰å½“é‡‡ç”¨ç›¸åŒepochçš„æ—¶å€™ï¼Œä¸¤ç§åˆ†ç±»å¾€å¾€æ²¡æœ‰åªä¸€ç§åˆ†ç±»æ•ˆæœå¥½ï¼Œå› ä¸ºåšäº†æ›´å¤šå·¥ä½œï¼›

ï¼ˆ2ï¼‰ä½†éšç€è®­ç»ƒï¼Œæœ‰äº›æ—¶å€™ï¼Œå¤šç›®æ ‡è¯†åˆ«çš„ç»“æœä¼šæ¯”å•ç›®æ ‡çš„è¡¨ç°è¦å¥½ã€‚

### 7.4 åè°ƒè¿‡æ»¤

å¤§æ ‡é¢˜ï¼Œç›´æ¥çœ‹fastbook8

## 7: Collaborative filtering(fastbook8)

å¾ˆå¤šæ—¶å€™æˆ‘ä»¬åœ¨é‡‡é›†æ•°æ®çš„æ—¶å€™æ— æ³•è·å¾—å…ƒæ•°æ®ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥è·å¾—å¾ˆå¤šä¸»ä½“ã€å¾ˆå¤šå¯¹è±¡ä»¥åŠæ¯ä¸ªä¸»ä½“å¯¹å„ç§å¯¹è±¡çš„æ€åº¦ï¼Œå½“æˆ‘ä»¬æƒ³é¢„æµ‹ä¸»ä½“Aå¯¹å¯¹è±¡Bçš„æ€åº¦æ—¶ï¼Œå³ä¾¿ä¸çŸ¥é“Açš„æŠ½è±¡åå¥½ï¼ˆå…ƒæ•°æ®ï¼‰ï¼ŒåªçŸ¥é“Aå¯¹å…¶å®ƒå¯¹è±¡çš„æ€åº¦ï¼Œå³ä¾¿ä¸çŸ¥é“Bçš„æŠ½è±¡ç±»å‹ï¼ˆå…ƒæ•°æ®ï¼‰ï¼ŒåªçŸ¥é“å…¶å®ƒä¸»ä½“å¯¹Bçš„æ€åº¦ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥é¢„æµ‹Aå¯¹Bçš„æ€åº¦ï¼Œè¿™ä¸ªå°±éœ€è¦ç”¨ååŒè¿‡æ»¤ã€‚

### 7.1 å­¦ä¹ æ½œåœ¨å› å­

ï¼ˆ1ï¼‰åˆå§‹åŒ–ä¸€äº›å‚æ•°ï¼Œå¦‚ä¸‹å›¾ç®­å¤´æ‰€ç¤ºä½ç½®

![](D:\Git\a\Path-Records\img\dlcf_0802.png)

ï¼ˆ2ï¼‰è®¡ç®—é¢„æµ‹ï¼Œå¦‚ä¸Šå›¾ä¸»ä½“ä½ç½®

ï¼ˆ3ï¼‰è®¡ç®—æŸå¤±ï¼Œæ¯”å¦‚ç”¨RMSE

è¦ä½¿ç”¨é€šå¸¸çš„Learner.fitå‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„æ•°æ®æ”¾å…¥DataLoadersä¸­ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç°åœ¨ä¸“æ³¨äºè¿™ä¸€ç‚¹ã€‚

### 7.2 åˆ›å»ºDLs

```python
ratings = pd.read_csv(path/'u.data', delimiter='\t', header=None,
                      names=['user','movie','rating','timestamp'])
movies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',
                     usecols=(0,1), names=('movie','title'), header=None)
ratings = ratings.merge(movies)
dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)
dls.show_batch()
'''
åœ¨CollabDataLoadersä¸­ï¼š
user_name é»˜è®¤å€¼æ˜¯ 'user'
item_name é»˜è®¤å€¼æ˜¯ 'item'
rating_name é»˜è®¤å€¼æ˜¯ 'rating'
å› æ­¤å…¶ä¸­æœ‰äº›åå­—åˆšå¥½æ˜¯é»˜è®¤å€¼ï¼Œå°±ä¸éœ€è¦ç‰¹æ®ŠæŒ‡å®šäº†
'''
```

|      | user | title                      | rating |
| ---- | ---- | -------------------------- | ------ |
| 0    | 207  | å››ä¸ªå©šç¤¼å’Œä¸€ä¸ªè‘¬ç¤¼ï¼ˆ1994ï¼‰ | 3      |
| 1    | 565  | æ—¥æ®‹ä½™ï¼ˆ1993ï¼‰             | 5      |
| 2    | 506  | å°å­©ï¼ˆ1995ï¼‰               | 1      |
| 3    | 845  | è¿½æ±‚è‰¾ç±³ï¼ˆ1997ï¼‰           | 3      |
| 4    | 798  | äººç±»ï¼ˆ1993ï¼‰               | 2      |
| 5    | 500  | ä½ä¿—æ³•åˆ™ï¼ˆ1986ï¼‰           | 4      |
| 6    | 409  | æ— äº‹ç”Ÿéï¼ˆ1993ï¼‰           | 3      |
| 7    | 721  | å‹‡æ•¢çš„å¿ƒï¼ˆ1995ï¼‰           | 5      |
| 8    | 316  | ç²¾ç¥ç—…æ‚£è€…ï¼ˆ1960ï¼‰         | 2      |
| 9    | 883  | åˆ¤å†³ä¹‹å¤œï¼ˆ1993ï¼‰           | 5      |

### 7.3 Embedding

è§fastbookä¸­çš„5.1ï¼Œä»¥userçš„åµŒå…¥ä¸ºä¾‹ï¼š

```python
n_users  = len(dls.classes['user'])
user_factors = torch.randn(n_users, 5)
one_hot_3 = one_hot(3, n_users).float() #ç‹¬çƒ­
user_factors.t() @ one_hot_3 # @ä¸€ä¸ªtensorï¼ˆåƒæŸ¥è¡¨çš„è¡¨ä¸€æ ·ï¼‰
```

### 7.4 ååŒè¿‡æ»¤

```python
class DotProductBias(Module): #ç»§æ‰¿äº†è¶…ç±»Module
    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):
        self.user_factors = Embedding(n_users, n_factors) #ç”Ÿæˆäº†ä¸€ä¸ªnn.Embeddingçš„ç±»ï¼ˆæŸ¥è¡¨çš„é‚£ä¸ªè¡¨ï¼‰
        self.user_bias = Embedding(n_users, 1) #å¢åŠ äº†åç½®
        self.movie_factors = Embedding(n_movies, n_factors)
        self.movie_bias = Embedding(n_movies, 1) #å¢åŠ äº†åç½®
        self.y_range = y_range
        
    def forward(self, x): 
        '''
        è¿™ä¸ªxå…¶å®æ˜¯åé¢è¦ä¼ å…¥çš„DataLoadersä¸­çš„ä¸¤ä¸ªè‡ªå˜é‡ï¼Œä¸€ä¸ªæ˜¯userã€ä¸€ä¸ªæ˜¯title
        è¿™ä¸ªæ¨¡å—å…¶å®å°±æ˜¯æˆ‘ä»¬æ„å»ºçš„ç½‘ç»œç»“æ„
        '''
        users = self.user_factors(x[:,0])
        '''
        usersæ˜¯å¯¹ç…§x[:,0]é‡Œé¢çš„useråœ¨user_factorsé‚£ä¸ªembeddingè¡¨ä¸­æŸ¥å‡ºæ¥çš„ï¼Œå¤§å°æ˜¯batch_size(å‡ æ¡useræ•°æ®)*n_factors
        '''
        movies = self.movie_factors(x[:,1]) #titleçš„embedding
        res = (users * movies).sum(dim=1, keepdim=True)
        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) #userçš„å‘é‡*titleçš„å‘é‡+userçš„åç½®+titleçš„åç½®
        return sigmoid_range(res, *self.y_range) #è¾“å‡ºå‹ç¼©èŒƒå›´
```

| epoch | train_loss | valid_loss |  time |
| ----: | ---------: | ---------: | ----: |
|     0 |   0.897588 |   0.936690 | 00:09 |
|     1 |   0.583255 |   0.917793 | 00:08 |
|     2 |   0.387062 |   0.939465 | 00:08 |
|     3 |   0.329430 |   0.955679 | 00:08 |
|     4 |   0.311792 |   0.955723 | 00:08 |

å¯ä»¥çœ‹åˆ°train_lossä¸€ç›´åœ¨å‡å°‘ï¼Œå¯æ˜¯valid_losså´å…ˆå‡å°‘å†å¢åŠ ï¼Œè¿™å…¶å®å°±æ„å‘³ç€åœ¨epoch=1çš„æ—¶å€™è¿‡æ‹Ÿåˆäº†

### 7.5 Weight Decay

å…·ä½“åŸç†å‚è€ƒPDL2022çš„4.7

å¯¹äºå¢åŠ äº† weight decayçš„lossæ¥è¯´ï¼Œå®ƒæ˜¯loss_with_wd = loss + wd * (parameters**2).sum()ï¼Œå…¶ä¸­parameterså°±æ˜¯æ‰€æœ‰çš„è®­ç»ƒç›®æ ‡å‚æ•°ã€‚

åœ¨pythonä¸­å¯ä»¥å†™æˆï¼š

```python
loss_with_wd = loss + wd * (parameters**2).sum()
```

ä½†å…¶å®åœ¨è®¡ç®—æ¢¯åº¦æ—¶éœ€è¦æ±‚å¯¼ï¼Œè€Œwd * (parameters\**2).sum()çš„å¯¼æ•°æ˜¯å¾ˆå¥½æ±‚å¾—çš„ï¼Œæ‰€ä»¥å®é™…ä¸Šåœ¨pythonä¸­ï¼Œå¯ä»¥ä¿ç•™lossï¼Œåœ¨æ¢¯åº¦ä¸Šç›´æ¥åŠ wd * (parameters**2).sum()çš„å¯¼æ•°ï¼Œå†™æˆï¼š

```python
parameters.grad += wd * 2 * parameters
```

ä¸è¿‡ä»¥ä¸Šçš„æˆ‘ä»¬åˆ†æ­¥å†™ç®—æ³•è¦è€ƒè™‘çš„äº‹æƒ…ï¼Œåœ¨fastaiä¸­ï¼Œæˆ‘ä»¬å·²ç»è€ƒè™‘äº†è¿™ä¸ªï¼Œå› æ­¤åªè¦åœ¨è®­ç»ƒä¸­è¿™æ ·å†™ï¼š

```python
model = DotProductBias(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3, wd=0.1) #å…¶ä¸­wdæ˜¯weight decayçš„é‚£ä¸ªæŠ˜å‡ç³»æ•°
```

### 7.6 ä»å¤´å¼€å§‹ååŒè¿‡æ»¤

```python
def create_params(size): #åˆ›å»ºåµŒå…¥çŸ©é˜µçš„å‡½æ•°
    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))
class DotProductBias(Module):
    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): #å¼•ç”¨å‡½æ•°ï¼Œåˆ›å»ºå¥½å‡ ä¸ªåµŒå…¥çŸ©é˜µ
        self.user_factors = create_params([n_users, n_factors])
        self.user_bias = create_params([n_users])
        self.movie_factors = create_params([n_movies, n_factors])
        self.movie_bias = create_params([n_movies])
        self.y_range = y_range

    def forward(self, x):  #åµŒå…¥çŸ©é˜µå’ŒåµŒå…¥çŸ©é˜µä¹‹é—´è®¡ç®—
        users = self.user_factors[x[:,0]]
        movies = self.movie_factors[x[:,1]]
        res = (users*movies).sum(dim=1)
        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]
        return sigmoid_range(res, *self.y_range)
model = DotProductBias(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat()) #è¿™ä¸ªLearnerè›®å¼ºå¤§
learn.fit_one_cycle(5, 5e-3, wd=0.1)
```

ä¸€äº›pytorchè¯­æ³•ä¸Šçš„ä¸œè¥¿â†“

```python
'(1)'
class T(Module):
    def __init__(self): self.a = torch.ones(3)
L(T().parameters())

'(2)'
class T(Module):
    def __init__(self): self.a = nn.Parameter(torch.ones(3))
L(T().parameters())

'''
ä¸Šé¢è¿™ä¸¤ä¸ªæ¡ˆä¾‹ä¸­ï¼Œï¼ˆ1ï¼‰è¿”å›[]ï¼Œï¼ˆ2ï¼‰è¿”å›tensorï¼Œè¿™æ˜¯å› ä¸ºç»§æ‰¿nn.Moduleåï¼Œéœ€è¦é€šè¿‡nn.Parameter()å‘Šè¯‰Moduleï¼Œå…¶ä¸­çš„å˜é‡æ˜¯ç›®æ ‡å‚æ•°
'''

'(3)'
class T(Module):
    def __init__(self): self.a = nn.Linear(1, 3, bias=False)
t = T()
L(t.parameters())

'''
ä¸è¿‡ï¼ˆ3ï¼‰æ²¡æœ‰åˆ›å»ºnn.Parameter()ç±»ï¼Œä½†ä¾ç„¶å¯ä»¥è¿”å›ä¸€ä¸ªtensorï¼Œè¿™æ˜¯å› ä¸ºnn.Linear()æ˜¯pytorchä¸­çš„ç±»ï¼Œpytorchå·²ç»åœ¨nn.Linearè¿™ä¸ªæ¨¡å—çš„å†…éƒ¨å¸®ä½ ä½¿ç”¨nn.ParameteråŒ…è£…å¥½äº†ï¼Œæœ€ç»ˆè°ƒç”¨ t.parameters() æ—¶ï¼Œæ‹¿åˆ°çš„æ˜¯å®ƒå†…éƒ¨å·²ç»åˆ›å»ºå¹¶åŒ…è£…å¥½çš„å‚æ•°ã€‚
'''
```

### 7.7 ä¸€äº›æ¨¡å‹è§£é‡Š

#### 7.7.1 åç½®çš„è§£é‡Š

ä»¥ä¸‹æ˜¯åå·®å‘é‡ä¸­å€¼æœ€ä½çš„ç”µå½±ï¼š

åœ¨ç”¨æˆ·å¯¹ç”µå½±çš„è¯„ä»·ä¸­ï¼Œå³ä½¿ç”¨æˆ·ä¸å…¶æ½œåœ¨å› ç´ éå¸¸åŒ¹é…ï¼ˆç¨åæˆ‘ä»¬å°†çœ‹åˆ°ï¼Œè¿™äº›å› ç´ å¾€å¾€ä»£è¡¨åŠ¨ä½œæ°´å¹³ã€ç”µå½±å¹´é¾„ç­‰ç­‰ï¼‰ï¼Œä»–ä»¬é€šå¸¸ä»ç„¶ä¸å–œæ¬¢å®ƒï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ä¸ä»…ä»…æ˜¯ç”µå½±æ˜¯äººä»¬ä¸å–œæ¬¢è§‚çœ‹çš„ç±»å‹ï¼Œè€Œä¸”å³ä½¿æ˜¯ä»–ä»¬æœ¬æ¥ä¼šå–œæ¬¢çš„ç±»å‹ï¼Œäººä»¬ä¹Ÿå€¾å‘äºä¸å–œæ¬¢è§‚çœ‹ã€‚

```python
movie_bias = learn.model.movie_bias.squeeze()
idxs = movie_bias.argsort()[:5]
[dls.classes['title'][i] for i in idxs]
```

#### 7.7.2 åµŒå…¥çŸ©é˜µçš„è§£é‡Š

å¯¹ç”µå½±çš„EmbeddingçŸ©é˜µè¿›è¡Œä¸»æˆåˆ†åˆ†æPCAï¼Œå°†å®ƒä»¬é™æˆ2ç»´ï¼Œç„¶åç»˜åˆ¶åœ¨åæ ‡ç³»é‡Œã€‚å°±å¯ä»¥çœ‹åˆ°ï¼Œè®­ç»ƒå‡ºæ¥çš„EmbeddingçŸ©é˜µå·²ç»éšå«äº†èšç±»æ¡ä»¶ã€‚

æˆ‘æ˜¯æ€ä¹ˆéƒ½æ²¡çœ‹å‡ºä»–è¯´çš„éšå«ä¿¡æ¯ï¼Œæ„Ÿè§‰å°±æ˜¯ä¸€å †ç‚¹ã€‚

```python
g = ratings.groupby('title')['rating'].count()
top_movies = g.sort_values(ascending=False).index.values[:1000]
top_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])
movie_w = learn.model.movie_factors[top_idxs].cpu().detach()
movie_pca = movie_w.pca(3)
fac0,fac1,fac2 = movie_pca.t()
idxs = list(range(50))
X = fac0[idxs]
Y = fac2[idxs]
plt.figure(figsize=(12,12))
plt.scatter(X, Y)
for i, x, y in zip(top_movies[idxs], X, Y):
    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)
plt.show()
```

#### 7.7.3 å¼•å¯¼æ¨¡å‹

ä¸€ä¸ªæ–°æ³¨å†Œç”¨æˆ·æ€ä¹ˆç¡®å®štaçš„åµŒå…¥çŸ©é˜µï¼Ÿå¯ä»¥é€‰æ‹©â€œå¹³å‡å€¼â€æ¥ä»£è¡¨taï¼Œè¿™ä¸ªå¹³å‡å€¼é€‰è‡ªæŸä¸ªç‰¹å®šç”¨æˆ·ã€‚

å½“ç„¶ç”¨æˆ·åœ¨æ³¨å†Œæ—¶ä¼šå¡«å†™ä¸€äº›è¡¨æ ¼ï¼Œå¯ä»¥ç”¨å®ƒæ¥æ„å»ºåˆå§‹åµŒå…¥å‘é‡ã€‚å½“ç”¨æˆ·æ³¨å†Œæ—¶ï¼Œè€ƒè™‘ä¸€ä¸‹æ‚¨å¯ä»¥è¯¢é—®å“ªäº›é—®é¢˜æ¥å¸®åŠ©æ‚¨äº†è§£ä»–ä»¬çš„å£å‘³ã€‚ç„¶åï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œå…¶ä¸­å› å˜é‡æ˜¯ç”¨æˆ·çš„åµŒå…¥å‘é‡ï¼Œè€Œè‡ªå˜é‡æ˜¯æ‚¨é—®ä»–ä»¬çš„é—®é¢˜çš„ç»“æœï¼Œä»¥åŠä»–ä»¬çš„æ³¨å†Œå…ƒæ•°æ®ã€‚

ä½†æ˜¯åœ¨ç§¯ç´¯ç”¨æˆ·çš„è¿‡ç¨‹ä¸­ç”±äºæ­£åé¦ˆå¾ªç¯å¯èƒ½ä¼šä½¿ç³»ç»Ÿæ¶åŒ–ï¼Œä¾‹å¦‚ï¼Œåœ¨ç”µå½±æ¨èç³»ç»Ÿä¸­ï¼Œçœ‹åŠ¨æ¼«çš„äººå¾€å¾€ä¼šçœ‹å¾ˆå¤šåŠ¨æ¼«ï¼Œè€Œä¸”ä¸æ€ä¹ˆçœ‹å…¶ä»–ä¸œè¥¿ï¼ŒèŠ±å¾ˆå¤šæ—¶é—´åœ¨ç½‘ç«™ä¸Šè¯„åˆ†ï¼Œå› æ­¤ï¼ŒåŠ¨æ¼«å¾€å¾€åœ¨è®¸å¤šâ€œæœ‰å²ä»¥æ¥æœ€ä½³ç”µå½±â€åˆ—è¡¨ä¸­è¢«è¿‡åº¦ä»£è¡¨ï¼Œå¹¶ä¸”ä¼šå¸å¼•æ›´å¤šçš„äººçœ‹åŠ¨æ¼«å¹¶æ‰“åˆ†ã€‚è¿™æ ·ç³»ç»Ÿå¯¹ç”¨æˆ·æ¨èå†…å®¹çš„åˆ¤æ–­å°±ä¼šæ…¢æ…¢ä¸å‡†ç¡®ã€‚è¿™ç§åè§å¾ˆå¤šæ—¶å€™æ˜¯éå¸¸ä¸æ˜æ˜¾çš„ï¼Œæ‚¨åº”è¯¥å‡è®¾æ‚¨ä¼šçœ‹åˆ°å®ƒä»¬ï¼Œä¸ºæ­¤åšå¥½è®¡åˆ’ï¼Œå¹¶æå‰ç¡®å®šå¦‚ä½•å¤„ç†è¿™äº›é—®é¢˜ï¼Œå°è¯•è€ƒè™‘åé¦ˆå¾ªç¯å¯èƒ½åœ¨æ‚¨çš„ç³»ç»Ÿä¸­è¡¨ç¤ºçš„æ‰€æœ‰æ–¹å¼ï¼Œä»¥åŠæ‚¨å¦‚ä½•èƒ½å¤Ÿåœ¨æ•°æ®ä¸­è¯†åˆ«å®ƒä»¬ã€‚è¿™ä¸€åˆ‡éƒ½æ˜¯ä¸ºäº†ç¡®ä¿æœ‰äººå‚ä¸å…¶ä¸­ï¼›æœ‰ä»”ç»†çš„ç›‘æ§ï¼Œä»¥åŠä¸€ä¸ªæ¸è¿›å’Œå‘¨åˆ°çš„æ¨å‡ºã€‚

### 7.8 fastaiåšååŒè¿‡æ»¤

```python
learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))
learn.fit_one_cycle(5, 5e-3, wd=0.1)
learn.model
'''
EmbeddingDotBias(
  (u_weight): Embedding(944, 50)
  (i_weight): Embedding(1665, 50)
  (u_bias): Embedding(944, 1)
  (i_bias): Embedding(1665, 1)
)
'''
#åç½®
movie_bias = learn.model.i_bias.weight.squeeze()
idxs = movie_bias.argsort(descending=True)[:5]
[dls.classes['title'][i] for i in idxs]
#åµŒå…¥è·ç¦»ï¼šé€šè¿‡æ‰¾åˆ°è·ç¦»'Silence of the Lambs, The (1991)'ä»£è¡¨çš„50å‘é‡æœ€è¿‘çš„å¦ä¸€ä¸ª50å‘é‡ï¼Œå¯¹åº”çš„ç”µå½±ä¸ä¹‹æœ€ç±»ä¼¼çš„ç”µå½±
movie_factors = learn.model.i_weight.weight
idx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']
distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])
idx = distances.argsort(descending=True)[1]
dls.classes['title'][idx]
```

### 7.9 ååŒè¿‡æ»¤çš„æ·±åº¦å­¦ä¹ 

ä¸Šé¢éƒ½æ˜¯ç‚¹ç§¯æ¨¡å‹ï¼Œç§°ä¸ºæ¦‚ç‡çŸ©é˜µåˆ†è§£ï¼ˆPMFï¼‰ã€‚å¦ä¸€ç§æ–¹æ³•ä¹Ÿä¼šæœ‰ç±»ä¼¼æ•ˆæœï¼Œæ˜¯æ·±åº¦å­¦ä¹ ã€‚

```python
embs = get_emb_sz(dls) #è¿™æ˜¯åŸºäºdlsï¼Œfastaiç»™å‡ºçš„userå’Œitemçš„embeddingæ¨èå¤§å°
'[(944, 74), (1635, 101)]'

class CollabNN(Module):
    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):
        self.user_factors = Embedding(*user_sz)
        self.item_factors = Embedding(*item_sz)
        self.layers = nn.Sequential(
            nn.Linear(user_sz[1]+item_sz[1], n_act),
            nn.ReLU(),
            nn.Linear(n_act, 1))
        self.y_range = y_range

    def forward(self, x): #xæ˜¯ä¸€ä¸ªå¤§å°ä¸º(batch_sz,2)çš„tensorï¼Œç¬¬0åˆ—æ˜¯userï¼Œç¬¬1åˆ—æ˜¯item
        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])
        x = self.layers(torch.cat(embs, dim=1))
        return sigmoid_range(x, *self.y_range)
    
model = CollabNN(*embs)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3, wd=0.01)
```

ç”¨fastaiå¯ä»¥åˆ›å»ºè¿™ä¸ªç®—æ³•ï¼š

```python
learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) 
#use_nn=Trueï¼Œå®ƒå°±çŸ¥é“è¦ç”¨æ·±åº¦å­¦ä¹ ï¼ŒembeddingçŸ©é˜µå¤§å°ç”¨get_emb_szçš„æ¨èå€¼
learn.fit_one_cycle(5, 5e-3, wd=0.1)
```

æ·±åº¦å­¦ä¹ æ–¹æ³•æœ¬è´¨ä¸Šå·²ç»ä¸å†æ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„ååŒè¿‡æ»¤ï¼Œè€Œæ˜¯â€œæ­£å¸¸çš„â€æ·±åº¦å­¦ä¹ ï¼Œä½†å› ä¸ºå®ƒçš„ç›®çš„ï¼Œå°±ä¾ç„¶ä¿ç•™äº†ååŒè¿‡æ»¤çš„åå­—ã€‚

ä¸¤è€…å¯ä»¥æ··åˆä½¿ç”¨ï¼š

```py
# ç»“åˆç‚¹ç§¯å’Œç¥ç»ç½‘ç»œçš„ä¼˜åŠ¿
def forward(self, x):
    user_emb = self.user_emb(x[:,0])
    movie_emb = self.movie_emb(x[:,1])
    
    # ç‚¹ç§¯éƒ¨åˆ†ï¼ˆå¯è§£é‡Šçš„ç›¸ä¼¼åº¦ï¼‰
    dot_product = (user_emb * movie_emb).sum(dim=1, keepdim=True)
    
    # ç¥ç»ç½‘ç»œéƒ¨åˆ†ï¼ˆå¤æ‚äº¤äº’ï¼‰
    mlp_output = self.mlp(torch.cat([user_emb, movie_emb], dim=1))
    
    # èåˆè·¯å¾„ï¼šæ‹¼æ¥ä¸¤è·¯è¾“å‡º
    concat = torch.cat([gmf_out, mlp_out], dim=-1)
    out = torch.sigmoid(self.output_layer(concat))
    return out
```

| æ¨¡å‹  | æ ¸å¿ƒæ€æƒ³          | æ˜¯å¦çº¿æ€§ | å‚æ•°é‡ | å¯è§£é‡Šæ€§ | èƒ½å¦èåˆç‰¹å¾ | ä¼˜ç‚¹                 | ç¼ºç‚¹               |
| ----- | ----------------- | -------- | ------ | -------- | ------------ | -------------------- | ------------------ |
| PMF   | ç”¨æˆ·â€“ç‰©å“å†…ç§¯é¢„æµ‹ | âœ… çº¿æ€§   | å°‘     | å¼º       | âŒ            | ç®€æ´é«˜æ•ˆï¼Œå¯è§£é‡Šæ€§å¥½ | è¡¨è¾¾èƒ½åŠ›æœ‰é™       |
| NCF   | ç”¨MLPæ›¿ä»£å†…ç§¯     | âŒ éçº¿æ€§ | å¤š     | å¼±       | âœ…            | èƒ½å»ºæ¨¡å¤æ‚äº¤äº’       | è®­ç»ƒæ…¢ï¼Œè¿‡æ‹Ÿåˆé£é™© |
| NeuMF | èåˆGMF + MLP     | âœ…+âŒ æ··åˆ | å¤š     | ä¸­       | âœ…            | å…¼é¡¾çº¿æ€§ä¸éçº¿æ€§     | æ¨¡å‹å¤æ‚ã€è°ƒå‚éš¾   |

| ç¼©å†™  | å…¨ç§°ï¼ˆè‹±æ–‡ï¼‰                       | ä¸­æ–‡åç§°     | è¯´æ˜                                      |
| ----- | ---------------------------------- | ------------ | ----------------------------------------- |
| PMF   | Probabilistic Matrix Factorization | æ¦‚ç‡çŸ©é˜µåˆ†è§£ | ä¼ ç»ŸååŒè¿‡æ»¤çš„æ¦‚ç‡å»ºæ¨¡å½¢å¼                |
| GMF   | Generalized Matrix Factorization   | å¹¿ä¹‰çŸ©é˜µåˆ†è§£ | ç”¨ç¥ç»ç½‘ç»œæ›¿ä»£å†…ç§¯çš„PMFå˜ä½“               |
| MLP   | Multi-Layer Perceptron             | å¤šå±‚æ„ŸçŸ¥æœº   | ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå­¦ä¹ ç”¨æˆ·-ç‰©å“éçº¿æ€§å…³ç³»     |
| NCF   | Neural Collaborative Filtering     | ç¥ç»ååŒè¿‡æ»¤ | å°†GMFä¸MLPç»“åˆçš„ç»Ÿä¸€æ¡†æ¶                  |
| NeuMF | Neural Matrix Factorization        | ç¥ç»çŸ©é˜µåˆ†è§£ | NCFçš„å…·ä½“å®ç°ï¼Œç”¨ç¥ç»ç½‘ç»œèåˆGMFå’ŒMLPéƒ¨åˆ† |

## 8: Convolutions (CNNs)

### 8.1 Embedding è¡¥å……

- å…³äºembeddingï¼Œåœ¨fastaiä¸­ï¼Œåˆ›å»ºtabular_learnerç±»æ—¶ï¼Œä¼ é€’ç»™å®ƒdlsï¼Œdlså·²ç»æŒ‡æ˜äº†å“ªäº›æ˜¯è¿ç»­å˜é‡å“ªäº›æ˜¯åˆ†ç±»å˜é‡ï¼Œtabular_larnerä¼šè‡ªåŠ¨åœ°ä¸ºåˆ†ç±»å˜é‡ä½¿ç”¨embeddingã€‚å¯è§tabular_learnerè¿‡äºé¡¶å±‚ï¼Œå¾ˆå¤šæ—¶å€™è¦è‡ªå·±æ„å»ºDLç»“æ„ï¼Œè¿˜æ˜¯å¾—ç”¨Learner
- åœ¨Entity Embeddings of Categorical Variablesè¿™ç¯‡2016å¹´çš„è®ºæ–‡ä¸­ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè®­ç»ƒäº†embeddingsï¼Œç„¶åå›ºå®šembeddingsï¼Œå°†å®ƒç”¨äºå…¶å®ƒç®—æ³•çš„åˆ†ç±»å˜é‡çš„è¡¨ç¤ºï¼Œå‘ç°æ‰€æœ‰çš„ç®—æ³•performanceéƒ½æå‡äº†ï¼Œè¿™äº›ç®—æ³•åŒ…æ‹¬ç¥ç»ç½‘ç»œã€éšæœºæ£®æ—ã€gradient boosted treesç­‰ã€‚

### 8.2 CNNs

åœ¨CNNsä¸­ï¼Œå¯ä»¥å¯¹æŸä¸€å±‚çš„activationsè¿›è¡Œmax poolingï¼Œä¸è¿‡ç°åœ¨æ›´åŠ å¸¸ç”¨çš„æ–¹å¼æ˜¯é€šè¿‡è®¾ç½®strideç¼©å°activationsçš„å½¢çŠ¶ã€‚

å¦å¤–åœ¨æœ€åä¸€ä¸ªè¾“å‡ºå±‚ï¼Œä»¥å‰æ˜¯ç”¨activationsâœ–dense weightsï¼Œç°åœ¨æ›´å¸¸è§çš„æ˜¯é€šè¿‡strideå°†activationså½¢çŠ¶é€æ­¥ç¼©å°åˆ°7*7ï¼Œç„¶åæ±‚è¿™7\*7=49ä¸ªæ•°å­—çš„å¹³å‡å€¼ï¼ˆaverage poolingï¼‰ï¼Œæœ€åå†ç”¨å…¨è¿æ¥æˆ–softmaxè¾“å‡ºã€‚ï¼ˆGAP- Global average poolingï¼‰

ä¸€äº›ç›´è§‚çš„è§£é‡Šï¼šéšç€å±‚æ•°çš„å¢åŠ ï¼Œæˆ‘ä»¬æœ€åä¼šè·å¾—å¾ˆå¤š7*7çš„å°å—ï¼Œæ¯”å¦‚è¾“å‡º[batch, 512, 7, 7]ï¼Œå¤„ç†åå¾—åˆ°[batch, 512]ã€‚ä½†åˆ°åº•æ˜¯averageè¿˜æ˜¯max poolingè¿˜æ˜¯è¦çœ‹å®é™…åº”ç”¨ã€‚å¦‚æœæ˜¯ç›®æ ‡è¯†åˆ«ï¼Œè¯†åˆ«çš„ç›®æ ‡é•¿å¾—å¾ˆå°ï¼Œæˆ‘ä»¬å¯èƒ½è¦max poolingï¼ˆGMP- Global max poolingï¼‰ã€‚

### 8.3 Drop out

åœ¨CNNsçš„æŸä¸€å±‚ï¼Œæˆ‘ä»¬å†³å®šä½¿ç”¨Dropoutï¼Œæˆ‘ä»¬åšçš„äº‹æƒ…å¦‚ä¸‹ï¼š

- è®¾ç½®ä¸€ä¸ªdropoutå€¼nï¼Œ0~1ä¹‹é—´ï¼›
- æ„å»ºä¸€ä¸ªä¸è¯¥å±‚activationsç›¸åŒå½¢çŠ¶çš„çŸ©é˜µfilterï¼Œå…¶ä¸­æœ‰næ˜¯0ï¼Œ1-næ˜¯1ï¼›
- ç”¨activationsâœ–filter

è¿™æ ·å°±ç›¸å½“äºåœ¨æŸä¸€å±‚éšæœºä¸¢å¼ƒäº†ä¸€éƒ¨åˆ†ä¿¡æ¯ï¼Œç›¸å½“äºåœ¨æŸä¸€å±‚è¿›è¡Œäº†æ•°æ®å¼ºåŒ–ï¼Œä¹Ÿå¯ä»¥å¸®åŠ©æˆ‘ä»¬é¿å…over fittingã€‚

### 8.4 activation functions

æ¿€æ´»å‡½æ•°

ç”¨ä»€ä¹ˆæ ·çš„æ¿€æ´»å‡½æ•°å¤šåŠå¯¹æ¨¡å‹è¡¨ç°æ²¡å¤ªå¤šå½±å“ï¼Œåªè¦å®ƒæ˜¯ä¸€ä¸ªéçº¿æ€§çš„ã€‚
