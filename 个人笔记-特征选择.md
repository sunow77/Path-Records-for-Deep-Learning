# 一、特征独立性对深度学习的影响

| **特征独立性** | **深度学习模型的表现**                               | **影响**                                                     | **解决策略**                                                 |
| -------------- | ---------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **完全独立**   | 模型能够正常训练并捕捉到特征之间的关系               | - 模型可以独立地学习每个特征的信息。 - 通常没有冗余信息，避免过拟合。 | - 不需要特别处理，正常训练即可。 - 可使用正则化防止过拟合。  |
| **部分相关**   | 模型可能仍然能够捕捉到特征之间的部分依赖关系         | - 一定程度的冗余可能会影响训练速度和模型复杂度。 - 如果相关性过高，可能导致训练不稳定。 | - 可以使用特征选择、PCA 或 L1 正则化减少冗余。 - 使用Dropout提高泛化能力。 |
| **高度相关**   | 模型可能会出现过拟合，训练速度变慢，模型稳定性差     | - 高度相关的特征提供冗余信息，导致模型可能依赖相似信息，增加过拟合的风险。 - 可能导致梯度消失/爆炸问题，影响收敛。 | - 使用PCA进行降维，减少相关特征的维度。 - 应用L1正则化去除冗余特征。 - 使用更多的数据增强和正则化方法。 |
| **完全不独立** | 如果特征之间的依赖关系过于复杂，模型可能无法高效训练 | - 极其复杂的特征依赖关系可能导致模型难以优化。 - 特征间的干扰可能使得模型对噪声敏感。 | - 通过模型调优（如更好的权重初始化和学习率调节）缓解干扰。 - 适当降维和特征选择。 |

## 1 解决方案

### 1.1 **使用PCA（主成分分析）降维**

- **PCA** 可以将冗余特征进行降维，将相关性较高的特征映射到新的主成分空间中，去掉不必要的冗余信息。
- 这有助于减少模型的复杂度，并减少**过拟合风险**。

### 1.2 **特征选择（Feature Selection）**

- 可以通过 **特征重要性评估**（比如通过L1正则化、树模型的特征重要性等）选择出**最重要的特征**，剔除冗余或者低相关性的特征。

### 1.3 正则化方法（也是特征选择的一种）

- **L1正则化**：能够强制模型“选择”较为重要的特征并“丢弃”那些冗余或不重要的特征，这样即使有高度相关的特征，模型也能“自动”挑选最优的表示。
- **Dropout**：它在训练过程中随机丢弃一些神经元，能有效避免神经网络过于依赖某个特征或输入的某些信息。

### 1.4 数据增强

- 对于特征高度相关的情况，可以通过数据增强技术来帮助模型学到更多不同的数据模式，从而减轻训练中冗余特征带来的问题。

## 2 方案选择

### 2.1 做PCA还是特征选择？

#### （1） 做PCA，还是做特征选择？

- **PCA适合做降维**，当你面临的数据集特征较多，且你知道有些特征之间存在较高相关性时，使用PCA降维能够显著减少特征空间的维度，降低模型的复杂度，同时减少**多重共线性**问题。
- **特征选择适合去除无用或冗余特征**，如果你已经知道哪些特征可能对模型预测不重要或者高度冗余，直接使用特征选择方法（如基于模型的特征选择、L1正则化等）能更好地精简特征集，提升模型的**解释性**。
#### （2） 结合使用：PCA + 特征选择

有时，PCA 和特征选择可以结合使用：

- **首先进行PCA降维**：当特征数量非常大时，首先应用PCA来降维，减少特征的维度，去掉冗余信息。

- **然后进行特征选择**：在PCA降维后，剩下的特征（即主成分）如果还包含无关或冗余的信息，可以使用特征选择方法进一步挑选重要特征。

这种**先降维，再选择**的方法可以减少特征的数量，同时保留最具信息量的部分。
#### （3） 对比

| **情境**                     | **选择PCA**                                      | **选择特征选择**                                           |
| ---------------------------- | ------------------------------------------------ | ---------------------------------------------------------- |
| **特征之间相关性高**         | ✅ PCA 可以有效去除冗余信息，减少相关性问题。     | ❌ 特征选择可能会很难处理高度相关的特征。                   |
| **特征数量庞大**             | ✅ PCA 降低特征维度，提高计算效率。               | ❌ 特征选择可能无法减少维度，特别是当特征过多时。           |
| **需要保持特征的可解释性**   | ❌ PCA 转化后的特征是主成分，可能失去可解释性。   | ✅ 特征选择保持原始特征，可以清楚地理解每个特征的作用。     |
| **特征中包含无关信息或噪声** | ✅ PCA 可以去掉噪声，但不一定能完全去除无关信息。 | ✅ 特征选择可以直接去掉那些无关的特征。                     |
| **计算复杂度**               | ✅ PCA 可以显著减少维度，降低计算复杂度。         | ✅ 特征选择可以通过简单方法（如基于模型的重要性）快速筛选。 |

- 如果特征之间高度相关，且你不需要每个特征的具体含义：
  * 推荐先进行 **PCA** 降维。这样可以减少冗余信息，提高模型效率。
  * 如果你发现降维后的主成分中仍有无关特征，可以再应用特征选择进一步精简。

- 如果你关心模型的可解释性，想要保留原始特征：
  * 推荐使用 **特征选择**。这样能够更直观地了解哪些特征对预测最重要。

- 如果特征数量非常庞大，且部分特征冗余：
  * 可以尝试 **先做PCA降维**，然后再通过 **特征选择** 来选择最重要的主成分。这样可以在降低维度的同时减少无关特征。

### 2.2 高度相关特征筛选

#### （1）线性关系

- 如果你想要简单、快速地识别特征之间的线性相关性，使用**相关系数矩阵**是一个非常好的选择。它能够帮助你快速了解各个特征之间的关系，特别是当数据集中的特征数量较少时。

- 如果你需要更加深入地检测特征的共线性，并且希望获得每个特征的具体多重共线性评估，那么**VIF**是更适合的方法，特别是在回归模型中使用时，可以量化特征之间的影响。
- 在一些情况下，你可以结合两者一起使用。首先通过**相关系数矩阵**查看哪些特征之间高度相关，然后使用**VIF**进一步确定每个特征的多重共线性情况，确保你做出合理的特征选择。

| **特征**     | **相关系数矩阵**                                     | **方差膨胀因子（VIF）**                                      |
| ------------ | ---------------------------------------------------- | ------------------------------------------------------------ |
| **目的**     | 计算特征之间的线性关系，识别高度相关的特征对。       | 测量一个特征与其他特征的多重共线性，计算特征的膨胀程度。     |
| **适用场景** | 适用于检查两两特征之间的线性相关性。                 | 适用于检测多个特征之间的多重共线性，特别适合回归模型。       |
| **计算方式** | 计算每对特征之间的相关系数，通常使用皮尔逊相关系数。 | 通过回归模型计算每个特征的VIF值，评估该特征与其他特征的共线性程度。 |
| **输出**     | 相关系数矩阵，呈现所有特征之间的相关性。             | VIF值，每个特征一个值。VIF较高的特征说明其与其他特征共线性较强。 |
| **优点**     | - 直观易懂，便于通过热力图查看高度相关的特征对。     | - 可以量化每个特征的多重共线性程度，帮助识别需要去除的特征。 |
| **缺点**     | - 只考虑特征之间的直接相关性，不能处理复杂的共线性。 | - 计算较复杂，且仅适用于回归模型，不适用于分类任务。         |
| **优选情况** | - 如果特征之间的相关性较为简单，且数据量较少。       | - 如果特征之间可能存在更复杂的共线性，并且你关心各个特征的膨胀因子。 |
| **缺点情况** | - 当特征数量较多时，相关系数矩阵会变得难以解释。     | - VIF值过高时可能导致模型不稳定，且VIF高的特征不一定是冗余的。 |

#### （2）非线性关系

- **互信息（Mutual Information）**：互信息是衡量特征和目标变量之间关系的一个指标，它不仅仅适用于线性关系，还能捕捉到**非线性**的关系。你可以通过计算特征与目标变量之间的互信息来评估特征的重要性。
- **主成分分析（PCA）**：PCA 是一种降维方法，尽管它通常用于线性数据，但如果数据呈现复杂的非线性结构，可以考虑使用**核PCA**。核PCA能够通过使用核函数将数据映射到更高维的空间，捕捉非线性关系。
- **使用神经网络自编码器（Autoencoder）**：自编码器是一种无监督学习算法，可以学习数据的低维表示，特别适合捕捉复杂的非线性关系。通过这种方法，可以将数据映射到低维空间，从而减少特征冗余。【不是直接抛弃冗余信息，是通过压缩缓解问题】

| **方法**         | **适用场景**                                       | **优点**                     | **缺点**                             |
| ---------------- | -------------------------------------------------- | ---------------------------- | ------------------------------------ |
| **互信息**       | 适用于捕捉特征与特征之间以及特征与目标之间的依赖性 | 适用于线性和非线性关系       | 计算复杂度较高，且不提供显著性检验   |
| **相关系数矩阵** | 适用于线性关系的特征对                             | 直观易懂，适合发现线性相关性 | 只能处理线性关系，无法捕捉非线性关系 |
| **VIF**          | 适用于回归模型中的多重共线性检测                   | 能量化每个特征的共线性程度   | 主要处理线性关系，且计算较为复杂     |

#### （3）高维空间特征相似性

- 距离/相似度分析（如余弦相似度），图像、文本领域也可以扩展使用。

### 2.3 L1正则化（嵌入法的一种）

#### （1）L1

通常，在训练模型时，优化目标是最小化损失函数，而L1正则化的作用是修改损失函数，使得在计算损失时，除了预测误差，还要加上一个**权重的L1范数**（即权重的绝对值之和）。

| **特性**       | **L1正则化**                     | **L2正则化**                       |
| -------------- | -------------------------------- | ---------------------------------- |
| **惩罚项**     | 权重的绝对值之和：(\sum_i        | w_i（权重的平方）                  |
| **稀疏性**     | 产生稀疏解，许多权重变为零       | 权重不会变为零，只会变得更小       |
| **效果**       | 自动进行特征选择，保留重要特征   | 平滑权重分布，避免过大的权重       |
| **优化特性**   | 对一些参数施加硬性约束，使其为零 | 权重分布更加均匀，不容易产生零权重 |
| **计算复杂度** | 相对较简单                       | 需要更多的计算                     |

小补充：如果你用的是神经网络

- 在深度学习中，L1 正则一般加在某一层的权重上。
- 虽然不如线性模型那样“完全可解释”，但你仍然可以查看参数矩阵，统计每个特征的归零情况。
- 想提升可解释性，也可以用像 Lasso（线性模型+L1）、或者先训练模型后用<u>**SHAP/LIME**等方法分析特征贡献</u>。

| 问题                        | 答案                                                       |
| --------------------------- | ---------------------------------------------------------- |
| L1 正则一定只加在第一层吗？ | ❌ 不一定，但 **做特征选择**时通常加在第一层                |
| 加在后面层有什么问题？      | 没法追踪到原始输入特征被谁“剪掉”了                         |
| 想筛选特征怎么办？          | ✅ 把 L1 正则加在第一层的权重矩阵上，训练后查看哪些列接近 0 |

#### （2）SHAP/LIME



# 二、特征选择的三个主要方法：

| 方法类型               | 原理概括                                               | 举个例子                                 |
| ---------------------- | ------------------------------------------------------ | ---------------------------------------- |
| **Filter（过滤法）**   | 用统计指标快速评估每个特征和目标之间的关系，按分数选   | 比如看哪些特征与预测值相关性高，留下它们 |
| **Wrapper（包装法）**  | 把特征子集喂给模型试试看效果好不好，用模型表现来选特征 | 比如试着加一个特征，看模型准确率是否提升 |
| **Embedded（嵌入法）** | 直接在模型训练过程中进行特征选择，模型自己决定重要性   | 比如用随机森林训练后自动排序特征重要性   |

## 1 过滤法

### 1.1 基本思路

> 用**统计指标**来评估每个特征与目标变量之间的“相关性”，再按得分高低进行筛选，留下“关系强”的特征，剔除“关系弱”的。

特点：
 ✅ 快速简单
 ✅ 可用于预处理阶段
 ⚠️ 不考虑特征之间的组合关系（只能单独看每个特征）

| 方法     | 适合什么数据 | 是否线性假设 | 优点                 | 缺点                 |
| -------- | ------------ | ------------ | -------------------- | -------------------- |
| 方差过滤 | 所有特征     | 无           | 快速剔除无变化的特征 | 不能识别与目标的关系 |
| 相关系数 | 连续变量     | 是           | 简单直观             | 只适用于线性关系     |
| 卡方检验 | 类别特征     | 否           | 分类任务常用         | 要求特征是离散的     |
| 互信息   | 所有变量     | 否           | 可以识别非线性       | 计算稍复杂           |
| 信息增益 | 分类数据     | 否           | 衡量分类效果强       | 可能偏向高基数特征   |

线性关系的要求：

| 方法                             | 特征与目标的关系             | 特征处理要求                         | 模型能学什么           |
| -------------------------------- | ---------------------------- | ------------------------------------ | ---------------------- |
| **线性回归、逻辑回归等传统模型** | 假设特征与目标是线性或单调的 | 需要手工构造和筛选线性特征           | 主要学线性关系         |
| **随机森林、XGBoost 等树模型**   | 不要求线性                   | 适应非线性，但容易过拟合             | 能学非线性，但解释性差 |
| **神经网络（深度学习）**         | ✅ 完全不要求线性             | 原始特征也能输入，模型自己建复杂关系 | 可学任意复杂的函数     |

### 1.2 常见方法

#### 1. 方差选择法（Variance Threshold）

- 原理：去掉方差接近 0 的特征（这些特征几乎是常数，对模型无贡献）
- 应用场景：常用于去除“重复值很多”的无效特征

------

#### 2. 相关系数法（Correlation Coefficient）

- 原理：用相关系数（如皮尔森系数）评估特征和目标变量的线性相关性
- 常用的相关系数：
  - **皮尔森相关（Pearson）**：衡量两个连续变量之间的线性关系
  - **斯皮尔曼相关（Spearman）**：适合排序变量，衡量单调关系
- 一般会设置阈值，如 |相关系数| > 0.2 的特征才保留

------

#### 3. 卡方检验（Chi-Square Test）

- 原理：检验两个类别变量之间是否独立（常用于分类任务）
- 应用：比如判断“天气类别”和“是否发生事故”是否有关
- 不能识别非线性关系

------

#### 4. 互信息（Mutual Information）——更适用于深度学习

- 原理：衡量两个变量之间的“信息共享量”，可以处理非线性关系

- 特点：比相关系数更强大，不要求线性相关，适合复杂场景

 | 任务场景                     | 推荐方法                     | 原因                               |
  | ---------------------------- | ---------------------------- | ---------------------------------- |
  | 你所有特征都是类别型（离散） | **卡方检验 or 互信息都可以** | 卡方解释性好，互信息更强但计算略慢 |
  | 你的特征中包含连续型变量     | ✅ **互信息更合适**           | 不需要分箱，也能捕捉非线性关系     |
  | 想捕捉复杂/非线性依赖        | ✅ **互信息强烈推荐**         | 卡方很可能漏掉它们                 |
  | 特征之间存在多种类型混合     | ✅ **互信息通吃**             | 比卡方通用性强太多                 |

- 它很全能
|优势 | 说明|
|---|---|
|✅ 非线性关系也能发现 | 卡方和皮尔森等线性方法可能错过这一类|
|✅ 支持连续和离散特征 | 你不需要为连续变量做“人工分箱”|
|✅ 结果是非负分数，容易理解 | 分数越高，表示“信息量”越大|
|✅ 对目标变量类型不挑 | 分类/回归任务都可以使用对应版本|
|✅ 无需强分布假设 | 不要求正态分布、线性关系等统计前提|

- 唯一注意的一些细节

| 注意点                                  | 解释                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| 🔄 sklearn 中默认使用 **近似估计方法**   | 在样本较少或特征分布稀疏时，结果可能不稳定（不过一般没啥大碍） |
| 🧮 连续特征的互信息估计使用核密度方法    | 如果你是数学控，可能会想手动优化带宽等参数                   |
| 🚫 不提供显著性检验（不像卡方那样有p值） | 如果你需要统计意义检验，就可能要补其他方法                   |
- 互信息没有显著性，如果想要“非线性特征”的显著性分析，可以：

| 场景                             | 建议方法                          |
| -------------------------------- | --------------------------------- |
| 要做建模用的特征选择             | ✅ 互信息 + Permutation Importance |
| 要分析模型中每个特征如何驱动预测 | ✅ SHAP（尤其对深度学习有效）      |
| 要做严谨的统计检验报告           | ✅ Kruskal-Wallis / Mann-Whitney   |

------

#### 5. 信息增益 / 增益率（Information Gain / Gain Ratio）

- 原理：衡量一个特征对目标变量的不确定性减少了多少
- 常见于：决策树建模时常用的特征选择依据

## 2 包装法

### 1.1 基本思路

**包装法**是通过选择一组特征，然后用这个特征子集训练一个机器学习模型，评估其性能。接着，通过不断地增加或删除特征来寻找最优特征子集。也就是说，它是通过“包裹”住一个学习算法来优化特征子集的。

如果特征维度非常高时，可能不太适用。

### 1.2 常见方法

#### （1）递归特征消除法（RFE, Recursive Feature Elimination）

### 1.3 评价

| 优点                                     | 缺点                                                         |
| ---------------------------------------- | ------------------------------------------------------------ |
| **高效**：能够找到真正对模型有用的特征   | **计算量大**：每次都会训练一个模型，计算开销高               |
| **能考虑特征之间的关系**：比过滤法更精确 | **容易过拟合**：特别是在特征数目很多时，容易在训练数据上过拟合 |
| **性能好**：通常能取得较好的预测效果     | **耗时**：需要多次训练模型，时间成本较高                     |

## 3 嵌入法

### 3.1 基本思路

嵌入法是将**特征选择**与**模型训练过程**结合起来的方法。与包装法和过滤法不同，嵌入法不仅依赖于外部的特征选择步骤，而是在**训练模型的过程中**进行特征选择。

步骤：

- 模型训练：在训练模型时，模型会自动地根据特征的重要性对特征进行选择或权重调整。

- 特征筛选：通过训练得到的**权重**、**系数**等，直接决定哪些特征更为重要，哪些可以忽略。

- 特征更新：基于模型的反馈（如权重的大小），模型会在训练过程中逐步优化特征。

### 3.2 常见方法

#### （1）L1正则化

#### （2）决策树

如随机森林、梯度提升树：树模型通过分裂节点来选择最重要的特征，特征的重要性可以通过树的深度或信息增益等度量。

步骤：

- 使用决策树模型（或其变体）对原始数据建模
  - 比如 `DecisionTreeClassifier`、`RandomForest`、`XGBoost`、`LightGBM` 等
  - 模型在训练过程中自动评估各个特征的重要性

- 根据特征重要性分数，筛选出前 N 个重要特征
  - 可以设定阈值（如特征重要性 > 0.01）
  - 也可以选固定数量（如选前 20 个最重要的特征）

- 将选出的特征输入深度学习模型

## 4 比较

方法回顾：

| 方法               | 选择时机          | 是否依赖模型 | 常见算法                     |
| ------------------ | ----------------- | ------------ | ---------------------------- |
| 过滤法（Filter）   | 模型训练前        | 否           | 方差、相关系数、互信息、卡方 |
| 包装法（Wrapper）  | 训练时 + 多次训练 | 是           | RFE、逐步选择、贝叶斯优化    |
| 嵌入法（Embedded） | 模型训练中        | 是           | 决策树、L1正则、稀疏模型     |

横向比较：

| 维度                   | 过滤法                 | 包装法                       | 嵌入法                   |
| ---------------------- | ---------------------- | ---------------------------- | ------------------------ |
| **速度**               | ✅ 非常快               | ❌ 非常慢                     | ✅ 中等偏快               |
| **适配非线性特征**     | ❌ 一般（如互信息稍好） | ✅ 很强                       | ✅ 很强（特别是树模型）   |
| **对深度学习的配合度** | ⭘ 有一定作用           | ✅ 效果最优，但代价大         | ✅ 效果好，代价低         |
| **解释性**             | ✅ 明确、独立           | ✅ 明确                       | ⭘ 依赖模型结构，不易解释 |
| **稳定性**             | ⭘ 特征间冗余难处理     | ❌ 容易不稳定（小变化大影响） | ✅ 稳定性较高             |

结论与建议：

> **“嵌入法 > 过滤法 > 包装法”**

- **嵌入法**（如基于树模型的特征重要性）
  - 通常最推荐，因为它能挖掘非线性关系，稳定、速度适中
  - 特别适合预处理复杂特征送入深度模型
- **过滤法**
  - 在特征维度特别高时（比如文本、图像），先做一轮过滤法非常有效
  - 可与嵌入法搭配使用
- **包装法**
  - 理论上效果最好（精细评估模型性能），但由于其计算极度消耗资源，**不适合用于深度学习前的大规模筛选**

| 场景                             | 推荐方法                                    |
| -------------------------------- | ------------------------------------------- |
| 特征维度很高（如图像、文本）     | 先过滤法（如方差/互信息）+ 嵌入法（树模型） |
| 特征较少，但质量不一             | 嵌入法优先（可配合 L1 正则）                |
| 模型训练资源充足、小规模探索     | 可以尝试包装法做补充对比                    |
| 要用深度模型，但不确定特征重要性 | 用 LightGBM/XGBoost 提前排序做嵌入法筛选    |

# 三、总结

**目前进行特征工程是两方面选择：**

（1）特征和特征之间具有关联，训练模型冗余较多，容易过拟合；

（2）特征和因变量之间关联性不足，训练效果不好。

**对（1），有4种方式：**

- PCA主成成分分析（降维）
- 特征选择
  - 相关系数矩阵（线性）、互信息（非线性）（特征-特征、特征-因变量关系）
  - VIF（线性）（专门用于特征-特征关系）
  - L1正则化（非线性）（在神经网络中直接实现）
  - 距离/相似度分析（如余弦相似度）（高维度相似性）

- 神经网络自编码器（不直接筛出冗余，而是通过压缩缓解问题）
- 数据增强（通过数据缓解问题）

**对（2），有3种方式： **

- 过滤法

- 包装法

- 嵌入法

  - L1正则化

  - 树模型（决策树、随机森林、XGBoost等）
