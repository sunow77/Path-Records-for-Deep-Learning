**<font size=8>fast.ai</font>**

## 0

| æ ‡é¢˜ | æ—¥æœŸ | ç¬”è®° |
| :--: | :--: | :--: |
|[å®˜ç½‘](https://www.fast.ai/)|-|-|
|[è®ºå›](https://forums.fast.ai/)|-|åŒ…å«é’ˆå¯¹å„ç‰ˆæœ¬è¯¾ç¨‹çš„æ¨¡å—|
|[Practical Deep Learning for Coders 2022](https://course.fast.ai/)|2022|2022ç‰ˆï¼Œ2 parts|
|Practical Deep Learning for Coders 2020|2020|2020ç‰ˆï¼Œå·²ç»è¢«æ›¿ä»£|
|[Practical Deep Learning for Coders 2019](https://course19.fast.ai/ )|2019|2019ç‰ˆï¼Œ2 partsï¼›[hiromisçš„notes (github.com)](https://github.com/hiromis/notes/tree/master)|
|Practical Deep Learning for Coders 2018|2018|2018ç‰ˆï¼Œå·²ç»è¢«æ›¿ä»£|
|[fast.ai](https://docs.fast.ai/)|-|fastaiçš„documentation|
|[Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD]([Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD: Howard, Jeremy, Gugger, Sylvain: 9781492045526: Amazon.com: Books](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527))|-|ä¸€æœ¬ä¹¦ï¼Œ[å…è´¹]([Practical Deep Learning for Coders - The book](https://course.fast.ai/Resources/book.html))|
|[new fast.ai course: A Code-First Introduction to Natural Language Processing â€“ fast.ai](https://www.fast.ai/posts/2019-07-08-fastai-nlp.html)|-|å…¶ä»–ç±»å‹è¯¾ç¨‹|

## [Practical Deep Learning 2022](https://course.fast.ai/) & [Fastbook](https://github.com/fastai/fastbook)
### 1: Getting started (PDL2022)

#### *Is it a bird? Creating a model from your own data*

*   æµç¨‹

  1. ä¸‹è½½åº“ï¼Œå‡†å¤‡å¥½
  2. ç”Ÿæˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
  3. è®­ç»ƒ
  4. éªŒè¯
* ä¸€äº›åˆå­¦

  ```python
  from fastcore.all import * 
  'å°½ç®¡fastaiå¯ä»¥è‡ªåŠ¨å¤„ç†å¯¹fastcoreçš„ä¾èµ–ï¼Œä½¿fastaiå¯ä»¥ä½¿ç”¨fastcoreçš„éƒ¨åˆ†åŠŸèƒ½ï¼Œä½†æ˜¯ä¸ºäº†ä»£ç æ›´ç®€æ´ï¼Œä¾ç„¶æ˜¾å¼åœ°å¯¼å…¥äº†fastcore'
  
  L #è½¬æ¢æˆfastcoreä¸­çš„å”¯ä¸€ä¸€ä¸ªç±»L
  dest = (path/o) #path/o
  failed = verify_images(get_image_files(path)) #failedæ˜¯Lç±»
  ```
  ```python
  from fastai.vision.all import *
  
  Image.open('path').to_thumb(200,200) #Image.open().to_thumb()ç­‰æ¯”ä¾‹æ”¾å¤§
  download_images(dest, urls=search_images(f'{o} photo')) #download_images
  resize_images(path/o, max_size=200, dest=path/o) #resize_images
  get_image_files(path) #get_image_files
  verify_images(get_image_files(path)) #verify_images
  failed.map(Path.unlink) #failed.map
  dls = DataBlock( #DataBlock
    blocks = (ImageBlock, CategoryBlock), #ImageBlock, CategoryBlock
    get_items = get_image_files, #get_image_files
    splitter = RandomSplitter(valid_pct = 0.2, seed = 42), #RandomSplitter
    get_y = parent_label, #parent_label
    item_tfms = [Resize(200, method = 'squish')] #Resize
  ).dataloaders(path, bs = 32) #dataloaders
  dls.show_batch(max_n=6) #show_batch
  learn = vision_learner(dls, resnet18, metrics = error_rate) #vision_learner
  learn.fine_tune(5) #fine_tune
  cat_or_dog,_,pros_cat = learn.predict(PILImage.create('dog.jpg')) #predict; PILImage.createä¹Ÿå¯å±äºfastaiï¼Œåªæ˜¯å®ƒæœ‰è‡ªå·±çš„åº•å±‚ä¾èµ–åº“
  ```

  ```python
  from duckduckgo_search import DDGS 'å¯¼å…¥åº“'
  from fastdownload import download_url 'ç”¨äºä¸‹è½½åº“'
  import time 'é‰´äºæ•ˆç‡éå¸¸é‡è¦ï¼Œå› æ­¤è¦è®°å½•time'
  ```

### 1: intro (fastbook)

#### 1.1 åˆå­¦

```python
#å›¾åƒè¯†åˆ«
path = untar_data(URLs.PETS)/'images' 
#untar_data()ä»fastaiå†…ç½®çš„æ•°æ®åº“ä¸­ä¸‹è½½è§£å‹æ•°æ®å¹¶è¿›å…¥'images'çš„æ–‡ä»¶å¤¹ä¸­
#untar_data(URLs.PETS)è¿”å›äº†ä¸€ä¸ªPathå¯¹è±¡ï¼Œæ˜¯fastaiå†…ç½®åº“PETSçš„è·¯å¾„

def is_cat(x): return x[0].isupper() #è¿™ä¸ªæ•°æ®é›†ä¸­ï¼ŒCatæ˜¯å¤§å†™ï¼Œdogæ˜¯å°å†™ï¼Œä»¥æ­¤æ¥åŒºåˆ†çŒ«å’Œç‹—
dls = ImageDataLoaders.from_name_func( 
    path, 
    get_image_files(path),  #è·å¾—æ‰€æœ‰å›¾åƒæ–‡ä»¶
    valid_pct=0.2, seed=42,
    label_func=is_cat,  #è¿”å›Trueæˆ–False
    item_tfms=Resize(224) #ç¼©æ”¾åˆ°224*224åƒç´ 
)
#ImageDataLoaders.from_name_funcæ˜¯fastaiçš„é«˜çº§å°è£…ï¼Œåˆ›å»ºæ•°æ®åŠ è½½å™¨æ›´åŠ ç®€æ´ï¼Œä¸“é—¨ç”¨äºä»æ–‡ä»¶åæå–æ ‡ç­¾çš„ä»»åŠ¡
#å¦‚æœä½¿ç”¨DataBlockğŸ‘‡
dls = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=is_cat,
    item_tfms=Resize(224)
).dataloaders(path)
#`item_tfms`åº”ç”¨äºæ¯ä¸ªé¡¹ç›®ï¼ˆåœ¨æœ¬ä¾‹ä¸­ï¼Œæ¯ä¸ªé¡¹ç›®éƒ½è¢«è°ƒæ•´ä¸º 224 åƒç´ çš„æ­£æ–¹å½¢ï¼‰ï¼Œè€Œ`batch_tfms`åº”ç”¨äºä¸€æ¬¡å¤„ç†ä¸€æ‰¹é¡¹ç›®çš„ GPU

learn = vision_learner(dls, resnet34, metrics=error_rate) #error_rate & accuracy
learn.fine_tune(1)
```

```python
from types import SimpleNamespace
uploader = SimpleNamespace(data = ['images/chapter1_cat_example.jpg'])
#ç›¸å½“äºuploader = {'data':['images/chapter1_cat_example.jpg']}
```

```python
#Image Classification
#segmentation
path = untar_data(URLs.CAMVID_TINY) #å†…ç½®åº“
dls = SegmentationDataLoaders.from_label_func(
    path, bs=8, fnames = get_image_files(path/"images"),
    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',
    codes = np.loadtxt(path/'codes.txt', dtype=str) #è¿™ä¸ªæ˜¯å‚¨å­˜äº†åˆ†å‰²ä»»åŠ¡ä¸­çš„ç±»åˆ«çš„æ–‡ä»¶ï¼Œæ¯”å¦‚é“è·¯ã€å»ºç­‘ã€æ±½è½¦ç­‰ï¼Œoå°±æ˜¯fnamesä¸­çš„ä¸€ä¸ªå…ƒç´ ï¼Œo.stemæ˜¯æ–‡ä»¶åï¼Œo.suffixæ˜¯æ‰©å±•å
)
#label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}' #è‹¥oæ˜¯images/cat.jpgï¼Œåˆ™è¿”å›labels/cat_P.jpg

learn = unet_learner(dls, resnet34)
learn.fine_tune(8)

learn.show_results(max_n=6, figsize=(7,8))
```

```python
#NLP
from fastai.text.all import *

dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')
learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)
learn.fine_tune(4, 1e-2)

learn.predict("I really liked that movie!")
#ç»“æœï¼š('pos', tensor(1), tensor([0.0041, 0.9959]))
```

```python
#Tabular
from fastai.tabular.all import *
path = untar_data(URLs.ADULT_SAMPLE)

dls = TabularDataLoaders.from_csv(path/'adult.csv', #è¿™æ˜¯æŒ‡å®šæ•°æ®æ–‡ä»¶
                                  path=path, #è¿™æ˜¯æŒ‡å®šæ•°æ®æ–‡ä»¶æ‰€åœ¨çš„è·¯å¾„
                                  y_names="salary",
    cat_names = ['workclass', 'education', 'marital-status', 'occupation',
                 'relationship', 'race'],
    cont_names = ['age', 'fnlwgt', 'education-num'],
    procs = [Categorify, FillMissing, Normalize]) #è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†çš„æ­¥éª¤ã€‚fastai ä¼šè‡ªåŠ¨åº”ç”¨è¿™äº›é¢„å¤„ç†æ­¥éª¤ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œé¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬ï¼šCategorify: å°†åˆ†ç±»å˜é‡è½¬æ¢ä¸ºç±»åˆ«ç±»å‹ï¼ˆé€šå¸¸æ˜¯æ•´æ•°ç¼–ç ï¼‰ï¼›FillMissing: å¡«å……ç¼ºå¤±å€¼ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼›Normalize: å¯¹è¿ç»­å˜é‡è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼ˆé€šå¸¸æ˜¯å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®ï¼‰ã€‚

learn = tabular_learner(dls, metrics=accuracy) #salaryæ˜¯åˆ†ç±»æ˜¯å¦ä¸ºé«˜æ”¶å…¥è€…ï¼Œæ‰€ä»¥metricsä»ç”¨accuracyæˆ–error_rateå¦‚æœæ˜¯è¿ç»­å˜é‡ï¼Œåˆ™ä¸èƒ½ä½¿ç”¨è¿™ä¸ªmetrics
learn.fit_one_cycle(3) #æ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‰€ä»¥ä¸ç”¨fine_tune
```

```python
#Collaborative Filteringï¼Œåè°ƒè¿‡æ»¤çš„ç»“æ„ä¸€èˆ¬æ¯”è¾ƒç®€å•
#æ ¹æ®ç”¨æˆ·ä»¥å‰çš„è§‚å½±ä¹ æƒ¯ï¼Œé¢„æµ‹ç”¨æˆ·å¯èƒ½å–œæ¬¢çš„ç”µå½±
from fastai.collab import *
path = untar_data(URLs.ML_SAMPLE)
dls = CollabDataLoaders.from_csv(path/'ratings.csv') #è¿™æ˜¯ç”¨äº†é»˜è®¤è®¾ç½®ï¼Œcsvä¸­æœ€åä¸€åˆ—æ˜¯å› å˜é‡ï¼›ååŒè¿‡æ»¤çš„ç‰¹å¾é€šå¸¸ä¹Ÿéƒ½æ˜¯ç¦»æ•£çš„åˆ†ç±»å‚æ•°ï¼Œå¦‚æœæœ‰è¿ç»­å‚æ•°ï¼Œå°±ä¸èƒ½ç”¨CollabDataLoadersç±»ç®€å•é¢„æµ‹
learn = collab_learner(dls, y_range=(0.5,5.5))
learn.fine_tune(10) #è¿™é‡Œä½¿ç”¨äº†fine_tuneè€Œä¸æ˜¯fit_one_cycle
learn.show_results()
```

#### 1.2 **æ€æ ·å¿«é€Ÿè·å–fastaiä¸­æ–¹æ³•çš„è§£é‡Š-doc**

```python
doc(learn.predict)
'''è¿”å›learn.predictæ–¹æ³•çš„è§£é‡Š'''
```

#### 1.3 è¿‡æ‹Ÿåˆ

å³ä½¿æ‚¨çš„æ¨¡å‹å°šæœªå®Œå…¨è®°ä½æ‰€æœ‰æ•°æ®ï¼Œåœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µå¯èƒ½å·²ç»è®°ä½äº†å…¶ä¸­çš„æŸäº›éƒ¨åˆ†ã€‚å› æ­¤ï¼Œæ‚¨è®­ç»ƒçš„æ—¶é—´è¶Šé•¿ï¼Œæ‚¨åœ¨è®­ç»ƒé›†ä¸Šçš„å‡†ç¡®æ€§å°±ä¼šè¶Šå¥½ï¼›éªŒè¯é›†çš„å‡†ç¡®æ€§ä¹Ÿä¼šåœ¨ä¸€æ®µæ—¶é—´å†…æé«˜ï¼Œä½†æœ€ç»ˆä¼šå¼€å§‹å˜å·®ï¼Œå› ä¸ºæ¨¡å‹å¼€å§‹è®°ä½è®­ç»ƒé›†è€Œä¸æ˜¯åœ¨æ•°æ®ä¸­æ‰¾åˆ°å¯æ³›åŒ–çš„æ½œåœ¨æ¨¡å¼ã€‚å½“è¿™ç§æƒ…å†µå‘ç”Ÿæ—¶ï¼Œæˆ‘ä»¬è¯´æ¨¡å‹*è¿‡æ‹Ÿåˆ*ã€‚

æˆ‘ä»¬æœ‰å¾ˆå¤šé¿å…è¿‡æ‹Ÿåˆçš„åŠæ³•ï¼Œä½†åªæœ‰çœŸçš„å‡ºç°è¿‡æ‹Ÿåˆäº†æ‰ä¼šç”¨è¿™äº›åŠæ³•ã€‚æˆ‘ä»¬ç»å¸¸çœ‹åˆ°ä¸€äº›äººè®­ç»ƒæ¨¡å‹ï¼Œä»–ä»¬æœ‰å……è¶³çš„æ•°æ®ï¼Œä½†æ˜¯è¿‡æ—©åœ°ä½¿ç”¨äº†é¿å…è¿‡æ‹Ÿåˆçš„åŠæ³•ï¼Œç»“æœå¯¼è‡´æ¨¡å‹çš„å‡†ç¡®æ€§ä¸å¥½ï¼Œè¿˜ä¸å¦‚è¿‡æ‹Ÿåˆäº†çš„æ¨¡å‹å‡†ç¡®æ€§é«˜ã€‚

#### 1.4 æ¶æ„

* CNNï¼šåˆ›å»ºè®¡ç®—æœºè§†è§‰æ¨¡å‹çš„å½“å‰æœ€å…ˆè¿›æ–¹æ³•

  ResNetï¼Œä¸€ç§æ ‡å‡†æ¶æ„ï¼Œæœ‰18ã€34ã€50ã€101å’Œ152

#### 1.5 é¢„è®­ç»ƒ/è¿ç§»å­¦ä¹ 

ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ˜¯æˆ‘ä»¬è®­ç»ƒæ›´å‡†ç¡®ã€æ›´å¿«é€Ÿã€ä½¿ç”¨æ›´å°‘æ•°æ®å’Œæ›´å°‘æ—¶é—´å’Œé‡‘é’±çš„æœ€é‡è¦æ–¹æ³•ã€‚æ‚¨å¯èƒ½ä¼šè®¤ä¸ºä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å°†æ˜¯å­¦æœ¯æ·±åº¦å­¦ä¹ ä¸­æœ€ç ”ç©¶çš„é¢†åŸŸ...ä½†æ‚¨ä¼šéå¸¸ã€éå¸¸é”™è¯¯ï¼é¢„è®­ç»ƒæ¨¡å‹çš„é‡è¦æ€§é€šå¸¸åœ¨å¤§å¤šæ•°è¯¾ç¨‹ã€ä¹¦ç±æˆ–è½¯ä»¶åº“åŠŸèƒ½ä¸­**æ²¡æœ‰**å¾—åˆ°è®¤å¯æˆ–è®¨è®ºï¼Œå¹¶ä¸”åœ¨å­¦æœ¯è®ºæ–‡ä¸­å¾ˆå°‘è¢«è€ƒè™‘ã€‚å½“æˆ‘ä»¬åœ¨ 2020 å¹´åˆå†™è¿™ç¯‡æ–‡ç« æ—¶ï¼Œäº‹æƒ…åˆšåˆšå¼€å§‹æ”¹å˜ï¼Œä½†è¿™å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ã€‚å› æ­¤è¦å°å¿ƒï¼šæ‚¨ä¸ä¹‹äº¤è°ˆçš„å¤§å¤šæ•°äººå¯èƒ½ä¼šä¸¥é‡ä½ä¼°æ‚¨å¯ä»¥åœ¨æ·±åº¦å­¦ä¹ ä¸­ä½¿ç”¨å°‘é‡èµ„æºåšäº›ä»€ä¹ˆï¼Œå› ä¸ºä»–ä»¬å¯èƒ½ä¸ä¼šæ·±å…¥äº†è§£å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚

ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ¥æ‰§è¡Œä¸€ä¸ªä¸å…¶æœ€åˆè®­ç»ƒç›®çš„ä¸åŒçš„ä»»åŠ¡è¢«ç§°ä¸º***è¿ç§»å­¦ä¹ ***ã€‚ä¸å¹¸çš„æ˜¯ï¼Œ**ç”±äºè¿ç§»å­¦ä¹ ç ”ç©¶ä¸è¶³ï¼Œå¾ˆå°‘æœ‰é¢†åŸŸæä¾›é¢„è®­ç»ƒæ¨¡å‹**ã€‚ä¾‹å¦‚ï¼Œç›®å‰åœ¨åŒ»å­¦é¢†åŸŸå¾ˆå°‘æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨ï¼Œè¿™ä½¿å¾—åœ¨è¯¥é¢†åŸŸä½¿ç”¨è¿ç§»å­¦ä¹ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œç›®å‰è¿˜ä¸æ¸…æ¥šå¦‚ä½•å°†è¿ç§»å­¦ä¹ åº”ç”¨äºè¯¸å¦‚æ—¶é—´åºåˆ—åˆ†æä¹‹ç±»çš„ä»»åŠ¡ã€‚

#### 1.6 head

When using a pretrained model, `vision_learner` will remove the last layer, since that is always specifically customized to the original training task (i.e. ImageNet dataset classification), and replace it with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the *head*.

#### 1.7 è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†

åœ¨ç”¨è®­ç»ƒé›†è®­ç»ƒåï¼Œæˆ‘ä»¬ç”¨éªŒè¯é›†æŸ¥çœ‹è®­ç»ƒæ•ˆæœï¼Œæ ¹æ®éªŒè¯é›†çš„æ•ˆæœï¼Œè°ƒæ•´è¶…å‚æ•°hyperparameterï¼Œå› æ­¤ï¼ŒéªŒè¯é›†ä»ç„¶åŠæš´éœ²åœ¨è®­ç»ƒæ¨¡å‹ä¸­ã€‚ä¸ºäº†èƒ½æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ•ˆæœï¼Œç”¨éªŒè¯é›†æ˜¾ç„¶æ˜¯ä¸ç†æƒ³çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜ä¼šéš”ç»å‡ºä¸€ä¸ªå®Œå…¨æ²¡æœ‰ç”¨è¿‡çš„æµ‹è¯•é›†ã€‚

å½“ç„¶å¦‚æœæ•°æ®é‡ä¸å¤Ÿï¼Œæµ‹è¯•é›†å¹¶ä¸ä¸€å®šæ˜¯å¿…é¡»çš„ï¼Œä½†æ˜¯å½“ç„¶æœ‰æ˜¯æœ€å¥½çš„ã€‚

åœ¨æˆ‘ä»¬å®é™…çš„è®­ç»ƒä¸­ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†çš„é€‰æ‹©å¾ˆæœ‰è®²ç©¶ã€‚æ¯”å¦‚æˆ‘ä»¬é¢„æµ‹æ—¶é—´åºåˆ—ï¼Œæœ€å¥½çš„åˆ’åˆ†æ˜¯æŠŠæœ€è¿‘çš„ä¸€æ®µæ—¶é—´ä½œä¸ºéªŒè¯é›†/æµ‹è¯•é›†ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥è¯„ä¼°æ¨¡å‹å¯¹æœªæ¥çš„é¢„æµ‹æ•ˆæœï¼›æ¯”å¦‚æˆ‘ä»¬è¯†åˆ«é©¾é©¶å‘˜çš„è¡Œä¸ºï¼Œæœ€å¥½çš„åˆ’åˆ†æ˜¯æŠŠä¸€ä¸ªé©¾é©¶å‘˜å®Œå…¨éš”ç»æˆéªŒè¯é›†/æµ‹è¯•é›†ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥è¯„ä¼°æ¨¡å‹å¯¹ä¸åŒçš„äººæ˜¯ä¸æ˜¯éƒ½æœ‰å¾ˆå¥½çš„è¯†åˆ«æ•ˆæœã€‚

#### 1.8 å…¶ä»–

* æ—¶é—´åºåˆ—è½¬æ¢æˆå›¾åƒ

  æ—¶é—´åºåˆ—æ•°æ®æœ‰å„ç§è½¬æ¢æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œfast.ai å­¦ç”Ÿ Ignacio Oguiza ä½¿ç”¨ä¸€ç§ç§°ä¸º**Gramian Angular Difference Fieldï¼ˆGADFï¼‰**çš„æŠ€æœ¯ï¼Œä»ä¸€ä¸ªæ—¶é—´åºåˆ—æ•°æ®é›†ä¸­ä¸ºæ©„æ¦„æ²¹åˆ†ç±»åˆ›å»ºå›¾åƒï¼Œä½ å¯ä»¥åœ¨å›¾ 1-15 ä¸­çœ‹åˆ°ç»“æœã€‚ç„¶åï¼Œä»–å°†è¿™äº›å›¾åƒè¾“å…¥åˆ°ä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡å‹ä¸­ï¼Œå°±åƒä½ åœ¨æœ¬ç« ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚å°½ç®¡åªæœ‰ 30 ä¸ªè®­ç»ƒé›†å›¾åƒï¼Œä½†ä»–çš„ç»“æœå‡†ç¡®ç‡è¶…è¿‡ 90%ï¼Œæ¥è¿‘æœ€å…ˆè¿›æ°´å¹³ã€‚

  ![](img/dlcf_0115.png)

  å¦ä¸€ä¸ªæœ‰è¶£çš„ fast.ai å­¦ç”Ÿé¡¹ç›®ç¤ºä¾‹æ¥è‡ª Gleb Esmanã€‚ä»–åœ¨ Splunk ä¸Šè¿›è¡Œæ¬ºè¯ˆæ£€æµ‹ï¼Œä½¿ç”¨äº†ç”¨æˆ·é¼ æ ‡ç§»åŠ¨å’Œé¼ æ ‡ç‚¹å‡»çš„æ•°æ®é›†ã€‚ä»–é€šè¿‡ç»˜åˆ¶æ˜¾ç¤ºé¼ æ ‡æŒ‡é’ˆä½ç½®ã€é€Ÿåº¦å’ŒåŠ é€Ÿåº¦çš„å›¾åƒï¼Œä½¿ç”¨å½©è‰²çº¿æ¡ï¼Œå¹¶ä½¿ç”¨[å°å½©è‰²åœ†åœˆ](https://oreil.ly/6-I_X)æ˜¾ç¤ºç‚¹å‡»ï¼Œå°†è¿™äº›è½¬æ¢ä¸ºå›¾ç‰‡ï¼Œå¦‚å›¾ 1-16 æ‰€ç¤ºã€‚ä»–å°†è¿™äº›è¾“å…¥åˆ°ä¸€ä¸ªå›¾åƒè¯†åˆ«æ¨¡å‹ä¸­ï¼Œå°±åƒæˆ‘ä»¬åœ¨æœ¬ç« ä¸­ä½¿ç”¨çš„é‚£æ ·ï¼Œæ•ˆæœéå¸¸å¥½ï¼Œå¯¼è‡´äº†è¿™ç§æ–¹æ³•åœ¨æ¬ºè¯ˆåˆ†ææ–¹é¢çš„ä¸“åˆ©ï¼

  ![](img/dlcf_0116.png)

  å¦ä¸€ä¸ªä¾‹å­æ¥è‡ª Mahmoud Kalash ç­‰äººçš„è®ºæ–‡â€œä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œæ¶æ„è½¯ä»¶åˆ†ç±»â€ï¼Œè§£é‡Šäº†â€œæ¶æ„è½¯ä»¶äºŒè¿›åˆ¶æ–‡ä»¶è¢«åˆ†æˆ 8 ä½åºåˆ—ï¼Œç„¶åè½¬æ¢ä¸ºç­‰æ•ˆçš„åè¿›åˆ¶å€¼ã€‚è¿™ä¸ªåè¿›åˆ¶å‘é‡è¢«é‡å¡‘ï¼Œç”Ÿæˆäº†ä¸€ä¸ªä»£è¡¨æ¶æ„è½¯ä»¶æ ·æœ¬çš„ç°åº¦å›¾åƒâ€ï¼Œå¦‚å›¾ 1-17 æ‰€ç¤ºã€‚

  ![](img/dlcf_0117.png)

#### 1.9 æœ¯è¯­

  | Term             | Meaning                                                      |
  | ---------------- | ------------------------------------------------------------ |
  | Label            | The data that we're trying to predict, such as "dog" or "cat" |
  | Architecture     | The _template_ of the model that we're trying to fit; the actual mathematical function that we're passing the input data and parameters to |
  | Model            | The combination of the architecture with a particular set of parameters |
  | Parameters       | The values in the model that change what task it can do, and are updated through model training |
  | Fit/æ‹Ÿåˆ         | Update the parameters of the model such that the predictions of the model using the input data match the target labels |
  | Train            | A synonym for _fit_                                          |
  | Pretrained model | A model that has already been trained, generally using a large dataset, and will be fine-tuned |
  | Fine-tune        | Update a pretrained model for a different task               |
  | Epoch            | One complete pass through the input data                     |
  | Loss             | A measure of how good the model is, chosen to drive training via SGD |
  | Metric           | A measurement of how good the model is, using the validation set, chosen for human consumption |
  | Validation set   | A set of data held out from training, used only for measuring how good the model is |
  | Training set     | The data used for fitting the model; does not include any data from the validation set |
  | Overfitting      | Training a model in such a way that it _remembers_ specific features of the input data, rather than generalizing well to data not seen during training |
  | CNN              | Convolutional neural network; a type of neural network that works particularly well for computer vision tasks |

### 2ï¼šDeployment (PDL2022)-æš‚æ—¶è·³è¿‡

### 2: Production (fastbook)-æš‚æ—¶è·³è¿‡

### 3: Neural net foundations (PDL2022)

#### *How does a neural net really work?*

paperspace

### 3: mnist_basics (fastbook4)

#### 3.1 åƒç´ ï¼šè®¡ç®—æœºè§†è§‰çš„åŸºç¡€

```python
from fastcore.all import *
from fastai.vision.all import *

path = untar_data(URLs.MNIST_SAMPLE)
path.ls()  #Lç±»
(path/'train').ls()

threes = (path/'train'/'3').ls().sorted()
sevens = (path/'train'/'7').ls().sorted()
threes

im3 = Image.open(threes[1]) #PILæ‰“å¼€image
im3

im3_t = tensor(im3)[4:12,4:10]
print(im3_t)
len(im3_t)
df = pd.DataFrame(im3_t)
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')

seven_tensors = [tensor(Image.open(o)) for o in sevens]
three_tensors = [tensor(Image.open(o)) for o in threes]
len(three_tensors), len(seven_tensors) #â€˜6265â€™æ–‡ä»¶å¤¹7è½¬æ¢ä¸ºseven_tensorsï¼Œè¿™å¤§æ¦‚å¯ä»¥ç®—ä½œæ˜¯ä¸€ä¸ªå¼ é‡çš„listï¼ˆLç±»ï¼‰
show_image(three_tensors[1]) #ä¸ç”¨PILæ‰“å¼€imageï¼Œç”¨fasiaiä¸­çš„show_imageæ‰“å¼€tensorä»£è¡¨çš„imageï¼Œä¹Ÿæ˜¯ä¸ªå›¾ç‰‡

stacked_sevens = torch.stack(seven_tensors).float()/255 #æŠŠè¿™ä¸ªtensorçš„Lç”¨torch.stackæ–¹æ³•å †å æˆä¸€ä¸ª3-rankå¼ é‡
stacked_threes = torch.stack(three_tensors).float()/255
stacked_threes.shape #torch.Size([6131, 28, 28])
len(stacked_sevens.shape) #å¼ é‡çš„ç§©ï¼Œä¹Ÿå°±æ˜¯å¼ é‡çš„ç»´åº¦

mean3 = stacked_threes.mean(0) #æ²¿ç€ç»´åº¦0æ±‚å¹³å‡å€¼
show_image(mean3)
mean7 = stacked_sevens.mean(0)

a_3 = stacked_threes[1] #float
F.l1_loss(a_3, mean7) #l1æ˜¯æ ‡å‡†æ•°å­¦æœ¯è¯­å¹³å‡ç»å¯¹å€¼çš„ç¼©å†™ï¼ˆåœ¨æ•°å­¦ä¸­ç§°ä¸ºL1 èŒƒæ•°ï¼‰
F.mse_loss(a_3, mean7).sqrt() #mseå‡æ–¹è¯¯å·®ï¼Œsqrt()å¼€æ ¹ï¼ŒRMSEæ˜¯L2èŒƒæ•°
#MSEç›¸æ¯”L1èŒƒæ•°æ¥è¯´ä¼šæ›´ç‹ åœ°æƒ©ç½šå¤§çš„è¯¯å·®ï¼Œè€Œå¯¹å°è¯¯å·®æ›´åŠ å®½å®¹
```

#### 3.2 NumPy æ•°ç»„å’Œ PyTorch å¼ é‡

* [NumPy](https://numpy.org) æ˜¯ Python ä¸­ç”¨äºç§‘å­¦å’Œæ•°å€¼ç¼–ç¨‹æœ€å¹¿æ³›ä½¿ç”¨çš„åº“ã€‚å®ƒæä¾›äº†ç±»ä¼¼çš„åŠŸèƒ½å’Œç±»ä¼¼çš„ APIï¼Œä¸ PyTorch æä¾›çš„åŠŸèƒ½ç›¸ä¼¼ï¼›ç„¶è€Œï¼Œå®ƒä¸æ”¯æŒä½¿ç”¨ GPU æˆ–è®¡ç®—æ¢¯åº¦ï¼Œè¿™ä¸¤è€…å¯¹äºæ·±åº¦å­¦ä¹ éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚

| #    | Numpy                                          | Pytorch                               |
| ---- | ---------------------------------------------- | ------------------------------------- |
| 1    | ä¸è§„åˆ™æ•°ç»„ï¼šå¯ä»¥æ˜¯æ•°ç»„çš„æ•°ç»„ï¼Œå†…éƒ¨æ•°ç»„å¤§å°ä¸åŒ | ä¸å¯ä»¥æ˜¯ä¸è§„åˆ™çš„                      |
| 2    | ä¸èƒ½å­˜åœ¨GPUä¸Š                                  | å¯ä»¥å­˜å‚¨åœ¨GPUä¸Šï¼Œåç»­è®­ç»ƒæ›´å¿«         |
| 3    | ä¸èƒ½è®¡ç®—å¯¼æ•°                                   | å¯ä»¥è‡ªåŠ¨è®¡ç®—å¯¼æ•°ï¼Œå¯ä»¥è¿›è¡ŒSGDæ¢¯åº¦è®¡ç®— |

```py
data = [[1,2,3],[4,5,6]]
arr = array (data)
tns = tensor(data)
tns[1]  #tensor([4, 5, 6])
tns[:, 1] #tensor([2, 5])
tns +1 # tensor([[2, 3, 4],[5, 6, 7]])
```

#### 3.3 ä½¿ç”¨Broadcastingè®¡ç®—Metrics

* å¯ä»¥ä½¿ç”¨MSEæˆ–L1èŒƒæ•°ä½œä¸ºmetrcsï¼Œä½†æ˜¯æœ‰æ—¶å€™ä¸å¤ªå¥½ç†è§£ï¼Œæ‰€ä»¥ä¸€èˆ¬æƒ…å†µä¸‹ä½¿ç”¨**accuracy**ä½œä¸ºmetrics

```python
valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]).float()/255
valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]).float()/255
valid_3_tens.shape, valid_7_tens.shape #validä¸­çš„å›¾ç‰‡è½¬æ¢æˆä¸€ä¸ªrank-3çš„å½’ä¸€åŒ–tensor

def mnist_distance(a,b):    return (a-b).abs().mean((-1,-2)) #å®šä¹‰ä¸€ä¸ªæ–¹æ³•è®¡ç®—L1èŒƒæ•°ï¼Œ-1/-2æ˜¯å‘Šè¯‰tensorè¦å¯¹æœ€åä¸¤ä¸ªè½´è¿›è¡Œå¹³å‡
mnist_distance(a_3, mean3)
valid_3_dist = mnist_distance(valid_3_tens, mean3) #è¿™é‡Œè‡ªåŠ¨ä½¿ç”¨äº†broadcastingå°†valid_3_tenså®½å±•äº†ä¸€ä¸ªç§©-1
valid_3_dist.shape, valid_3_dist #(torch.Size([1010])

def is_3(x): return mnist_distance(x, mean3) < mnist_distance(x, mean7) #ä¸æ˜¯3å°±æ˜¯7

accuracy_3s = is_3(valid_3_tens).float().mean()
```

#### 3.4 SGDéšæœºæ¢¯åº¦ä¸‹é™

![](img/dlcf_0401.png)

æ³¨æ„ç‰¹æ®Šæ–¹æ³•`requires_grad_`ï¼Ÿè¿™æ˜¯æˆ‘ä»¬å‘Šè¯‰ PyTorch æˆ‘ä»¬æƒ³è¦è®¡ç®—æ¢¯åº¦çš„ç¥å¥‡å’’è¯­ã€‚è¿™å®è´¨ä¸Šæ˜¯ç»™å˜é‡æ‰“ä¸Šæ ‡è®°ï¼Œè¿™æ · PyTorch å°±ä¼šè®°ä½å¦‚ä½•è®¡ç®—æ‚¨è¦æ±‚çš„å…¶ä»–ç›´æ¥è®¡ç®—çš„æ¢¯åº¦ã€‚

```python
xt = tensor(3.).requires_grad_() #è®©pytorchçŸ¥é“æˆ‘ä»¬åé¢ä¼šè¦æ±‚è®¡ç®—è¿™ä¸ªtensorçš„æ¢¯åº¦
yt = f(xt)
yt.backward() #backward,å…¶å®å°±æ˜¯calculates_grad
xt.grad #è®¡ç®—æ¢¯åº¦ï¼Œtensor(6.)
```

#### 3.5 å­¦ä¹ ç‡

```python
w -= w.grad * lr #lrå­¦ä¹ ç‡
```

#### 3.6 å®ä¾‹

```python
time = torch.arange(0,20).float()
speed = torch.randn(20)*3 + 0.75*(time-9.5)**2+1 #åœ¨yä¸­æ·»åŠ äº†å™ªå£°
plt.scatter(time,speed)

#å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°ï¼Œç”¨å®ƒæ¥æ‹Ÿåˆ(time,speed)
def f(t,params): 
    a,b,c = params
    return a*(t**2)+b*t+c

#è®¡ç®—loss
def mse(preds, targets): return ((preds-targets)**2).mean()

#åˆå§‹åŒ–éšæœºparameters
params = torch.randn(3).requires_grad_()

#å®šä¹‰å­¦ä¹ ç‡
lr = 1e-5

#å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ‹Ÿåˆ
def apply_step(params, prn=True):
    preds = f(time,params)
    loss = mse(preds, speed)
    loss.backward()
    params.data -= lr*params.grad.data #è¿™ä¸ªå¿…é¡»å¾—åŠ .dataæ–¹æ³•ï¼Œå¦åˆ™ä¼šæŠ¥é”™
    params.grad = None
    if prn: print(loss.item()) #loss.item()ä¸å†æ˜¯tensor
    return params

#å¾ªç¯
while loss<3:
    apply_step(params)
```

#### 3.7 MNIST codes

* sigmoidï¼šæˆ‘ä»¬é¢„æµ‹è¿™ä¸ªpredctionsæ€»æ˜¯åœ¨0~1ï¼Œä½†å®é™…ä¸Šå®ƒå¯èƒ½åœ¨è¿™ä¸ªèŒƒå›´ä¹‹å¤–ï¼Œå°±éœ€è¦é‡‡ç”¨ä¸€ç§æ–¹æ³•æŠŠå®ƒæ”¾è¿›æ¥

```python
import matplotlib.pyplot as plt
def plot_function(f, title=None, min=-1, max=1):
    x = torch.arange(min, max, 0.1)
    y = tensor([f(xi) for xi in x])
    plt.plot(x, y)
    if title:
        plt.title(title)
    plt.show()
plot_function(torch.sigmoid, title='sigmoid',min=-4, max=4)
```

* å°æ‰¹æ¬¡ï¼šä¸ºæ•´ä¸ªæ•°æ®é›†è®¡ç®—å°†éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚ä¸ºå•ä¸ªæ•°æ®é¡¹è®¡ç®—å°†ä¸ä¼šä½¿ç”¨å¤ªå¤šä¿¡æ¯ï¼Œå› æ­¤ä¼šå¯¼è‡´ä¸ç²¾ç¡®å’Œä¸ç¨³å®šçš„æ¢¯åº¦ã€‚æ‚¨å°†è´¹åŠ›æ›´æ–°æƒé‡ï¼Œä½†åªè€ƒè™‘è¿™å°†å¦‚ä½•æ”¹å–„æ¨¡å‹åœ¨è¯¥å•ä¸ªæ•°æ®é¡¹ä¸Šçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åšå‡ºå¦¥åï¼šæˆ‘ä»¬ä¸€æ¬¡è®¡ç®—å‡ ä¸ªæ•°æ®é¡¹çš„å¹³å‡æŸå¤±ã€‚è¿™è¢«ç§°ä¸º*å°æ‰¹æ¬¡* **mini-batch**ã€‚é€‰æ‹©ä¸€ä¸ªå¥½çš„æ‰¹æ¬¡å¤§å°æ˜¯æ‚¨ä½œä¸ºæ·±åº¦å­¦ä¹ ä»ä¸šè€…éœ€è¦åšå‡ºçš„å†³å®šä¹‹ä¸€ï¼Œä»¥ä¾¿å¿«é€Ÿå‡†ç¡®åœ°è®­ç»ƒæ‚¨çš„æ¨¡å‹ã€‚
* æ¯”å¦‚æœ‰2000ç»„æ•°æ®åˆ†ä¸º4*500çš„mini-batchï¼Œåœ¨ä¸€ä¸ªepochä¸­å°±ä¼š4æ¬¡æ›´æ–°parameters

```python
coll = range(15)
dl = DataLoader(coll, batch_size=5, shuffle=True)
list(dl)
'''[tensor([11,  4,  3,  0,  2]),
 tensor([ 8,  7, 13, 14,  5]),
 tensor([12,  9,  6, 10,  1])]'''
ds = L(enumerate(string.ascii_lowercase))
dl = DataLoader(ds, batch_size=6, shuffle=True)
list(dl)
'''[(tensor([20,  7, 12, 24, 22,  9]), ('u', 'h', 'm', 'y', 'w', 'j')),
 (tensor([ 1, 25, 17, 19,  3,  6]), ('b', 'z', 'r', 't', 'd', 'g')),
 (tensor([13,  0,  5, 10, 18,  2]), ('n', 'a', 'f', 'k', 's', 'c')),
 (tensor([ 8,  4, 21, 15, 23, 16]), ('i', 'e', 'v', 'p', 'x', 'q')),
 (tensor([14, 11]), ('o', 'l'))]'''
```

##### nn.Linearï¼šåšçš„äº‹æƒ…å¦‚ä¸‹

```python
#åˆå§‹åŒ–å‚æ•°
def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()
weights = init_params((28*28,1)) # torch.Size([784,1])
bias = init_params(1) # torch.Size(1)
#è®¡ç®—predsçš„å‡½æ•°
def linear1(xb): return xb@weights+bias

#å¯ä»¥ç®€åŒ–ä¸º
linear_model = nn.Linear(28*28,1)
w,b=linear_model.parameters()
'''ä¸Šé¢çš„linear1å°±å˜æˆäº†linear_model'''
```

##### SGDï¼šä¸‹é¢ä¸€å…±ä¸‰ç‰ˆï¼Œæ…¢æ…¢ç®€åŒ–åç”¨åˆ°SGD()ç±»

```python
'ç¬¬ä¸€ç‰ˆï¼šå…¨éƒ¨è‡ªå·±å®šä¹‰å‡½æ•°'
#å®šä¹‰è®¡ç®—æ¢¯åº¦çš„å‡½æ•°
def calc_grad(xb,yb,model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è®¡ç®—éªŒè¯é›†çš„å‡†ç¡®ç‡
def valid_epoch(model):
    accs = [batch_accuracy(model(xb),yb) for i in valid_dl]
    return round(torch.stack(accs).mean().item(),4)
#æ›´æ–°æ¢¯åº¦çš„å‡½æ•°-------------------------ä¸»è¦æ˜¯è¿™ä¸€æ­¥å˜æˆäº†ç¬¬äºŒç‰ˆçš„ç±»+æ›´æ–°æ¢¯åº¦çš„å‡½æ•°
def train_epoch(model, lr, params):
    for xb,yb in dl:
        calc_grad(xb,yb,model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()
#è·‘èµ·æ¥
lr = 1.
params = weights,bias
for i in range(20):
    train_epoch(linear1,lr,params)
    print(valid_epoch(linear1))
------------------------------------------------------------------------------------------------------------
'ç¬¬äºŒç‰ˆï¼šåˆ›å»ºäº†ä¸€ä¸ªç±»ï¼ŒæŠŠä¸€äº›å‡½æ•°æ•´åˆè¿›ç±»é‡Œé¢'
#å®šä¹‰è®¡ç®—æ¢¯åº¦çš„å‡½æ•°
def calc_grad(xb,yb,model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è®¡ç®—éªŒè¯é›†çš„å‡†ç¡®ç‡
def valid_epoch(model):
    accs = [batch_accuracy(model(xb),yb) for i in valid_dl]
    return round(torch.stack(accs).mean().item(),4)
#åˆ›å»ºä¸€ä¸ªä¼˜åŒ–å™¨çš„ç±»ï¼ŒæŠŠæ›´æ–°æ¢¯åº¦å’Œå½’é›¶æ¢¯åº¦ä½œä¸ºæ–¹æ³•æ”¾è¿›äº†ç±»ä¸­--------------------------è¿™ä¸€æ­¥å˜æˆäº†ç¬¬ä¸‰ç‰ˆçš„SGD()
class BasicOptim:
    def __init__(self,params,lr):
        self.params = list(params)
        self.lr = lr
    def step(self, *args, **kwargs):
        for p in self.params:
            p.data -= p.grad.data*self.lr
    def zero_grad(self, *args, **kwargs):
        for p in self.params:
            p.grad = None
lr=1.
opt = BasicOptim(linear_model.parameters(),lr)
#æ›´æ–°æ¢¯åº¦çš„å‡½æ•°-----------------------------
def train_epoch(model):
    for xb,yb in dl:
        cal_grad(xb,yb,model)
        opt.step()
        opt.zero_grad()
#è·‘èµ·æ¥
def train_model(model,epochs):
    for i in range(epochs):
        train_epoch(model)
        print(valid_epoch(model))
train_model(linear_model,20)
------------------------------------------------------------------------------------------------------------
'ç¬¬ä¸‰ç‰ˆï¼šç”¨äº†ç°æˆçš„SGDç±»'
#å®šä¹‰è®¡ç®—æ¢¯åº¦çš„å‡½æ•°
def calc_grad(xb,yb,model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è®¡ç®—éªŒè¯é›†çš„å‡†ç¡®ç‡
def valid_epoch(model):
    accs = [batch_accuracy(model(xb),yb) for i in valid_dl]
    return round(torch.stack(accs).mean().item(),4)
#å®ä¾‹åŒ–ä¸€ä¸ªç±»-------------------------------------------------------
lr=1.
opt=SGD(linear_model.parameters(),lr)
#æ›´æ–°æ¢¯åº¦çš„å‡½æ•°-----------------------------------------------------
def train_epoch(model):
    for xb,yb in dl:
        cal_grad(xb,yb,model)
        opt.step()
        opt.zero_grad()
#è·‘èµ·æ¥
def train_model(model,epochs):
    for i in range(epochs):
        train_epoch(model)
        print(valid_epoch(model))
train_model(linear_model,20)
```

##### Learn.fitï¼š

```python
'ç¬¬ä¸€ç‰ˆ'
#åˆå§‹åŒ–å‚æ•°&å®ä¾‹åŒ–è®¡ç®—predsçš„å‡½æ•°
linear_model = nn.Linear(28*28,1)
'è§ä¸Šnn.Linear'
#å®šä¹‰è®¡ç®—lossçš„å‡½æ•°
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
#ä¼˜åŒ–æ­¥éª¤optimization step
#å®šä¹‰è®¡ç®—æ¢¯åº¦çš„å‡½æ•°
def calc_grad(xb,yb,model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()
#å®ä¾‹åŒ–ç±»+æ›´æ–°æ¢¯åº¦çš„å‡½æ•°-----------------------------------------------------
lr=1.
opt=SGD(linear_model.parameters(),lr)
#æ›´æ–°æ¢¯åº¦çš„å‡½æ•°
def train_epoch(model):
    for xb,yb in dl:
        cal_grad(xb,yb,model)
        opt.step()
        opt.zero_grad()
'è§ä¸ŠSGD'
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è®¡ç®—éªŒè¯é›†çš„å‡†ç¡®ç‡
def valid_epoch(model):
    accs = [batch_accuracy(model(xb),yb) for i in valid_dl]
    return round(torch.stack(accs).mean().item(),4)
#è·‘èµ·æ¥----------------------------------------------------------------------
def train_model(model,epochs):
    for i in range(epochs):
        train_epoch(model)
        print(valid_epoch(model))
train_model(linear_model,10)

------------------------------------------------------------------------------------------------------------
'ç®€åŒ–ç‰ˆï¼šä¸ç”¨train_modeläº†'
#åˆå§‹åŒ–å‚æ•°&å®ä¾‹åŒ–è®¡ç®—predsçš„å‡½æ•°
linear_model = nn.Linear(28*28,1)
'è§ä¸Šnn.Linear'
#å®šä¹‰è®¡ç®—lossçš„å‡½æ•°
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è·‘èµ·æ¥
learn = Learner(dls,nn.Linear(28*28,1),opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy)
lr=1.
learn.fit(10,lr=lr)
```

##### å®ä¾‹


```python
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1,28*28) #æŠŠstacked_threeså’Œstacked_sevensç»„åˆæˆ1ä¸ªtensorï¼Œå¹¶ä¸”æ¯å¼ å›¾ç‰‡çš„æ•°æ®28*28é¦–å°¾ç›¸è¿æˆä¸€ä¸ªrank-1çš„tensor
#train_x.shape # torch.Size([12396,784])

train_y = tensor([1]*len(threes)+[0]*len(sevens)).unsqueeze(1) #unsqueeze(1)æ˜¯å°†å®ƒæ‰©å±•ä¸ºrank-2ï¼Œå¦åˆ™sizeæ˜¯([12396])
#train_y.shape # torch.Size([12396,1])

dset = list(zip(train_x, train_y)) #list(zip())å¯ä»¥æŠŠä¸¤ä¸ªtensorç»„åˆæˆ1ä¸ªå…ƒç»„
#x,y = dset[0] #å°±å¯ä»¥è¿™æ ·ç´¢å¼•äº†

valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1,28*28)
valid_y = tensor([1]*len(valid_3_tens)+[0]*len(valid_7_tens)).unsqueeze(1)
valid_dset = list(zip(valid_x,valid_y))

#å‡†å¤‡å¥½dataloader
dl = DataLoader(dset, batch_size=256)
valid_dl = DataLoader(valid_dset, batch_size=256)
dls = DataLoaders(dl,valid_dl)

#è®¡ç®—å‡†ç¡®ç‡
#corrects = (preds>0.0).float() == train_y
#corrects.float().mean().item()
#å¦‚æœç”¨å‡†ç¡®ç‡åšlosså°±ä¼šå¯¼è‡´lossçš„å˜åŒ–ä¸æ˜æ˜¾ï¼ˆåªæœ‰ä»0è·³åˆ°1çš„æ—¶å€™æ‰å˜åŒ–ï¼‰ï¼Œè¿™æ ·ä¼šå¯¼è‡´backwardè®¡ç®—çš„gradå¾ˆå¤šæ—¶å€™ä¸º0ï¼Œæ— æ³•æ‹Ÿåˆ.å®ƒä¸èƒ½æœ‰å¤§çš„å¹³å¦éƒ¨åˆ†å’Œå¤§çš„è·³è·ƒï¼Œè€Œå¿…é¡»æ˜¯ç›¸å½“å¹³æ»‘çš„.

#å®šä¹‰è®¡ç®—lossçš„å‡½æ•°
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#è·‘èµ·æ¥
learn = Learner(dls,nn.Linear(28*28,1),opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy)
lr=1.
learn.fit(10,lr=lr) 
```

##### **æ±‡æ€»codes**

```python
from fastcore.all import *
from fastai.vision.all import *
path = untar_data(URLs.MNIST_SAMPLE)
threes = (path/'train'/'3').ls().sorted()
sevens = (path/'train'/'7').ls().sorted()
seven_tensors = [tensor(Image.open(o)) for o in sevens]
three_tensors = [tensor(Image.open(o)) for o in threes]
stacked_sevens = torch.stack(seven_tensors).float()/255 #æŠŠè¿™ä¸ªtensorçš„Lç”¨torch.stackæ–¹æ³•å †å æˆä¸€ä¸ª3-rankå¼ é‡
stacked_threes = torch.stack(three_tensors).float()/255
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1,28*28) 
train_y = tensor([1]*len(threes)+[0]*len(sevens)).unsqueeze(1)
dset = list(zip(train_x, train_y))
valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]).float()/255
valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]).float()/255
valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1,28*28)
valid_y = tensor([1]*len(valid_3_tens)+[0]*len(valid_7_tens)).unsqueeze(1)
valid_dset = list(zip(valid_x,valid_y))
dl = DataLoader(dset, batch_size=256)
valid_dl = DataLoader(valid_dset, batch_size=256)
dls = DataLoaders(dl,valid_dl)
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
learn = Learner(dls,nn.Linear(28*28,1),opt_func=SGD,loss_func=mnist_loss,metrics=batch_accuracy)
lr=1.
learn.fit(10,lr=lr) 
```

#### 3.8 æ·»åŠ éçº¿æ€§

*rectified linear unit*

##### Activation Function

* F.relu

![](img/dlcf_04in16.png)

* F.sigmoid

##### å®ä¾‹

```python
#å‰é¢å¦‚ä¸Šä¸€ä¸ªå®ä¾‹
#æ„å»ºä¸€ä¸ªå¤šå±‚ç¥ç»ç½‘ç»œ
simple_net = nn.Sequential(
    nn.Linear(28*28,30),
    nn.ReLU(),
    nn.Linear(30,1)
)
#å®šä¹‰è®¡ç®—lossçš„å‡½æ•°
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()
#è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
def batch_accuracy(xb,yb):
    preds = xb.sigmoid()
    correct = (preds>0.5)==yb
    return correct.float().mean()
#å®ä¾‹åŒ–
learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)
#è®­ç»ƒ
learn.fit(30,0.1)
```

<img src="D:\Git\a\Path-Records\img\dlcf_0402.png" style="zoom:50%;" />

è®­ç»ƒè¿‡ç¨‹è®°å½•åœ¨`learn.recorder`ä¸­ï¼Œè¾“å‡ºè¡¨å­˜å‚¨åœ¨`values`å±æ€§ä¸­ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§

```python
plt.plot(L(learn.recorder.values).itemgot(2))
'''train_loss:L(learn.recorder.values).itemgot(0)
valid_loss:L(learn.recorder.values).itemgot(1)
batch_accuracy:L(learn.recorder.values).itemgot(2)'''
```

#### 3.9 æœ¯è¯­

| Term | Meaning|
|:---|---|
|ReLU | Function that returns 0 for negative numbers and doesn't change positive numbers.|
|Mini-batch | A small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch).|
|Forward pass | Applying the model to some input and computing the predictions.|
|Loss | A value that represents how well (or badly) our model is doing.|
|Gradient | The derivative of the loss with respect to some parameter of the model.|
|Backward pass | Computing the gradients of the loss with respect to all model parameters.|
|Gradient descent | Taking a step in the directions opposite to the gradients to make the model parameters a little bit better.|
|Learning rate | The size of the step we take when applying SGD to update the parameters of the model.|
|Activations | Numbers that are calculated (both by linear and nonlinear layers) |
|Parameters | Numbers that are randomly initialized, and optimized (that is, the numbers that define the model) |
|Special Tensors | Rank zero: scalar / Rank one: vector / Rank two: matrix |

### 4: Natural Language (NLP) (PDL2022)

#### 4.1 å‘å±•

ï¼ˆ1ï¼‰**ULMFit** (ç”¨çš„RNN): Wikitext(103)Language Model (30%å‡†ç¡®ç‡) â†’ IMDb Language Model â†’ IMDb Classifier

ï¼ˆ2ï¼‰**Transformers** (æ©ç è¯­è¨€å»ºæ¨¡): ç”¨äºæœºå™¨é˜…è¯»ç†è§£ã€å¥å­åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦ç­‰

ï¼ˆ3ï¼‰...

å¯ä»¥çœ‹åˆ°ä¼¼ä¹Transformerè¦æ¯”ULMFité«˜çº§ï¼Œå®é™…ä¸Šä¸¤è€…çš„ç”¨é€”ä¸åŒï¼›å¦å¤–ï¼ŒULMFitèƒ½å¤Ÿé˜…è¯»æ›´é•¿çš„å¥å­ï¼Œå¦‚æœä¸€ä¸ªdocumentåŒ…å«è¶…è¿‡2000ä¸ªå•è¯ï¼Œé‚£ä¹ˆå°±æ›´æ¨èä½¿ç”¨ULMFitè¿›è¡Œåˆ†ç±»ã€‚

#### 4.2 æœ€é‡è¦çš„package

1/ pandas

2/ numpy

3/ matplotlib

4/ pytorch

##### å‚è€ƒä¹¦ï¼š[Python for Data Analysis, 3E About the Open Edition]([Python for Data Analysis, 3E](https://wesmckinney.com/book/))

#### 4.3 Tokenization

A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:

- *Tokenization*: Split each text up into words (or actually, as we'll see, into *tokens*)
- *Numericalization*: Convert each word (or token) into a number.

The details about how this is done actually depend on the particular model we use. So first we'll need to pick a model. There are thousands of models available, but a reasonable starting point for nearly any NLP problem is to use thisï¼š

`microsoft/deberta-v3-small`

```python
tokz.tokenize("A platypus is an ornithorhynchus anatinus.")
'''
['â–A',
 'â–platypus',
 'â–is',
 'â–an',
 'â–or',
 'ni',
 'tho',
 'rhynch',
 'us',
 'â–an',
 'at',
 'inus',
 '.']
'''
```

#### 4.4 è®­ç»ƒé›†ä¸éªŒè¯é›†çš„åˆ’åˆ†

Training set & Validation set

In practice, a random split like we've used here might not be a good idea -- here's what Dr Rachel Thomas has to say about it:

> "*One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a `train_test_split` method, this method takes a random subset of the data, which is a poor choice for many real-world problems.*"

I strongly recommend reading her article [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/) to more fully understand this critical topic.

* éšæœºåˆ’åˆ†æ•°æ®é›†ä¸é€‚ç”¨çš„æƒ…å†µï¼šæ—¶é—´åºåˆ—ã€æ–°äººè„¸ã€æ–°çš„èˆ¹ç­‰ç­‰ï¼›
* cross-validationæ¯”è¾ƒå±é™©ï¼Œé™¤éç”¨åˆ°çš„caseæ˜¯é‚£ç§å¯ä»¥éšæœºæ´—ç‰Œçš„æƒ…å†µï¼ˆéšæœºåˆ†ABCä¸‰ç»„æ•°æ®é›†ï¼ŒABåˆå¹¶åšè®­ç»ƒé›†-CåšéªŒè¯é›†ï¼Œä¸‰ç»„å¾ªç¯ï¼Œæœ€åæ±‚å¹³å‡å€¼ä½œä¸ºæ¨¡å‹çš„performanceï¼‰ï¼›
* æ‰€ä»¥ç”¨ä¸€ä¸ªtest setæµ‹è¯•é›†å»æœ€ç»ˆç¡®è®¤ä¸€ä¸‹æ¨¡å‹çš„å¥½åä¹Ÿè›®é‡è¦çš„ã€‚

#### 4.5 Metrics

In real life, outside of Kaggle, things not easy... As my partner Dr Rachel Thomas notes in [The problem with metrics is a big problem for AI](https://www.fast.ai/2019/09/24/metrics/):

> At their heart, what most current AI approaches do is to optimize metrics. The practice of optimizing metrics is not new nor unique to AI, yet AI can be particularly efficient (even too efficient!) at doing so. This is important to understand, because any risks of optimizing metrics are heightened by AI. While metrics can be useful in their proper place, there are harms when they are unthinkingly applied. Some of the scariest instances of algorithms run amok all result from over-emphasizing metrics. We have to understand this dynamic in order to understand the urgent risks we are facing due to misuse of AI.

* Metricså¾ˆå¤šæ—¶å€™å¹¶ä¸æ˜¯*æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„äº‹æƒ…*ï¼Œå®ƒåªæ˜¯*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*çš„ä¸€ä¸ªä»£ç†ï¼Œå¦‚æˆ‘ä»¬å…³å¿ƒè€å¸ˆçš„æ•™å­¦æ•ˆæœï¼Œmetricsæ˜¯å­¦ç”Ÿçš„åˆ†æ•°ï¼›
* Metricsä¼šè¢«æ•…æ„åœ°ã€ä½œå¼Šåœ°æ‹‰é«˜ï¼Œä½¿å®ƒå¤±å»äº†è¡¡é‡*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*çš„èƒ½åŠ›ï¼Œå¦‚è€å¸ˆäººä¸ºæ‹‰é«˜å­¦ç”Ÿçš„åˆ†æ•°ï¼Œå®ƒä¸å†èƒ½åæ˜ è€å¸ˆçš„æ•™å­¦æ•ˆæœï¼›
* Metricsä¼šæ›´çŸ­è§†ï¼Œæ¯”å¦‚é“¶è¡Œä¸€æ—¦å°†cross-sellingè¿™ä¸ªmetricsä½œä¸ºç›®æ ‡ï¼Œå°±ä¼šå‚¬ç”Ÿå‡ºå„ç§è™šå‡å¼€æˆ·ï¼Œè€Œå®é™…ä¸Šé“¶è¡Œçš„ç›®æ ‡æ˜¯ç»´æŠ¤è‰¯å¥½çš„å®¢æˆ·å…³ç³»ï¼Œåè€…æ‰æ˜¯é•¿è¿œçš„æˆ˜ç•¥ï¼Œæ¯”å¦‚*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*æ˜¯æé«˜è§†é¢‘å½±å“åŠ›ï¼Œç”¨äº†ç‚¹å‡»ç‡ä½œä¸ºMetricsï¼Œå°±æ²¡æœ‰è€ƒè™‘åˆ°ä¸€äº›è§†é¢‘é•¿æœŸæ¥çœ‹å¯¹è¯»è€…çš„å¸®åŠ©å’Œå¡‘é€ ï¼›
* å¾ˆå¤šMetricsæ˜¯åœ¨ä¸€ä¸ªé«˜åº¦æˆç˜¾çš„ç¯å¢ƒæ”¶é›†æ•°æ®çš„ï¼Œæ¯”å¦‚æ•°æ®æ”¶é›†åˆ°å°æœ‹å‹å–œæ¬¢åƒç”œé£Ÿï¼Œç®—æ³•ä¼šè®©é£Ÿç‰©è¶Šæ¥è¶Šç”œï¼Œæ°¸è¿œä¸å¯èƒ½outputå‡ºæœ‰è¥å…»çš„é£Ÿç‰©ï¼›
* å°½ç®¡å¦‚æ­¤Metricsä¾ç„¶å¾ˆæœ‰ç”¨ï¼Œéœ€è¦è€ƒè™‘å¤šä¸ªmetricsæ¥é¿å…ä¸Šè¿°é—®é¢˜ï¼Œä½†æœ€ç»ˆæˆ‘ä»¬è¦åŠªåŠ›å°†å®ƒä»¬æ•´åˆï¼›
* Metricsé€šè¿‡å®šé‡æ–¹å¼è¡¡é‡ç»“æœï¼Œä½†æˆ‘ä»¬ä¾ç„¶éœ€è¦å®šæ€§çš„ä¿¡æ¯æ‰èƒ½è·å¾—å¥½çš„metricsï¼›
* å»è¯¢é—®å·²åœ¨æ­¤å±±ä¸­çš„äººæ°¸è¿œå¯ä»¥foreseeä¸€äº›ä¸è‰¯åæœï¼Œå¦‚è€å¸ˆå¯ä»¥å¾ˆå®¹æ˜“åœ°çŸ¥é“ï¼Œç”¨å­¦ç”Ÿåˆ†æ•°ä½œä¸ºå”¯ä¸€è¡¡é‡æ ‡å‡†ä¼šå¯¼è‡´ä»€ä¹ˆç³Ÿç³•çš„ç»“æœã€‚

#### 4.6 codes

```python
# æ£€æŸ¥æ˜¯å¦ä¸ºkaggleç¯å¢ƒ
import os
iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')
iskaggle

# ä¸‹è½½datasets
from pathlib import Path
if iskaggle:
    path = Path('../input/us-patent-phrase-to-phrase-matching')
    ! pip install -q datasets #åªéœ€è¦è·‘ä¸€æ¬¡å°±åˆ°äº†é¡µé¢ä¸­äº†
    
# ï¼è¡¨ç¤ºåé¢çš„ä¸æ˜¯pythonå‘½ä»¤ï¼Œæ˜¯shellå‘½ä»¤ï¼Œä½†æ˜¯å¦‚æœæƒ³åˆ©ç”¨åˆ°pythonä¸­çš„å‚æ•°ï¼Œå°±ç”¨{}æ¡†èµ·æ¥
!ls {path}
```

```python
#æ•°æ®çš„é¢„å¤„ç†å·¥ä½œ
# å®šä¹‰ä¸€ä¸ªè®­ç»ƒé›†çš„dataframe
import pandas as pd
df = pd.read_csv(path/'train.csv')
df.describe(include='object') #ä¸€ä¸ªéå¸¸é‡è¦çš„dataframeæ–¹æ³•

# æ„å»ºä¸€ä¸ªdf
df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor
df.input.head()
'''
0    TEXT1: A47; TEXT2: abatement of pollution; ANC...
1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...
2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...
3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...
4    TEXT1: A47; TEXT2: forest region; ANC1: abatement
Name: input, dtype: object
'''
df['input'][0], df.input[0]
''TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement''
            
# å®ä¾‹åŒ–ä¸€ä¸ªè®­ç»ƒé›†çš„Dataset
from datasets import Dataset,DatasetDict
ds = Dataset.from_pandas(df)
'''
Dataset({
    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],
    num_rows: 36473
})
å…¶ä¸­inputæ˜¯æ•´ä¸ªstringå¥å­
'''

# é€‰æ‹©ä¸€ä¸ªmodel,å°±æœ‰äº†å’Œè¿™ä¸ªmodelå¯¹åº”çš„vocabularyï¼Œç„¶åå®ä¾‹åŒ–å®ƒçš„tokzå·¥å…·
model_nm = 'microsoft/deberta-v3-small'
from transformers import AutoModelForSequenceClassification,AutoTokenizer
tokz = AutoTokenizer.from_pretrained(model_nm)

# å®šä¹‰ä¸€ä¸ªç”¨æ¥tokzæŸæ®µæ–‡å­—çš„å‡½æ•°ï¼Œå¹¶åº”ç”¨
def tok_func(x): return tokz(x["input"])
toknum_ds = ds.map(tok_func, batched=True) #æ¥è‡ªHuggingFaceçš„datasetsåº“,è¿™ä½¿å¾—å®ƒä¸å†åªæœ‰'input'è¿˜æœ‰'input_ids'
'''
.map(tok_func, batched=True)ï¼šå¯¹ ds ä¸­çš„æ•°æ®åº”ç”¨ tok_func è¿›è¡Œæ˜ å°„ï¼ˆmapï¼‰ï¼Œå¹¶ä¸”å¯ç”¨æ‰¹å¤„ç†ï¼ˆbatched=Trueï¼‰ï¼Œä»¥æé«˜æ•ˆç‡ã€‚
tok_dsï¼šè¿”å›ä¸€ä¸ªæ–°çš„ Datasetï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬å·²ç»è¢« tok_func å¤„ç†è¿‡ï¼ˆé€šå¸¸æ˜¯ Tokenizerï¼‰
'''
#row = toknum_ds[0]
#row['input'], row['input_ids'] 'â‘ ç¬¬ä¸€å¥è¯stringï¼Œâ‘¡ä¸€ä¸²æ•°å­—'
#tokz.vocab['â–of'] #è¿™é‡Œæœ‰ä¸€ä¸ªvocabularyï¼Œtockenâ†’æ•°å­—
#'265'

# å‡†å¤‡ä¸€ä¸ªlablesï¼Œtransformerä¸€å‘éƒ½è®¤ä¸ºlabelså°±è¯´æœ‰ä¸€åˆ—å«åšlabelsï¼Œæ‰€ä»¥å¾—æ”¹åå­—
toknum_ds = toknum_ds.rename_columns({'score':'labels'})

# æŠŠä¹‹å‰å·²ç»æ•°å­—åŒ–äº†çš„Datasetåˆ†ä¸€ä¸‹ï¼Œåˆ†æˆä¸€ä¸ªè®­ç»ƒé›†ä¸€ä¸ªéªŒè¯é›†
dds = toknum_ds.train_test_split(0.25, seed=42)

# å®šä¹‰Metrics
import numpy as np
def corr(x,y): return np.corrcoef(x,y)[0][1]
def corr_d(eval_pred): return {'pearson': corr(*eval_pred)} #eval_predé€šå¸¸æ˜¯åŒ…å«é¢„æµ‹å€¼å’Œå®é™…å€¼çš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œ*è¡¨ç¤ºå°†é¢„æµ‹å€¼å’Œå®é™…å€¼æ‹†è§£æˆä½ç½®å‚æ•°ä¼ é€’ç»™corr() [å¦å¤–ï¼Œè¿™ä¸ª'pearson'æ ‡ç­¾æœ€åä¼šå‡ºç°åœ¨è®­ç»ƒç»“æœé‡Œ]
```

```python
#è®­ç»ƒ
from transformers import TrainingArguments,Trainer
bs = 128
epochs = 4
lr = 8e-5
args = TrainingArguments(
    'outputs',                # è¾“å‡ºç›®å½•ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹å’Œæ—¥å¿—å°†ä¿å­˜åœ¨æ­¤ç›®å½•ä¸­
    learning_rate=lr,         # å­¦ä¹ ç‡ï¼šæ¨¡å‹ä¼˜åŒ–çš„æ­¥é•¿ï¼ˆlr æ˜¯äº‹å…ˆå®šä¹‰å¥½çš„å˜é‡ï¼‰
    warmup_ratio=0.1,         # é¢„çƒ­æ¯”ä¾‹ï¼šå­¦ä¹ ç‡ä» 0 çº¿æ€§å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡çš„æ¯”ä¾‹ï¼ˆ0.1 è¡¨ç¤º 10% çš„è®­ç»ƒæ­¥æ•°ä½œä¸ºé¢„çƒ­é˜¶æ®µï¼‰
    lr_scheduler_type='cosine',  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼šä½¿ç”¨ä½™å¼¦é€€ç«ï¼ˆCosine Annealingï¼‰ç­–ç•¥æ¥é€æ­¥é™ä½å­¦ä¹ ç‡
    fp16=True,                # æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦è®­ç»ƒï¼šå°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æµ®ç‚¹æ•°ç²¾åº¦é™ä¸º 16 ä½ï¼Œå¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜å ç”¨
    evaluation_strategy="epoch",  # è¯„ä¼°ç­–ç•¥ï¼šæ¯ä¸ªè®­ç»ƒè½®ï¼ˆepochï¼‰ç»“æŸåè¿›è¡Œè¯„ä¼°
    per_device_train_batch_size=bs,  # æ¯ä¸ªè®¾å¤‡ï¼ˆGPUï¼‰ä¸Šè®­ç»ƒçš„æ‰¹é‡å¤§å°ï¼ˆ`bs` æ˜¯äº‹å…ˆå®šä¹‰çš„å˜é‡ï¼‰
    per_device_eval_batch_size=bs*2,  # æ¯ä¸ªè®¾å¤‡ä¸Šè¯„ä¼°çš„æ‰¹é‡å¤§å°ï¼Œé€šå¸¸è¯„ä¼°æ—¶æ‰¹é‡å¯ä»¥ç¨å¤§ä¸€äº›
    num_train_epochs=epochs,  # è®­ç»ƒè½®æ•°ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¿›è¡Œçš„å®Œæ•´æ•°æ®é›†è¿­ä»£æ¬¡æ•°ï¼ˆ`epochs` æ˜¯äº‹å…ˆå®šä¹‰çš„å˜é‡ï¼‰
    weight_decay=0.01,        # æƒé‡è¡°å‡ï¼šL2 æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    report_to='none' # ç¦ç”¨æŠ¥å‘Šï¼ˆå¦‚ä¸æŠ¥å‘Šåˆ° TensorBoard æˆ– WandBï¼‰ï¼Œå¦‚æœéœ€è¦æŠ¥å‘Šï¼Œå¯ä»¥è®¾ç½®ä¸º 'tensorboard' æˆ– 'wandb'
)
model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) #å°†ç”¨çš„æ¨¡å‹
trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'], tokenizer=tokz,
                  compute_metrics=corr_d) #å°†modelã€è¶…å‚æ•°ä»¬ã€dataæ•´åˆåœ¨ä¸€èµ·çš„ç±»

trainer.train()
```

```python
#æµ‹è¯•é›†
# å®šä¹‰ä¸€ä¸ªæµ‹è¯•é›†çš„dataframeï¼Œæ„å»ºä¸€ä¸ªeval_dfï¼ŒåŸºäºè¿™å®ä¾‹åŒ–ä¸€ä¸ªeval_dså¹¶å°†å®ƒtokenization
eval_df = pd.read_csv(path/'test.csv')
eval_df.describe()
eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor
eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)

# é¢„æµ‹æµ‹è¯•é›†
preds = trainer.predict(eval_ds).predictions.astype(float)
preds = np.clip(preds, 0, 1) #å°†<0>1çš„æ•°å€¼è§„æ•´åˆ°0~1

#å°†ç»“æœå¯¼å‡ºåˆ°csv
import datasets
submission = datasets.Dataset.from_dict({
    'id': eval_ds['id'],
    'score': preds
})
submission.to_csv('submission.csv', index=False)
```

#### 4.7 è¶…å‚æ•°ï¼šæƒé‡è¡°å‡weight_decay

L2 æ­£åˆ™åŒ–é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ªä¸æ¨¡å‹æƒé‡çš„å¹³æ–¹å’Œæˆæ­£æ¯”çš„é¡¹æ¥å®ç°æƒ©ç½šã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæŸå¤±å‡½æ•° `L(w)`ï¼Œè¡¨ç¤ºæ¨¡å‹çš„æŸå¤±ï¼Œå…¶ä¸­ `w` æ˜¯æ¨¡å‹çš„æƒé‡å‚æ•°ï¼Œé‚£ä¹ˆåŠ å…¥ L2 æ­£åˆ™åŒ–åçš„æŸå¤±å‡½æ•° `L2(w)` å°±æ˜¯ï¼š
$$
L2(w)=L(w) + \lambda \sum_{i} w_i^2
$$
å…¶ä¸­ï¼š

- `L(w)` æ˜¯åŸå§‹çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€å‡æ–¹è¯¯å·®ç­‰ï¼‰ã€‚
- `w_i` æ˜¯æ¨¡å‹çš„ç¬¬ `i` ä¸ªæƒé‡ã€‚
- `Î»` æ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„è¶…å‚æ•°ï¼Œæ§åˆ¶ L2 æ­£åˆ™åŒ–çš„å½±å“ã€‚è¾ƒå¤§çš„ `Î»` ä¼šå¯¹æ¨¡å‹çš„è®­ç»ƒäº§ç”Ÿè¾ƒå¤§çš„å½±å“ã€‚

**ä½œç”¨**

- **é™åˆ¶æƒé‡çš„å¤§å°**ï¼šL2 æ­£åˆ™åŒ–é¼“åŠ±æ¨¡å‹çš„æƒé‡ `w` å°½å¯èƒ½å°ï¼Œé¿å…å‡ºç°è¿‡å¤§çš„æƒé‡å€¼ï¼Œè¿™æ ·å¯ä»¥å‡å°‘æ¨¡å‹çš„å¤æ‚åº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- **å¹³æ»‘æ¨¡å‹**ï¼šé€šè¿‡æŠ‘åˆ¶å¤§æƒé‡ï¼ŒL2 æ­£åˆ™åŒ–ä¿ƒä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´åŠ å¹³æ»‘çš„å‡½æ•°ï¼Œè€Œéè¿‡äºå¤æ‚çš„ã€è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„å‡½æ•°ã€‚
- **æ”¹è¿›æ³›åŒ–èƒ½åŠ›**ï¼šé€šè¿‡é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒL2 æ­£åˆ™åŒ–ä½¿å¾—æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ï¼ˆæµ‹è¯•é›†ï¼‰ä¸Šçš„è¡¨ç°æ›´åŠ ç¨³å®šã€‚

### 4: nlp (fastbook10)

#### 4.1 è‡ªç›‘ç£å­¦ä¹ 

ä½¿ç”¨åµŒå…¥åœ¨è‡ªå˜é‡ä¸­çš„æ ‡ç­¾æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸æ˜¯éœ€è¦å¤–éƒ¨æ ‡ç­¾ã€‚ä¾‹å¦‚ï¼Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚è‡ªç›‘ç£å­¦ä¹ ä¹Ÿå¯ä»¥ç”¨äºå…¶ä»–é¢†åŸŸï¼›ä¾‹å¦‚ï¼Œå‚è§[â€œè‡ªç›‘ç£å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰â€](https://oreil.ly/ECjfJ)ä»¥äº†è§£è§†è§‰åº”ç”¨ã€‚

> åªè¦æœ‰å¯èƒ½ï¼Œå°½å¯èƒ½ä½¿ç”¨ä¸€ä¸ªå·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹ã€‚å³ä¾¿æ˜¯è¯¥æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸæ²¡æœ‰ç»è¿‡è®­ç»ƒï¼Œåªæ˜¯é€šç”¨èŒƒå›´å†…è®­ç»ƒäº†ï¼Œç”¨å®ƒçš„æ—©æœŸå‡ ä¸ªå±‚ä¾ç„¶å¯ä»¥æé«˜æ–°æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œæ•ˆæœã€‚
>
> è‡ªç›‘ç£å­¦ä¹ ï¼šmodelç”¨çš„labelsæ¥è‡ªäºinputsã€‚
>
> Pretrained modelåœ¨é¢„è®­ç»ƒæ—¶çš„é‚£ä¸ªä»»åŠ¡å«åšpretext taskï¼Œè€Œæˆ‘ä»¬è¦å°†å®ƒç”¨åœ¨ç‰¹å®šé¢†åŸŸçš„é‚£ä¸ªä»»åŠ¡å«åšdownstream tasksã€‚
>
> Autoencoderï¼šå°†ä¸€å¼ å›¾å‹ç¼©ï¼Œä¹‹åå†å°†å®ƒå°½å¯èƒ½åœ°è¿˜åŸæˆåŸå›¾ã€‚ä½†å¦‚æœä½ çš„downstream taskæ˜¯ç”Ÿæˆä¸€å¼ æ¯”åŸå›¾æ›´é«˜æ¸…çš„å›¾ç‰‡ï¼Œè¿™ä¸ªæ¨¡å‹å°±ä¸é€‚åˆåšä½ çš„pretrained modelã€‚å¯è§ï¼Œpretext taskå’Œdownstream taskè¦å¥½å¥½åœ°å¯¹åº”ä½œç”¨ã€‚
>
> åˆ«èŠ±å¤ªå¤šæ—¶é—´åœ¨åˆ›å»ºpretrained modelä¸Šï¼Œåªè¦å®ƒåˆç†åœ°å¿«å’Œç®€å•å°±è¡Œã€‚Note also that you can do multiple rounds of self-supervised pretraining and regular pretraining. For instance, you could use one of the above approaches for initial pretraining, and then do segmentation for additional pretraining, and then finally train your downstream task. You could also do multiple tasks at once (multi-task learning) at either or both stages. But of course, do the simplest thing first, and then add complexity only if you determine you really need it!
>
> **Consistency Loss** 
>
> ä¸¾ä¾‹ï¼šæˆ‘ä»¬æœ¬æ¥éœ€è¦10ä¸‡æ¡æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œç°åœ¨ç”¨1ä¸‡æ¡æ•°æ®ï¼Œå¹¶å¯¹è¿™ä¸ªæ•°æ®è¿›è¡Œå¤„ç†ï¼ˆç¿»è½¬ã€æ—‹è½¬ã€è£å‰ªç­‰ã€åŒä¹‰è¯æ›¿æ¢ã€å›è¯‘ç­‰ï¼‰åšæ•°æ®å¢å¼ºï¼Œç„¶åç”¨è¿™1ä¸‡æ¡+å¢å¼ºæ•°æ®è¿›è¡Œè®­ç»ƒã€‚åœ¨è®­ç»ƒä¸­ï¼Œé™¤äº†æ­£å¸¸è®­ç»ƒï¼Œè¿˜ä¼šå»çœ‹æºæ•°æ®å’Œå¢å¼ºæ•°æ®çš„é¢„æµ‹ç»“æœæ˜¯ä¸æ˜¯ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬ä¸€æ ·ï¼Œé‡åŒ–å®ƒå°±å¼•å…¥Consistency Loss
>
> ![](D:\Git\a\Path-Records\img\04-1-1.jpg)

è‡ªç›‘ç£å­¦ä¹ é€šå¸¸ä¸ç”¨äºç›´æ¥è®­ç»ƒçš„æ¨¡å‹ï¼Œè€Œæ˜¯ç”¨äºé¢„è®­ç»ƒç”¨äºè¿ç§»å­¦ä¹ çš„æ¨¡å‹ã€‚

* é€šç”¨è¯­è¨€æ¨¡å‹å¾®è°ƒï¼ˆULMFiTï¼‰æ–¹æ³•ï¼šæœ‰ä¸€ä¸ªåŸºäºç»´åŸºç™¾ç§‘è¯­æ–™åº“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨IMDbçš„è¯­æ–™åº“è¿›è¡Œå¾®è°ƒï¼Œå†è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»

![](D:\Git\a\Path-Records\img\dlcf_1001.png)

* åˆ†è¯æ–¹æ³•ï¼šåŸºäºå•è¯çš„ã€åŸºäºå­è¯çš„å’ŒåŸºäºå­—ç¬¦çš„

#### 4.2 Tokenization & Numericalization

ç”±åˆ†è¯è¿‡ç¨‹åˆ›å»ºçš„åˆ—è¡¨çš„ä¸€ä¸ªå…ƒç´ ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªå•è¯*word tokenization*ï¼Œä¸€ä¸ªå•è¯çš„ä¸€éƒ¨åˆ†ï¼ˆä¸€ä¸ª*å­è¯*ï¼‰*subword tokenization*ï¼Œæˆ–ä¸€ä¸ªå•ä¸ªå­—ç¬¦ã€‚

##### 4.2.1 Word Tokenization

```python
from fastai.text.all import *
path = untar_data(URLs.IMDB)
path.ls()
# è·å–pathä¸­æŒ‡å®šfoldersä¸­çš„files
files = get_text_files(path, folders=['train', 'test', 'unsup'])
txt = files[0].open().read()
txt[:100]
```

```python
# WordTokenizeræ˜¯ä¸€ä¸ªåˆ†è¯å™¨ç±»ï¼Œå®ä¾‹åŒ–WordTokenizerç±»
spacy = WordTokenizer()
# first()ä½œç”¨æ˜¯è¿”å›åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚è¿™é‡Œå®ƒå–çš„æ˜¯spacy([txt])å¤„ç†åè¿”å›çš„ç¬¬ä¸€ä¸ªDocå¯¹è±¡çš„tokenåˆ—è¡¨ã€‚åˆ†è¯å™¨æ¥å—æ–‡æ¡£é›†åˆï¼Œæ‰€ä»¥ç”¨[txt]
toks = first(spacy([txt]))
# æ˜¾ç¤º`collection`çš„å‰`n`ä¸ªé¡¹ç›®ï¼Œä»¥åŠå®Œæ•´çš„å¤§å°â€”â€”è¿™æ˜¯`L`é»˜è®¤ä½¿ç”¨çš„
print(coll_repr(toks,30))
```

```python
# é€šè¿‡Tokenizerç±»å¢åŠ é¢å¤–çš„åŠŸèƒ½
tkn = Tokenizer(spacy)
print(coll_repr(tkn(txt),30))
'''
(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]
'''
```

ğŸ‘†ä¸€äº›ç‰¹æ®Šæ ‡è®°ï¼š

| ä¸»è¦ç‰¹æ®Šæ ‡è®° | ä»£è¡¨                                                         |
| ------------ | ------------------------------------------------------------ |
| xxbos        | æŒ‡ç¤ºæ–‡æœ¬çš„å¼€å§‹ï¼Œæ„æ€æ˜¯â€œæµçš„å¼€å§‹â€ï¼‰ã€‚é€šè¿‡è¯†åˆ«è¿™ä¸ªå¼€å§‹æ ‡è®°ï¼Œæ¨¡å‹å°†èƒ½å¤Ÿå­¦ä¹ éœ€è¦â€œå¿˜è®°â€å…ˆå‰è¯´è¿‡çš„å†…å®¹ï¼Œä¸“æ³¨äºå³å°†å‡ºç°çš„å•è¯ã€‚ |
| xxmaj        | æŒ‡ç¤ºä¸‹ä¸€ä¸ªå•è¯ä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼ˆå› ä¸ºå‡å°vocabularyçš„ä½“é‡ï¼ŒèŠ‚çœè®¡ç®—å’Œå†…å­˜èµ„æºï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å­—æ¯è½¬æ¢ä¸ºå°å†™ï¼‰ |
| xxunk        | æŒ‡ç¤ºä¸‹ä¸€ä¸ªå•è¯æ˜¯æœªçŸ¥çš„                                       |

##### 4.2.2 Subword Tokenization

```python
# è¯»å–200ä¸ªfilesä¸­çš„å¥å­txts
txts = L(o.open().read() for o in files[:200])
# å°†å¥å­txtsæ‹†åˆ†æˆszä¸ªvocabularyï¼Œå¹¶tokenize txt
def subword(sz):
    sp = SubwordTokenizer(vocab_sz=sz)
    sp.setup(txts)
    return ' '.join(first(sp([txt]))[:40])
# åˆ†æˆ1000ä¸ªvocabulary
subword(1000)
'â– J ian g â– X ian â–us es â–the â–comp le x â–back st or y â–of â–L ing â–L ing â–and â–Ma o â–D a o b ing â–to â–st ud y â–Ma o \' s â–" c'
# åˆ†æˆ100ä¸ªvocabularyï¼Œä¸ºäº†èƒ½æ‹†åˆ†ï¼Œå¥½å¤šéƒ½æ‹†æˆå­—æ¯äº†
subword(100)
'â– J i a n g â– X i a n â– u s e s â–the â– c o m p l e x â– b a c k s t o r y â– o f â– L'
```

##### 4.2.3 Numericalization

```python
# å°†txtså‰200æ¡textéƒ½Word Tokenizationï¼ˆä¹Ÿå¯ä»¥subword tokenizationï¼Œæœ¬ä¾‹ç”¨äº†å‰è€…ï¼‰
toks200 = txts[:200].map(tkn)
toks200[0]
'''
è·å¾—äº†ä¸€ä¸ªåˆ†è¯åçš„åˆ—è¡¨ï¼Œåˆ—è¡¨é•¿åº¦ä¸º200
(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to'...]
'''

# ç±»æ¯”ä¸Šé¢çš„subword(),å› ä¸ºè¦æ‰‹åŠ¨å»ºç«‹ä¸ªvocabularyï¼Œè¿™ä¸ªå®ä¾‹åŒ–åä¹Ÿè¦setupä¸€ä¸‹
num = Numericalize()
num.setup(toks200) #åŸºäºåˆ†è¯åçš„ç»“æœè®¾ç½®æ•°å­—æ˜ å°„
nums = num(toks)[:20]
'''
TensorText([   0,    0, 1269,    9, 1270,    0,   14,    0,    0,   12,    0,
               0,   15, 1271,    0,   22,   24,    0,  795,   24])
'''
# å°†æ•°å­—åŒ–çš„å¥å­å†æ˜ å°„å›tokens
' '.join(num.vocab[o] for o in nums)
'''
'xxunk xxunk uses the complex xxunk of xxunk xxunk and xxunk xxunk to study xxunk \'s " xxunk revolution "'
'''
```

`Numericalize`çš„é»˜è®¤å€¼ä¸º`min_freq=3`å’Œ`max_vocab=60000`ã€‚`max_vocab=60000`å¯¼è‡´ fastai ç”¨ç‰¹æ®Šçš„*æœªçŸ¥å•è¯*æ ‡è®°`xxunk`æ›¿æ¢é™¤æœ€å¸¸è§çš„ 60,000 ä¸ªå•è¯ä¹‹å¤–çš„æ‰€æœ‰å•è¯ã€‚è¿™æœ‰åŠ©äºé¿å…è¿‡å¤§çš„åµŒå…¥çŸ©é˜µï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦å¹¶å ç”¨å¤ªå¤šå†…å­˜ï¼Œå¹¶ä¸”è¿˜å¯èƒ½æ„å‘³ç€æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥è®­ç»ƒç¨€æœ‰å•è¯çš„æœ‰ç”¨è¡¨ç¤ºã€‚ç„¶è€Œï¼Œé€šè¿‡è®¾ç½®`min_freq`æ¥å¤„ç†æœ€åä¸€ä¸ªé—®é¢˜æ›´å¥½ï¼›é»˜è®¤å€¼`min_freq=3`æ„å‘³ç€å‡ºç°å°‘äºä¸‰æ¬¡çš„ä»»ä½•å•è¯éƒ½å°†è¢«æ›¿æ¢ä¸º`xxunk`ã€‚

##### 4.2.4 å°†è¿™äº›txtæ”¾è¿›batchesé‡Œé¢ï¼Œå½¢æˆDataLoader

<img src="D:\Git\a\Path-Records\img\04-2-4.jpg" style="zoom:100%;" />

```python
# toks200æ˜¯200æ¡txtåˆ†è¯åçš„ï¼Œnum200å°±æ˜¯200æ¡åˆ†è¯å¥å­æ˜ å°„æˆæ•°å­—çš„
num200 = toks200.map(num)
# DataLoader
dl = LMDataLoader(num200)
x,y = first(dl)
x.shape, y.shape
'(torch.Size([64, 72]), torch.Size([64, 72]))ï¼Œå¯è§DataLoaderå°†streamæ‹†æˆäº†64ä¸ªmini-streamï¼Œæ¯ä¸ªmini-streamæœ‰72ä¸ªtokens'
# xå’Œyåªæ˜¯ç›¸å·®ä¸€ä¸ªtoken
' '.join(num.vocab[o] for o in x[0][:15])
'xxbos xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and'
' '.join(num.vocab[o] for o in y[0][:15])
'xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj'
```

#### 4.3 è®­ç»ƒæ–‡æœ¬åˆ†ç±»å™¨

* ä½¿ç”¨è¿ç§»å­¦ä¹ è®­ç»ƒæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ†ç±»å™¨æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¾®è°ƒåœ¨ Wikipedia ä¸Šé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä»¥é€‚åº” IMDb è¯„è®ºçš„è¯­æ–™åº“ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹æ¥è®­ç»ƒåˆ†ç±»å™¨ã€‚

##### 4.3.1 è¯­è¨€è¯†åˆ«-æ•°æ®åŠ è½½å™¨DataBlock

* **å®ä¾‹æ–¹æ³•**ï¼Œéœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç„¶åæ‰èƒ½è°ƒç”¨çš„æ–¹æ³•ï¼ŒMyClass.instance_method()ä¼šæŠ¥é”™ï¼›**ç±»æ–¹æ³•**å°±ä¸éœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç›´æ¥è°ƒç”¨MyClass.class_method()ä¸ä¼šæŠ¥é”™ï¼Œè€Œä¸”å¯ä»¥è®¿é—®ç±»å˜é‡ï¼›**é™æ€æ–¹æ³•**ä¹Ÿä¸éœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç›´æ¥è°ƒç”¨MyClass.static_method()ä¹Ÿä¸ä¼šæŠ¥é”™ï¼Œä½†æ²¡åŠæ³•è®¿é—®ç±»å˜é‡ã€‚

```python
from fastai.text.all import *
path = untar_data(URLs.IMDB)
# è¿™æ˜¯ä¸Šé¢å…¨éƒ¨æ‰‹åŠ¨ä»£ç çš„æ±‡æ€»--------------------------------------------------------------------------------
files = get_text_files(path, folders=['train', 'test', 'unsup'])
#txt = files[0].open().read()
spacy = WordTokenizer()
#toks = first(spacy([txt]))
tkn = Tokenizer(spacy)
# è¯»å–200ä¸ªfilesä¸­çš„å¥å­txts
txts = L(o.open().read() for o in files[:200])
# å°†å¥å­txtsæ‹†åˆ†æˆszä¸ªvocabularyï¼Œå¹¶tokenize txt
#def subword(sz):
#    sp = SubwordTokenizer(vocab_sz=sz)
#    sp.setup(txts)
#    return ' '.join(first(sp([txt]))[:40])
#tks = tkn(txt)
toks = txts.map(tkn)
num = Numericalize()
num.setup(toks)
#nums = num(toks)[:20]
num = toks.map(num)
dl = LMDataLoader(num) #æ²¡æ³•æŒ‡å®šbatch_size
x,y = first(dl)
# fastaiæœ‰ç°æˆçš„æ–¹æ³•---------------------------------------------------------------------------------------
get_imdb = partial(get_text_files, folders=['train','test','unsup'])
# è¯­è¨€æ¨¡å‹çš„æ•°æ®åŠ è½½å™¨
dls_lm = DataBlock(
    blocks = TextBlock.from_folder(path,is_lm=True),
    get_items = get_imdb,
    splitter = RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
dls_lm.show_batch(max_n=3)
```

|      | text                                                         | text_                                                        |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 0    | xxbos xxmaj it â€™s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard | xxmaj it â€™s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard xxunk |
| 1    | what xxmaj i â€˜ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this | xxmaj i â€˜ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this is |

##### 4.3.2 è¯­è¨€è¯†åˆ«-Fine-tune

* **Embedding**ï¼šåµŒå…¥æ˜¯æŠŠæ–‡å­—è½¬æ¢æˆè®¡ç®—æœºèƒ½ç†è§£çš„æ•°å­—ï¼Œè€Œä¸”è¿™ç§è½¬æ¢ä¸æ˜¯ç®€å•çš„ 1 å¯¹ 1 æ˜ å°„ï¼Œè€Œæ˜¯è®©è¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨æ•°å€¼ç©ºé—´é‡Œä¹Ÿé å¾—æ›´è¿‘ã€‚å¸¸è§çš„ NLP ä»»åŠ¡éƒ½ä¼šç”¨åˆ° Embeddingï¼Œæ¯”å¦‚ï¼š**Word2Vec**ï¼ˆGoogle å¼€å‘çš„è¯å‘é‡æ¨¡å‹ï¼‰ã€**GloVe**ï¼ˆæ–¯å¦ç¦å¼€å‘çš„è¯å‘é‡ï¼‰ã€**FastText**ï¼ˆFacebook å¼€å‘çš„è¯å‘é‡ï¼‰ã€**BERT / GPT**ï¼ˆç°ä»£ NLP æ¨¡å‹çš„åº•å±‚éƒ½ä¼šç”¨æ›´é«˜çº§çš„ Embeddingï¼‰ã€‚

```python
learn = language_model_learner(
    dls_lm,
    AWD_LSTM,
    drop_mult=0.3,      # Dropout ä¹˜æ•°ï¼ˆæ§åˆ¶æ­£åˆ™åŒ–çš„ç¨‹åº¦ï¼‰
    metrics=[accuracy, Perplexity()]     # è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ + å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
).to_fp16()      # å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦ï¼ˆFP16ï¼‰ï¼Œæå‡è®­ç»ƒé€Ÿåº¦
```

* **æŸå¤±å‡½æ•°**ï¼šäº¤å‰ç†µæŸå¤±
* **Perplexity** metricså¸¸ç”¨åœ¨NLPä¸­ï¼Œå®ƒæ˜¯æŸå¤±å‡½æ•°çš„æŒ‡æ•°ï¼ˆå³torch.exp(cross_entropy)ï¼‰

* **Dropout**æ˜¯ä¸€ç§é˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„æ–¹æ³•ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºâ€œä¸¢å¼ƒâ€ï¼ˆè®¾ä¸º 0ï¼‰ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦ä¾èµ–æŸäº›ç‰¹å®šçš„ç‰¹å¾ã€‚

###### â‘ **Dropout vs. Weight Decayï¼šåŒºåˆ«å¯¹æ¯”**

| ç‰¹æ€§        | Dropout                                              | Weight Decay (L2 æ­£åˆ™åŒ–)             |
| ---------------- | -------------------------------------------------------- | ---------------------------------------- |
| ä½œç”¨æ–¹å¼       | éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼Œè®©ç½‘ç»œå­¦ä¼šä¸åŒçš„ç‰¹å¾             | é™åˆ¶æƒé‡å¤§å°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ             |
| é€‚ç”¨èŒƒå›´    | æ›´é€‚ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå°¤å…¶æ˜¯ CNNã€RNNã€Transformerï¼‰ | é€‚ç”¨äºå‡ ä¹æ‰€æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹           |
| è®­ç»ƒä¸æµ‹è¯•   | åªåœ¨è®­ç»ƒæ—¶ç”Ÿæ•ˆï¼Œæµ‹è¯•æ—¶å…³é—­                           | è®­ç»ƒå’Œæµ‹è¯•æ—¶éƒ½ç”Ÿæ•ˆ                   |
| æ•°å­¦å…¬å¼     | è®©éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºè®¾ä¸º 0                                 | ç»™æŸå¤±å‡½æ•°å¢åŠ  Î»âˆ‘w^2 æƒ©ç½šé¡¹              |
| ç›´è§‚ç†è§£     | è®©ç¥ç»ç½‘ç»œå˜æˆä¸€ä¸ªå°å‹é›†æˆå­¦ä¹                        | å‡å°‘å¤§æƒé‡ï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ•°æ® |
| å¯¹è®¡ç®—çš„å½±å“ | å¢åŠ è®¡ç®—é‡ï¼Œå› ä¸ºæ¯æ¬¡è®­ç»ƒéƒ½è¦éšæœºä¸¢å¼ƒä¸åŒç¥ç»å…ƒ       | ä¸ä¼šå¢åŠ è®¡ç®—é‡                       |

###### â‘¡**ä»€ä¹ˆæ—¶å€™ç”¨ Dropoutï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨ Weight Decayï¼Ÿ**

è™½ç„¶å®ƒä»¬çš„å®ç°æ–¹å¼ä¸åŒï¼Œä½†ç›®çš„éƒ½æ˜¯ é˜²æ­¢æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

âœ” å¯ä»¥ä¸€èµ·ä½¿ç”¨ï¼

- Dropout ä¸»è¦ä½œç”¨åœ¨ ç½‘ç»œç»“æ„ å±‚é¢ï¼ˆä¸¢å¼ƒç¥ç»å…ƒï¼‰
- Weight Decay ä¸»è¦ä½œç”¨åœ¨ å‚æ•°ä¼˜åŒ– å±‚é¢ï¼ˆçº¦æŸæƒé‡å¤§å°ï¼‰
- ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸ ä¸¤è€…éƒ½ç”¨

| æƒ…å†µ                                   | æ›´é€‚åˆ Dropout    | æ›´é€‚åˆ Weight Decay |
| ------------------------------------------ | --------------------- | ----------------------- |
| æ•°æ®é‡å°ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ                   | âœ…                     | âœ…                       |
| ç¥ç»ç½‘ç»œå¾ˆæ·±ï¼ˆCNNã€LSTMã€Transformerï¼‰ | âœ…                     | âœ…                       |
| å‚æ•°é‡å°‘ï¼ˆå°å‹æ¨¡å‹ï¼Œå¦‚çº¿æ€§å›å½’ï¼‰       | âŒ                     | âœ…                       |
| è¿‡æ‹Ÿåˆä¸¥é‡                             | âœ…ï¼ˆæé«˜ Dropout ç‡ï¼‰  | âœ…ï¼ˆå¢å¤§ Weight Decayï¼‰  |
| æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆRNNï¼‰                    | âœ…ï¼ˆDropout ä¹Ÿèƒ½å¸®å¿™ï¼‰ | âŒ                       |

ğŸ’¡ ç»éªŒæ³•åˆ™ï¼š

- å¤§æ¨¡å‹ï¼ˆCNN/RNNï¼‰ â†’ ä¸¤è€…éƒ½ç”¨ï¼Œdropout=0.3~0.5 + wd=1e-4
- å°æ¨¡å‹ï¼ˆçº¿æ€§å›å½’ï¼‰ â†’ ä¸»è¦ç”¨ Weight Decay
- æ•°æ®é‡ç‰¹åˆ«å° â†’ Dropout å¯ä»¥å°‘ç”¨ï¼Œä½† Weight Decay ä»ç„¶æœ‰æ•ˆ

**è®­ç»ƒ**ï¼š

åƒ`vision_learner`ä¸€æ ·ï¼Œå½“ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆè¿™æ˜¯é»˜è®¤è®¾ç½®ï¼‰æ—¶ï¼Œ`language_model_learner`åœ¨ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨`freeze`ã€‚å› æ­¤è¿™å°†ä»…è®­ç»ƒåµŒå…¥å±‚ï¼Œå…¶ä»–éƒ¨åˆ†çš„æƒé‡æ˜¯è¢«å†»ç»“çš„ã€‚ä¹‹æ‰€ä»¥åªè®­ç»ƒåµŒå…¥å±‚ï¼Œæ˜¯å› ä¸ºåœ¨ IMDb è¯­æ–™åº“ä¸­ï¼Œå¯èƒ½ä¼šæœ‰ä¸€äº›è¯æ±‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯è¡¨ä¸­æ‰¾ä¸åˆ°ï¼Œè¿™äº›è¯çš„åµŒå…¥ï¼ˆembeddingsï¼‰éœ€è¦éšæœºåˆå§‹åŒ–ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œè€Œé¢„è®­ç»ƒæ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†å·²ç»æœ‰äº†è¾ƒå¥½çš„å‚æ•°ï¼Œå› æ­¤æš‚æ—¶ä¸ä¼šè¢«è°ƒæ•´ã€‚

fine_tuneä¸ä¼šä¿å­˜åŠæˆå“æ¨¡å‹ç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨äº†fit_one_cycle

```python
learn.fit_one_cycle(1, 2e-2)
```

###### â‘¢**fit vs. fit_one_cycle å¯¹æ¯”**

| å¯¹æ¯”é¡¹     | fit                      | fit_one_cycle               |
| ---------- | ------------------------ | --------------------------- |
| å­¦ä¹ ç‡è°ƒåº¦ | å›ºå®šå­¦ä¹ ç‡               | åŠ¨æ€è°ƒæ•´ï¼ˆwarm-up + decayï¼‰ |
| åŠ¨é‡è°ƒåº¦   | ä¸å˜                     | è‡ªé€‚åº”è°ƒæ•´                  |
| é€‚ç”¨åœºæ™¯   | å°è§„æ¨¡è®­ç»ƒã€ç®€å•ä»»åŠ¡     | æ·±åº¦å­¦ä¹ ã€å¤§è§„æ¨¡è®­ç»ƒ        |
| ä¼˜ç‚¹       | ç®€å•ã€ç¨³å®š               | æé«˜æ³›åŒ–èƒ½åŠ›ã€æ”¶æ•›æ›´å¿«      |
| ç¼ºç‚¹       | å¯èƒ½è®­ç»ƒæ…¢ï¼Œæ³›åŒ–èƒ½åŠ›ä¸ä½³ | éœ€è¦è°ƒæ•´å‚æ•°ï¼Œç¨å¤æ‚        |

##### 4.3.3 è¯­è¨€è¯†åˆ«-ä¿å­˜æ¨¡å‹

```python
# ä¿å­˜ç»å†1æ¬¡epochçš„æ¨¡å‹çŠ¶æ€
learn.save('1epoch')
# åŠ è½½æ¨¡å‹
learn = learn.load('1epoch')
# åˆå§‹è®­ç»ƒå®Œæˆï¼Œè§£å†»åç»§ç»­å¾®è°ƒæ¨¡å‹
learn.unfreeze()
learn.fit_one_cycle(10,2e-3)
# ä¿å­˜ç¼–ç å™¨encoderï¼šç¼–ç å™¨å°±æ˜¯æˆ‘ä»¬è®­ç»ƒçš„æ‰€æœ‰æ¨¡å‹ï¼Œé™¤äº†æœ€åä¸€å±‚ï¼ˆå®ƒå°†activationsè½¬æ¢æˆé¢„æµ‹tokençš„æ¦‚ç‡ï¼‰
learn.save_encoder('finetuned')
```

æ­¤æ—¶å·²ç»å®Œæˆç¬¬äºŒé˜¶æ®µï¼š

![](D:\Git\a\Path-Records\img\dlcf_1001.png)

```python
# Kaggleä¸­å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¸‹è½½ä¸‹æ¥
path1 = Path('/kaggle/working/')
learn.save(path1/'mymodel')  # ä¿å­˜æ¨¡å‹
learn.load(path1/'mymodel') #åŠ è½½æ¨¡å‹

path2 = Path('/kaggle/working/')
learn.save_encoder(path2/'finetuned')
learn.load_encoder(path2/'finetuned')
```

###### fastaiçš„å¿«æ·æ„é€ å™¨learn

ä¸åŒä»»åŠ¡æœ‰ä¸åŒçš„å¿«æ·æ„é€ å™¨ï¼š

| ä»»åŠ¡                 | å¿«æ·æ–¹æ³•                                            | åº•å±‚ç±»    |
| -------------------- | --------------------------------------------------- | --------- |
| è®¡ç®—æœºè§†è§‰           | `vision_learner`                                    | `Learner` |
| è‡ªç„¶è¯­è¨€å¤„ç†         | `language_model_learner`ã€`text_classifier_learner` | `Learner` |
| è¡¨æ ¼æ•°æ®             | `tabular_learner`                                   | `Learner` |
| ååŒè¿‡æ»¤ï¼ˆæ¨èç³»ç»Ÿï¼‰ | `collab_learner`                                    | `Learner` |

##### 4.3.4 æ–‡æœ¬ç”Ÿæˆï¼ˆä¸åœ¨é˜¶æ®µä¸­ï¼‰

```python
TEXT = "I liked this movie because"
N_WORDS = 40
N_SENTENCES = 2
# temperature=0.75ï¼šæ§åˆ¶éšæœºæ€§ã€‚>1æ›´éšæœºæ›´æœ‰åˆ›é€ åŠ›ä½†å¯èƒ½æ²¡æ„ä¹‰ï¼›<1æ›´ç¡®å®šæ›´åˆç†ä½†å¯èƒ½è¾ƒæ— èŠ
preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)
         for _ in range(N_SENTENCES)]
print("\n".join(preds))
```

##### 4.3.5 æ–‡æœ¬åˆ†ç±»-æ•°æ®åŠ è½½å™¨DataBlock

```python
# åˆ›å»ºæ•°æ®åŠ è½½å™¨
dls_clas = DataBlock(
    blocks=(TextBlock.from_folder(path,vocab=dls_lm.vocab),CategoryBlock),
    '''ä½¿ç”¨ dls_lm.vocab çš„è¯æ±‡è¡¨å¯¹æ–‡æœ¬è¿›è¡Œæ•°å€¼åŒ–ï¼Œä»¥ä¿æŒå’Œ dls_lm ç›¸åŒçš„è¯ç´¢å¼•ç¼–å·ï¼Œè¿™ä¸ªè¯æ±‡è¡¨ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å­—åºåˆ—ï¼Œ1 tokenâ†’1 integer
    é€šè¿‡ä¼ é€’ `is_lm=False`ï¼ˆæˆ–è€…æ ¹æœ¬ä¸ä¼ é€’ `is_lm`ï¼Œå› ä¸ºå®ƒé»˜è®¤ä¸º `False`ï¼‰ï¼Œæˆ‘ä»¬å‘Šè¯‰ `TextBlock` æˆ‘ä»¬æœ‰å¸¸è§„æ ‡è®°çš„æ•°æ®ï¼Œè€Œä¸æ˜¯å°†ä¸‹ä¸€ä¸ªæ ‡è®°ä½œä¸ºæ ‡ç­¾'''
    get_y=parent_label,
    get_items=partial(get_text_files,folders=['train','test']),
    splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path,path=path,bs=128,seq_len=72)
dls_clas.show_batch(max_n=4)
```

```python
# åªä¸ºäº†è¯´æ˜æ€ä¹ˆå¤„ç†åœ¨åˆ†ç±»ç®—æ³•ä¸­texté•¿çŸ­ä¸ä¸€çš„é—®é¢˜
files = get_text_files(path, folders=['train','test','unsup'])
txts = L(o.open().read() for o in files[:200])
spacy = WordTokenizer()
tkn = Tokenizer(spacy)
toks200 = txts.map(tkn)
num = Numericalize()
num.setup(toks200)
nums_samp = toks200[:10].map(num)
nums_samp.map(len)
'(#10) [158,319,181,193,114,145,260,146,252,295]ï¼šå¯è§Lç±»æ¯æ¡å¤§å°ä¸ç­‰'
```

ä½†æ˜¯ï¼ŒPyTorch çš„ DataLoaderéœ€è¦å°†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰é¡¹ç›®æ•´åˆåˆ°ä¸€ä¸ªå¼ é‡ä¸­ï¼Œè€Œä¸€ä¸ªå¼ é‡å…·æœ‰å›ºå®šçš„å½¢çŠ¶ï¼ˆå³ï¼Œæ¯ä¸ªè½´ä¸Šéƒ½æœ‰ç‰¹å®šçš„é•¿åº¦ï¼Œå¹¶ä¸”æ‰€æœ‰é¡¹ç›®å¿…é¡»ä¸€è‡´ï¼‰ï¼Œä¸ºäº†è®©å®ƒä»¬ç›¸ç­‰ï¼Œå°±å¾—å¡«å……ã€‚

* å¡«å……ï¼šæˆ‘ä»¬å°†æ‰©å±•æœ€çŸ­çš„æ–‡æœ¬ä»¥ä½¿å®ƒä»¬éƒ½å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„å¡«å……æ ‡è®°ï¼Œè¯¥æ ‡è®°å°†è¢«æˆ‘ä»¬çš„æ¨¡å‹å¿½ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…å†…å­˜é—®é¢˜å¹¶æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬å°†å¤§è‡´ç›¸åŒé•¿åº¦çš„æ–‡æœ¬æ‰¹é‡å¤„ç†åœ¨ä¸€èµ·ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªepochå‰ï¼ˆå¯¹äºè®­ç»ƒé›†ï¼‰æŒ‰é•¿åº¦å¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼Œåˆ†æˆå‡ ä¸ªbatchesï¼Œä½¿ç”¨æ¯ä¸ªbatchä¸­æœ€å¤§æ–‡æ¡£çš„å¤§å°ä½œä¸ºè¿™ä¸ªbatchçš„ç›®æ ‡å¤§å°ã€‚

* ä½¿ç”¨DataBlock+is_lm=Falseæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬æ“ä½œã€‚

###### DataBlock & DataLoaders

| **ç»„ä»¶**         | **ä½œç”¨**                            |
| ---------------- | ----------------------------------- |
| `DataBlock`      | åªæ˜¯ æ•°æ®å¤„ç†çš„æ¨¡æ¿ï¼Œä¸åŒ…å«æ•°æ®     |
| `.dataloaders()` | å°† `DataBlock` è½¬ä¸º `DataLoaders`   |
| `DataLoaders`    | çœŸæ­£çš„æ•°æ®åŠ è½½å™¨ï¼ŒåŒ…å«è®­ç»ƒ/éªŒè¯æ•°æ® |

###### DataLoadersçš„ä¸»è¦å˜ç§

| å˜ç§åç§°                  | é€‚ç”¨ä»»åŠ¡                                |
| ------------------------- | --------------------------------------- |
| `ImageDataLoaders`        | å›¾åƒåˆ†ç±»ï¼ˆè‡ªåŠ¨ä»æ–‡ä»¶å¤¹åŠ è½½å›¾åƒï¼‰        |
| `TextDataLoaders`         | è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼ˆè‡ªåŠ¨å¤„ç†æ–‡æœ¬æ•°æ®ï¼‰ |
| `TabularDataLoaders`      | è¡¨æ ¼æ•°æ®ï¼ˆç”¨äºç»“æ„åŒ–æ•°æ®ï¼Œå¦‚ CSVï¼‰      |
| `SegmentationDataLoaders` | å›¾åƒåˆ†å‰²ï¼ˆåƒç´ çº§åˆ†ç±»ä»»åŠ¡ï¼‰              |
| `DataLoaders`ï¼ˆåŸºç±»ï¼‰     | é€šç”¨æ•°æ®åŠ è½½å™¨ï¼Œé€‚ç”¨äºè‡ªå®šä¹‰æ•°æ®        |

##### 4.3.6 æ–‡æœ¬åˆ†ç±»-Fine-tune

```python
learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,metrics=accuracy).to_fp16()
learn = learn.load_encoder('/kaggle/input/finetuned/pytorch/default/1/finetuned')
```

* é€å±‚è§£å†»gradual unfreezingï¼šå†»ç»“æ¨¡å‹çš„éƒ¨åˆ†å±‚ï¼Œå…ˆè®­ç»ƒæ¨¡å‹çš„æœ€åä¸€å±‚ï¼Œç„¶åé€æ¸è§£å†»å‰é¢çš„å±‚ã€‚ä»æœ€åä¸€å±‚å¼€å§‹è®­ç»ƒï¼Œå› ä¸ºè¿™äº›å±‚å·²ç»åŒ…å«äº†å¤§éƒ¨åˆ†çš„ç‰¹å¾ä¿¡æ¯ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸è§£å†»å‰é¢çš„å±‚ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ã€‚
* é€å±‚è§£å†»çš„åŸå› ï¼šâ‘ é¢„è®­ç»ƒçš„ç‰¹å¾æå–å±‚-åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œé€šå¸¸å·²ç»å­¦åˆ°äº†é€šç”¨çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒè¿™äº›å±‚ï¼Œè€Œæ˜¯å†»ç»“å®ƒä»¬ï¼Œä¸“æ³¨äºè®­ç»ƒæ–°ä»»åŠ¡çš„æœ€åå‡ å±‚ã€‚â‘¡é¿å…è¿‡æ‹Ÿåˆ-å¦‚æœä¸€æ¬¡æ€§è§£å†»æ‰€æœ‰å±‚ï¼Œæ¨¡å‹å¯èƒ½ä¼šå¿«é€Ÿè¿‡æ‹Ÿåˆï¼Œå› ä¸ºè¾ƒå°çš„æ•°æ®é›†æ— æ³•æ”¯æŒæ‰€æœ‰å±‚çš„è®­ç»ƒã€‚é€å±‚è§£å†»æœ‰åŠ©äºé€æ¸é€‚åº”æ–°ä»»åŠ¡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆçš„é£é™©ã€‚â‘¢è®­ç»ƒæ•ˆç‡-å†»ç»“å‰é¢å‡ å±‚åï¼Œè®¡ç®—é‡å‡å°‘ï¼Œè®­ç»ƒé€Ÿåº¦åŠ å¿«ã€‚

```python
learn.fit_one_cycle(1,2e-2) #é»˜è®¤æ¨¡å‹ä¼šå†»ç»“ï¼Œå› æ­¤åªæ˜¯è®­ç»ƒæœ€åä¸€å±‚
# è®­ç»ƒå€’æ•°2å±‚
learn.freeze_to(-2)
learn.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2))
# è®­ç»ƒå€’æ•°3å±‚
learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))
# è®­ç»ƒæ‰€æœ‰å±‚
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
```

#### 4.4 æ€»æµç¨‹codes

```python
# å‡†å¤‡
from fastai.text.all import *
path = untar_data(URLs.IMDB)
# è¯­è¨€è¯†åˆ«æ•°æ®åŠ è½½å™¨
get_imdb = partial(get_text_files, folders=['train','test','unsup'])
dls_lm = DataBlock(
    blocks = TextBlock.from_folder(path,is_lm=True),
    get_items = get_imdb,
    splitter = RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
dls_lm.show_batch(max_n=3)
# è¯­è¨€è¯†åˆ«learn
learn = language_model_learner(
    dls_lm,
    AWD_LSTM,
    drop_mult=0.3,
    metrics=[accuracy, Perplexity()]
).to_fp16()
# è¯­è¨€è¯†åˆ«fine-tuneå’Œä¿å­˜ç¼–ç å™¨
learn.fit_one_cycle(1, 2e-2)
learn.save('1epoch')
learn.unfreeze()
learn.fit_one_cycle(1,2e-3)
learn.save_encoder('finetuned')
# æ–‡æœ¬åˆ†ç±»æ•°æ®åŠ è½½å™¨
dls_clas = DataBlock(
    blocks=(TextBlock.from_folder(path,vocab=dls_lm.vocab),CategoryBlock),
    get_y=parent_label,
    get_items=partial(get_text_files,folders=['train','test']),
    splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path,path=path,bs=128,seq_len=72)
# æ–‡æœ¬åˆ†ç±»learn
learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,metrics=accuracy).to_fp16()
learn = learn.load_encoder('/kaggle/input/finetuned/pytorch/default/1/finetuned')
# æ–‡æœ¬ç”Ÿæˆfine-tune
learn.fit_one_cycle(1,2e-2) 
learn.freeze_to(-2)
learn.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2))
learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
```

