**<font size=8>fast.ai</font>**

## [Practical Deep Learning 2022](https://course.fast.ai/) & [Fastbook](https://github.com/fastai/fastbook)
### 4: Natural Language (NLP) (PDL2022)

#### 4.1 å‘å±•

ï¼ˆ1ï¼‰**ULMFit** (ç”¨çš„RNN): Wikitext(103)Language Model (30%å‡†ç¡®ç‡) â†’ IMDb Language Model â†’ IMDb Classifier

ï¼ˆ2ï¼‰**Transformers** (æ©ç è¯­è¨€å»ºæ¨¡): ç”¨äºæœºå™¨é˜…è¯»ç†è§£ã€å¥å­åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦ç­‰

ï¼ˆ3ï¼‰...

å¯ä»¥çœ‹åˆ°ä¼¼ä¹Transformerè¦æ¯”ULMFité«˜çº§ï¼Œå®é™…ä¸Šä¸¤è€…çš„ç”¨é€”ä¸åŒï¼›å¦å¤–ï¼ŒULMFitèƒ½å¤Ÿé˜…è¯»æ›´é•¿çš„å¥å­ï¼Œå¦‚æœä¸€ä¸ªdocumentåŒ…å«è¶…è¿‡2000ä¸ªå•è¯ï¼Œé‚£ä¹ˆå°±æ›´æ¨èä½¿ç”¨ULMFitè¿›è¡Œåˆ†ç±»ã€‚

#### 4.2 æœ€é‡è¦çš„package

1/ pandas

2/ numpy

3/ matplotlib

4/ pytorch

##### å‚è€ƒä¹¦ï¼š[Python for Data Analysis, 3E About the Open Edition]([Python for Data Analysis, 3E](https://wesmckinney.com/book/))

#### 4.3 Tokenization

A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:

- *Tokenization*: Split each text up into words (or actually, as we'll see, into *tokens*)
- *Numericalization*: Convert each word (or token) into a number.

The details about how this is done actually depend on the particular model we use. So first we'll need to pick a model. There are thousands of models available, but a reasonable starting point for nearly any NLP problem is to use thisï¼š

`microsoft/deberta-v3-small`

```python
tokz.tokenize("A platypus is an ornithorhynchus anatinus.")
'''
['â–A',
 'â–platypus',
 'â–is',
 'â–an',
 'â–or',
 'ni',
 'tho',
 'rhynch',
 'us',
 'â–an',
 'at',
 'inus',
 '.']
'''
```

#### 4.4 è®­ç»ƒé›†ä¸éªŒè¯é›†çš„åˆ’åˆ†

Training set & Validation set

In practice, a random split like we've used here might not be a good idea -- here's what Dr Rachel Thomas has to say about it:

> "*One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a `train_test_split` method, this method takes a random subset of the data, which is a poor choice for many real-world problems.*"

I strongly recommend reading her article [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/) to more fully understand this critical topic.

* éšæœºåˆ’åˆ†æ•°æ®é›†ä¸é€‚ç”¨çš„æƒ…å†µï¼šæ—¶é—´åºåˆ—ã€æ–°äººè„¸ã€æ–°çš„èˆ¹ç­‰ç­‰ï¼›
* cross-validationæ¯”è¾ƒå±é™©ï¼Œé™¤éç”¨åˆ°çš„caseæ˜¯é‚£ç§å¯ä»¥éšæœºæ´—ç‰Œçš„æƒ…å†µï¼ˆéšæœºåˆ†ABCä¸‰ç»„æ•°æ®é›†ï¼ŒABåˆå¹¶åšè®­ç»ƒé›†-CåšéªŒè¯é›†ï¼Œä¸‰ç»„å¾ªç¯ï¼Œæœ€åæ±‚å¹³å‡å€¼ä½œä¸ºæ¨¡å‹çš„performanceï¼‰ï¼›
* æ‰€ä»¥ç”¨ä¸€ä¸ªtest setæµ‹è¯•é›†å»æœ€ç»ˆç¡®è®¤ä¸€ä¸‹æ¨¡å‹çš„å¥½åä¹Ÿè›®é‡è¦çš„ã€‚

#### 4.5 Metrics

In real life, outside of Kaggle, things not easy... As my partner Dr Rachel Thomas notes in [The problem with metrics is a big problem for AI](https://www.fast.ai/2019/09/24/metrics/):

> At their heart, what most current AI approaches do is to optimize metrics. The practice of optimizing metrics is not new nor unique to AI, yet AI can be particularly efficient (even too efficient!) at doing so. This is important to understand, because any risks of optimizing metrics are heightened by AI. While metrics can be useful in their proper place, there are harms when they are unthinkingly applied. Some of the scariest instances of algorithms run amok all result from over-emphasizing metrics. We have to understand this dynamic in order to understand the urgent risks we are facing due to misuse of AI.

* Metricså¾ˆå¤šæ—¶å€™å¹¶ä¸æ˜¯*æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„äº‹æƒ…*ï¼Œå®ƒåªæ˜¯*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*çš„ä¸€ä¸ªä»£ç†ï¼Œå¦‚æˆ‘ä»¬å…³å¿ƒè€å¸ˆçš„æ•™å­¦æ•ˆæœï¼Œmetricsæ˜¯å­¦ç”Ÿçš„åˆ†æ•°ï¼›
* Metricsä¼šè¢«æ•…æ„åœ°ã€ä½œå¼Šåœ°æ‹‰é«˜ï¼Œä½¿å®ƒå¤±å»äº†è¡¡é‡*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*çš„èƒ½åŠ›ï¼Œå¦‚è€å¸ˆäººä¸ºæ‹‰é«˜å­¦ç”Ÿçš„åˆ†æ•°ï¼Œå®ƒä¸å†èƒ½åæ˜ è€å¸ˆçš„æ•™å­¦æ•ˆæœï¼›
* Metricsä¼šæ›´çŸ­è§†ï¼Œæ¯”å¦‚é“¶è¡Œä¸€æ—¦å°†cross-sellingè¿™ä¸ªmetricsä½œä¸ºç›®æ ‡ï¼Œå°±ä¼šå‚¬ç”Ÿå‡ºå„ç§è™šå‡å¼€æˆ·ï¼Œè€Œå®é™…ä¸Šé“¶è¡Œçš„ç›®æ ‡æ˜¯ç»´æŠ¤è‰¯å¥½çš„å®¢æˆ·å…³ç³»ï¼Œåè€…æ‰æ˜¯é•¿è¿œçš„æˆ˜ç•¥ï¼Œæ¯”å¦‚*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*æ˜¯æé«˜è§†é¢‘å½±å“åŠ›ï¼Œç”¨äº†ç‚¹å‡»ç‡ä½œä¸ºMetricsï¼Œå°±æ²¡æœ‰è€ƒè™‘åˆ°ä¸€äº›è§†é¢‘é•¿æœŸæ¥çœ‹å¯¹è¯»è€…çš„å¸®åŠ©å’Œå¡‘é€ ï¼›
* å¾ˆå¤šMetricsæ˜¯åœ¨ä¸€ä¸ªé«˜åº¦æˆç˜¾çš„ç¯å¢ƒæ”¶é›†æ•°æ®çš„ï¼Œæ¯”å¦‚æ•°æ®æ”¶é›†åˆ°å°æœ‹å‹å–œæ¬¢åƒç”œé£Ÿï¼Œç®—æ³•ä¼šè®©é£Ÿç‰©è¶Šæ¥è¶Šç”œï¼Œæ°¸è¿œä¸å¯èƒ½outputå‡ºæœ‰è¥å…»çš„é£Ÿç‰©ï¼›
* å°½ç®¡å¦‚æ­¤Metricsä¾ç„¶å¾ˆæœ‰ç”¨ï¼Œéœ€è¦è€ƒè™‘å¤šä¸ªmetricsæ¥é¿å…ä¸Šè¿°é—®é¢˜ï¼Œä½†æœ€ç»ˆæˆ‘ä»¬è¦åŠªåŠ›å°†å®ƒä»¬æ•´åˆï¼›
* Metricsé€šè¿‡å®šé‡æ–¹å¼è¡¡é‡ç»“æœï¼Œä½†æˆ‘ä»¬ä¾ç„¶éœ€è¦å®šæ€§çš„ä¿¡æ¯æ‰èƒ½è·å¾—å¥½çš„metricsï¼›
* å»è¯¢é—®å·²åœ¨æ­¤å±±ä¸­çš„äººæ°¸è¿œå¯ä»¥foreseeä¸€äº›ä¸è‰¯åæœï¼Œå¦‚è€å¸ˆå¯ä»¥å¾ˆå®¹æ˜“åœ°çŸ¥é“ï¼Œç”¨å­¦ç”Ÿåˆ†æ•°ä½œä¸ºå”¯ä¸€è¡¡é‡æ ‡å‡†ä¼šå¯¼è‡´ä»€ä¹ˆç³Ÿç³•çš„ç»“æœã€‚

#### 4.6 codes

```python
# æ£€æŸ¥æ˜¯å¦ä¸ºkaggleç¯å¢ƒ
import os
iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')
iskaggle

# ä¸‹è½½datasets
from pathlib import Path
if iskaggle:
    path = Path('../input/us-patent-phrase-to-phrase-matching')
    ! pip install -q datasets #åªéœ€è¦è·‘ä¸€æ¬¡å°±åˆ°äº†é¡µé¢ä¸­äº†
    
# ï¼è¡¨ç¤ºåé¢çš„ä¸æ˜¯pythonå‘½ä»¤ï¼Œæ˜¯shellå‘½ä»¤ï¼Œä½†æ˜¯å¦‚æœæƒ³åˆ©ç”¨åˆ°pythonä¸­çš„å‚æ•°ï¼Œå°±ç”¨{}æ¡†èµ·æ¥
!ls {path}
```

```python
#æ•°æ®çš„é¢„å¤„ç†å·¥ä½œ
# å®šä¹‰ä¸€ä¸ªè®­ç»ƒé›†çš„dataframe
import pandas as pd
df = pd.read_csv(path/'train.csv')
df.describe(include='object') #ä¸€ä¸ªéå¸¸é‡è¦çš„dataframeæ–¹æ³•

# æ„å»ºä¸€ä¸ªdf
df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor
df.input.head()
'''
0    TEXT1: A47; TEXT2: abatement of pollution; ANC...
1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...
2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...
3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...
4    TEXT1: A47; TEXT2: forest region; ANC1: abatement
Name: input, dtype: object
'''
df['input'][0], df.input[0]
''TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement''
            
# å®ä¾‹åŒ–ä¸€ä¸ªè®­ç»ƒé›†çš„Dataset
from datasets import Dataset,DatasetDict
ds = Dataset.from_pandas(df)
'''
Dataset({
    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],
    num_rows: 36473
})
å…¶ä¸­inputæ˜¯æ•´ä¸ªstringå¥å­
'''

# é€‰æ‹©ä¸€ä¸ªmodel,å°±æœ‰äº†å’Œè¿™ä¸ªmodelå¯¹åº”çš„vocabularyï¼Œç„¶åå®ä¾‹åŒ–å®ƒçš„tokzå·¥å…·
model_nm = 'microsoft/deberta-v3-small'
from transformers import AutoModelForSequenceClassification,AutoTokenizer
tokz = AutoTokenizer.from_pretrained(model_nm)

# å®šä¹‰ä¸€ä¸ªç”¨æ¥tokzæŸæ®µæ–‡å­—çš„å‡½æ•°ï¼Œå¹¶åº”ç”¨
def tok_func(x): return tokz(x["input"])
toknum_ds = ds.map(tok_func, batched=True) #æ¥è‡ªHuggingFaceçš„datasetsåº“,è¿™ä½¿å¾—å®ƒä¸å†åªæœ‰'input'è¿˜æœ‰'input_ids'
'''
.map(tok_func, batched=True)ï¼šå¯¹ ds ä¸­çš„æ•°æ®åº”ç”¨ tok_func è¿›è¡Œæ˜ å°„ï¼ˆmapï¼‰ï¼Œå¹¶ä¸”å¯ç”¨æ‰¹å¤„ç†ï¼ˆbatched=Trueï¼‰ï¼Œä»¥æé«˜æ•ˆç‡ã€‚
tok_dsï¼šè¿”å›ä¸€ä¸ªæ–°çš„ Datasetï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬å·²ç»è¢« tok_func å¤„ç†è¿‡ï¼ˆé€šå¸¸æ˜¯ Tokenizerï¼‰
'''
#row = toknum_ds[0]
#row['input'], row['input_ids'] 'â‘ ç¬¬ä¸€å¥è¯stringï¼Œâ‘¡ä¸€ä¸²æ•°å­—'
#tokz.vocab['â–of'] #è¿™é‡Œæœ‰ä¸€ä¸ªvocabularyï¼Œtockenâ†’æ•°å­—
#'265'

# å‡†å¤‡ä¸€ä¸ªlablesï¼Œtransformerä¸€å‘éƒ½è®¤ä¸ºlabelså°±è¯´æœ‰ä¸€åˆ—å«åšlabelsï¼Œæ‰€ä»¥å¾—æ”¹åå­—
toknum_ds = toknum_ds.rename_columns({'score':'labels'})

# æŠŠä¹‹å‰å·²ç»æ•°å­—åŒ–äº†çš„Datasetåˆ†ä¸€ä¸‹ï¼Œåˆ†æˆä¸€ä¸ªè®­ç»ƒé›†ä¸€ä¸ªéªŒè¯é›†
dds = toknum_ds.train_test_split(0.25, seed=42)

# å®šä¹‰Metrics
import numpy as np
def corr(x,y): return np.corrcoef(x,y)[0][1]
def corr_d(eval_pred): return {'pearson': corr(*eval_pred)} #eval_predé€šå¸¸æ˜¯åŒ…å«é¢„æµ‹å€¼å’Œå®é™…å€¼çš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œ*è¡¨ç¤ºå°†é¢„æµ‹å€¼å’Œå®é™…å€¼æ‹†è§£æˆä½ç½®å‚æ•°ä¼ é€’ç»™corr() [å¦å¤–ï¼Œè¿™ä¸ª'pearson'æ ‡ç­¾æœ€åä¼šå‡ºç°åœ¨è®­ç»ƒç»“æœé‡Œ]
```

```python
#è®­ç»ƒ
from transformers import TrainingArguments,Trainer
bs = 128
epochs = 4
lr = 8e-5
args = TrainingArguments(
    'outputs',                # è¾“å‡ºç›®å½•ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹å’Œæ—¥å¿—å°†ä¿å­˜åœ¨æ­¤ç›®å½•ä¸­
    learning_rate=lr,         # å­¦ä¹ ç‡ï¼šæ¨¡å‹ä¼˜åŒ–çš„æ­¥é•¿ï¼ˆlr æ˜¯äº‹å…ˆå®šä¹‰å¥½çš„å˜é‡ï¼‰
    warmup_ratio=0.1,         # é¢„çƒ­æ¯”ä¾‹ï¼šå­¦ä¹ ç‡ä» 0 çº¿æ€§å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡çš„æ¯”ä¾‹ï¼ˆ0.1 è¡¨ç¤º 10% çš„è®­ç»ƒæ­¥æ•°ä½œä¸ºé¢„çƒ­é˜¶æ®µï¼‰
    lr_scheduler_type='cosine',  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼šä½¿ç”¨ä½™å¼¦é€€ç«ï¼ˆCosine Annealingï¼‰ç­–ç•¥æ¥é€æ­¥é™ä½å­¦ä¹ ç‡
    fp16=True,                # æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦è®­ç»ƒï¼šå°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æµ®ç‚¹æ•°ç²¾åº¦é™ä¸º 16 ä½ï¼Œå¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜å ç”¨
    evaluation_strategy="epoch",  # è¯„ä¼°ç­–ç•¥ï¼šæ¯ä¸ªè®­ç»ƒè½®ï¼ˆepochï¼‰ç»“æŸåè¿›è¡Œè¯„ä¼°
    per_device_train_batch_size=bs,  # æ¯ä¸ªè®¾å¤‡ï¼ˆGPUï¼‰ä¸Šè®­ç»ƒçš„æ‰¹é‡å¤§å°ï¼ˆ`bs` æ˜¯äº‹å…ˆå®šä¹‰çš„å˜é‡ï¼‰
    per_device_eval_batch_size=bs*2,  # æ¯ä¸ªè®¾å¤‡ä¸Šè¯„ä¼°çš„æ‰¹é‡å¤§å°ï¼Œé€šå¸¸è¯„ä¼°æ—¶æ‰¹é‡å¯ä»¥ç¨å¤§ä¸€äº›
    num_train_epochs=epochs,  # è®­ç»ƒè½®æ•°ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¿›è¡Œçš„å®Œæ•´æ•°æ®é›†è¿­ä»£æ¬¡æ•°ï¼ˆ`epochs` æ˜¯äº‹å…ˆå®šä¹‰çš„å˜é‡ï¼‰
    weight_decay=0.01,        # æƒé‡è¡°å‡ï¼šL2 æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    report_to='none' # ç¦ç”¨æŠ¥å‘Šï¼ˆå¦‚ä¸æŠ¥å‘Šåˆ° TensorBoard æˆ– WandBï¼‰ï¼Œå¦‚æœéœ€è¦æŠ¥å‘Šï¼Œå¯ä»¥è®¾ç½®ä¸º 'tensorboard' æˆ– 'wandb'
)
model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) #å°†ç”¨çš„æ¨¡å‹
trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'], tokenizer=tokz,
                  compute_metrics=corr_d) #å°†modelã€è¶…å‚æ•°ä»¬ã€dataæ•´åˆåœ¨ä¸€èµ·çš„ç±»

trainer.train()
```

```python
#æµ‹è¯•é›†
# å®šä¹‰ä¸€ä¸ªæµ‹è¯•é›†çš„dataframeï¼Œæ„å»ºä¸€ä¸ªeval_dfï¼ŒåŸºäºè¿™å®ä¾‹åŒ–ä¸€ä¸ªeval_dså¹¶å°†å®ƒtokenization
eval_df = pd.read_csv(path/'test.csv')
eval_df.describe()
eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor
eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)

# é¢„æµ‹æµ‹è¯•é›†
preds = trainer.predict(eval_ds).predictions.astype(float)
preds = np.clip(preds, 0, 1) #å°†<0>1çš„æ•°å€¼è§„æ•´åˆ°0~1

#å°†ç»“æœå¯¼å‡ºåˆ°csv
import datasets
submission = datasets.Dataset.from_dict({
    'id': eval_ds['id'],
    'score': preds
})
submission.to_csv('submission.csv', index=False)
```

#### 4.7 è¶…å‚æ•°ï¼šæƒé‡è¡°å‡weight_decay

L2 æ­£åˆ™åŒ–é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ªä¸æ¨¡å‹æƒé‡çš„å¹³æ–¹å’Œæˆæ­£æ¯”çš„é¡¹æ¥å®ç°æƒ©ç½šã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæŸå¤±å‡½æ•° `L(w)`ï¼Œè¡¨ç¤ºæ¨¡å‹çš„æŸå¤±ï¼Œå…¶ä¸­ `w` æ˜¯æ¨¡å‹çš„æƒé‡å‚æ•°ï¼Œé‚£ä¹ˆåŠ å…¥ L2 æ­£åˆ™åŒ–åçš„æŸå¤±å‡½æ•° `L2(w)` å°±æ˜¯ï¼š
$$
L2(w)=L(w) + \lambda \sum_{i} w_i^2
$$
å…¶ä¸­ï¼š

- `L(w)` æ˜¯åŸå§‹çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€å‡æ–¹è¯¯å·®ç­‰ï¼‰ã€‚
- `w_i` æ˜¯æ¨¡å‹çš„ç¬¬ `i` ä¸ªæƒé‡ã€‚
- `Î»` æ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„è¶…å‚æ•°ï¼Œæ§åˆ¶ L2 æ­£åˆ™åŒ–çš„å½±å“ã€‚è¾ƒå¤§çš„ `Î»` ä¼šå¯¹æ¨¡å‹çš„è®­ç»ƒäº§ç”Ÿè¾ƒå¤§çš„å½±å“ã€‚

**ä½œç”¨**

- **é™åˆ¶æƒé‡çš„å¤§å°**ï¼šL2 æ­£åˆ™åŒ–é¼“åŠ±æ¨¡å‹çš„æƒé‡ `w` å°½å¯èƒ½å°ï¼Œé¿å…å‡ºç°è¿‡å¤§çš„æƒé‡å€¼ï¼Œè¿™æ ·å¯ä»¥å‡å°‘æ¨¡å‹çš„å¤æ‚åº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- **å¹³æ»‘æ¨¡å‹**ï¼šé€šè¿‡æŠ‘åˆ¶å¤§æƒé‡ï¼ŒL2 æ­£åˆ™åŒ–ä¿ƒä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´åŠ å¹³æ»‘çš„å‡½æ•°ï¼Œè€Œéè¿‡äºå¤æ‚çš„ã€è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„å‡½æ•°ã€‚
- **æ”¹è¿›æ³›åŒ–èƒ½åŠ›**ï¼šé€šè¿‡é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒL2 æ­£åˆ™åŒ–ä½¿å¾—æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ï¼ˆæµ‹è¯•é›†ï¼‰ä¸Šçš„è¡¨ç°æ›´åŠ ç¨³å®šã€‚

### 4: nlp (fastbook10)

#### 4.1 è‡ªç›‘ç£å­¦ä¹ 

ä½¿ç”¨åµŒå…¥åœ¨è‡ªå˜é‡ä¸­çš„æ ‡ç­¾æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸æ˜¯éœ€è¦å¤–éƒ¨æ ‡ç­¾ã€‚ä¾‹å¦‚ï¼Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚è‡ªç›‘ç£å­¦ä¹ ä¹Ÿå¯ä»¥ç”¨äºå…¶ä»–é¢†åŸŸï¼›ä¾‹å¦‚ï¼Œå‚è§[â€œè‡ªç›‘ç£å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰â€](https://oreil.ly/ECjfJ)ä»¥äº†è§£è§†è§‰åº”ç”¨ã€‚

è‡ªç›‘ç£å­¦ä¹ é€šå¸¸ä¸ç”¨äºç›´æ¥è®­ç»ƒçš„æ¨¡å‹ï¼Œè€Œæ˜¯ç”¨äºé¢„è®­ç»ƒç”¨äºè¿ç§»å­¦ä¹ çš„æ¨¡å‹ã€‚

* é€šç”¨è¯­è¨€æ¨¡å‹å¾®è°ƒï¼ˆULMFiTï¼‰æ–¹æ³•ï¼šæœ‰ä¸€ä¸ªåŸºäºç»´åŸºç™¾ç§‘è¯­æ–™åº“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨IMDbçš„è¯­æ–™åº“è¿›è¡Œå¾®è°ƒï¼Œå†è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»

![](D:\Git\a\Path-Records\img\dlcf_1001.png)

* åˆ†è¯æ–¹æ³•ï¼šåŸºäºå•è¯çš„ã€åŸºäºå­è¯çš„å’ŒåŸºäºå­—ç¬¦çš„

#### 4.2 Tokenization & Numericalization

ç”±åˆ†è¯è¿‡ç¨‹åˆ›å»ºçš„åˆ—è¡¨çš„ä¸€ä¸ªå…ƒç´ ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªå•è¯*word tokenization*ï¼Œä¸€ä¸ªå•è¯çš„ä¸€éƒ¨åˆ†ï¼ˆä¸€ä¸ª*å­è¯*ï¼‰*subword tokenization*ï¼Œæˆ–ä¸€ä¸ªå•ä¸ªå­—ç¬¦ã€‚

##### 4.2.1 Word Tokenization

```python
from fastai.text.all import *
path = untar_data(URLs.IMDB)
path.ls()
# è·å–pathä¸­æŒ‡å®šfoldersä¸­çš„files
files = get_text_files(path, folders=['train', 'test', 'unsup'])
txt = files[0].open().read()
txt[:100]
```

```python
# WordTokenizeræ˜¯ä¸€ä¸ªåˆ†è¯å™¨ç±»ï¼Œå®ä¾‹åŒ–WordTokenizerç±»
spacy = WordTokenizer()
# first()ä½œç”¨æ˜¯è¿”å›åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚è¿™é‡Œå®ƒå–çš„æ˜¯spacy([txt])å¤„ç†åè¿”å›çš„ç¬¬ä¸€ä¸ªDocå¯¹è±¡çš„tokenåˆ—è¡¨ã€‚åˆ†è¯å™¨æ¥å—æ–‡æ¡£é›†åˆï¼Œæ‰€ä»¥ç”¨[txt]
toks = first(spacy([txt]))
# æ˜¾ç¤º`collection`çš„å‰`n`ä¸ªé¡¹ç›®ï¼Œä»¥åŠå®Œæ•´çš„å¤§å°â€”â€”è¿™æ˜¯`L`é»˜è®¤ä½¿ç”¨çš„
print(coll_repr(toks,30))
```

```python
# é€šè¿‡Tokenizerç±»å¢åŠ é¢å¤–çš„åŠŸèƒ½
tkn = Tokenizer(spacy)
print(coll_repr(tkn(txt),30))
'''
(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]
'''
```

ğŸ‘†ä¸€äº›ç‰¹æ®Šæ ‡è®°ï¼š

| ä¸»è¦ç‰¹æ®Šæ ‡è®° | ä»£è¡¨                                                         |
| ------------ | ------------------------------------------------------------ |
| xxbos        | æŒ‡ç¤ºæ–‡æœ¬çš„å¼€å§‹ï¼Œæ„æ€æ˜¯â€œæµçš„å¼€å§‹â€ï¼‰ã€‚é€šè¿‡è¯†åˆ«è¿™ä¸ªå¼€å§‹æ ‡è®°ï¼Œæ¨¡å‹å°†èƒ½å¤Ÿå­¦ä¹ éœ€è¦â€œå¿˜è®°â€å…ˆå‰è¯´è¿‡çš„å†…å®¹ï¼Œä¸“æ³¨äºå³å°†å‡ºç°çš„å•è¯ã€‚ |
| xxmaj        | æŒ‡ç¤ºä¸‹ä¸€ä¸ªå•è¯ä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼ˆå› ä¸ºå‡å°vocabularyçš„ä½“é‡ï¼ŒèŠ‚çœè®¡ç®—å’Œå†…å­˜èµ„æºï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å­—æ¯è½¬æ¢ä¸ºå°å†™ï¼‰ |
| xxunk        | æŒ‡ç¤ºä¸‹ä¸€ä¸ªå•è¯æ˜¯æœªçŸ¥çš„                                       |

##### 4.2.2 Subword Tokenization

```python
# è¯»å–200ä¸ªfilesä¸­çš„å¥å­txts
txts = L(o.open().read() for o in files[:200])
# å°†å¥å­txtsæ‹†åˆ†æˆszä¸ªvocabularyï¼Œå¹¶tokenize txt
def subword(sz):
    sp = SubwordTokenizer(vocab_sz=sz)
    sp.setup(txts)
    return ' '.join(first(sp([txt]))[:40])
# åˆ†æˆ1000ä¸ªvocabulary
subword(1000)
'â– J ian g â– X ian â–us es â–the â–comp le x â–back st or y â–of â–L ing â–L ing â–and â–Ma o â–D a o b ing â–to â–st ud y â–Ma o \' s â–" c'
# åˆ†æˆ100ä¸ªvocabularyï¼Œä¸ºäº†èƒ½æ‹†åˆ†ï¼Œå¥½å¤šéƒ½æ‹†æˆå­—æ¯äº†
subword(100)
'â– J i a n g â– X i a n â– u s e s â–the â– c o m p l e x â– b a c k s t o r y â– o f â– L'
```

##### 4.2.3 Numericalization

```python
# å°†txtså‰200æ¡textéƒ½Word Tokenizationï¼ˆä¹Ÿå¯ä»¥subword tokenizationï¼Œæœ¬ä¾‹ç”¨äº†å‰è€…ï¼‰
toks200 = txts[:200].map(tkn)
toks200[0]
'''
è·å¾—äº†ä¸€ä¸ªåˆ†è¯åçš„åˆ—è¡¨ï¼Œåˆ—è¡¨é•¿åº¦ä¸º200
(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to'...]
'''

# ç±»æ¯”ä¸Šé¢çš„subword(),å› ä¸ºè¦æ‰‹åŠ¨å»ºç«‹ä¸ªvocabularyï¼Œè¿™ä¸ªå®ä¾‹åŒ–åä¹Ÿè¦setupä¸€ä¸‹
num = Numericalize()
num.setup(toks200) #åŸºäºåˆ†è¯åçš„ç»“æœè®¾ç½®æ•°å­—æ˜ å°„
nums = num(toks)[:20]
'''
TensorText([   0,    0, 1269,    9, 1270,    0,   14,    0,    0,   12,    0,
               0,   15, 1271,    0,   22,   24,    0,  795,   24])
'''
# å°†æ•°å­—åŒ–çš„å¥å­å†æ˜ å°„å›tokens
' '.join(num.vocab[o] for o in nums)
'''
'xxunk xxunk uses the complex xxunk of xxunk xxunk and xxunk xxunk to study xxunk \'s " xxunk revolution "'
'''
```

`Numericalize`çš„é»˜è®¤å€¼ä¸º`min_freq=3`å’Œ`max_vocab=60000`ã€‚`max_vocab=60000`å¯¼è‡´ fastai ç”¨ç‰¹æ®Šçš„*æœªçŸ¥å•è¯*æ ‡è®°`xxunk`æ›¿æ¢é™¤æœ€å¸¸è§çš„ 60,000 ä¸ªå•è¯ä¹‹å¤–çš„æ‰€æœ‰å•è¯ã€‚è¿™æœ‰åŠ©äºé¿å…è¿‡å¤§çš„åµŒå…¥çŸ©é˜µï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦å¹¶å ç”¨å¤ªå¤šå†…å­˜ï¼Œå¹¶ä¸”è¿˜å¯èƒ½æ„å‘³ç€æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥è®­ç»ƒç¨€æœ‰å•è¯çš„æœ‰ç”¨è¡¨ç¤ºã€‚ç„¶è€Œï¼Œé€šè¿‡è®¾ç½®`min_freq`æ¥å¤„ç†æœ€åä¸€ä¸ªé—®é¢˜æ›´å¥½ï¼›é»˜è®¤å€¼`min_freq=3`æ„å‘³ç€å‡ºç°å°‘äºä¸‰æ¬¡çš„ä»»ä½•å•è¯éƒ½å°†è¢«æ›¿æ¢ä¸º`xxunk`ã€‚

##### 4.2.4 å°†è¿™äº›txtæ”¾è¿›batchesé‡Œé¢ï¼Œå½¢æˆDataLoader

<img src="D:\Git\a\Path-Records\img\04-2-4.jpg" style="zoom:100%;" />

```python
# toks200æ˜¯200æ¡txtåˆ†è¯åçš„ï¼Œnum200å°±æ˜¯200æ¡åˆ†è¯å¥å­æ˜ å°„æˆæ•°å­—çš„
num200 = toks200.map(num)
# DataLoader
dl = LMDataLoader(num200)
x,y = first(dl)
x.shape, y.shape
'(torch.Size([64, 72]), torch.Size([64, 72]))ï¼Œå¯è§DataLoaderå°†streamæ‹†æˆäº†64ä¸ªmini-streamï¼Œæ¯ä¸ªmini-streamæœ‰72ä¸ªtokens'
# xå’Œyåªæ˜¯ç›¸å·®ä¸€ä¸ªtoken
' '.join(num.vocab[o] for o in x[0][:15])
'xxbos xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and'
' '.join(num.vocab[o] for o in y[0][:15])
'xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj'
```

#### 4.3 è®­ç»ƒæ–‡æœ¬åˆ†ç±»å™¨

* ä½¿ç”¨è¿ç§»å­¦ä¹ è®­ç»ƒæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ†ç±»å™¨æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¾®è°ƒåœ¨ Wikipedia ä¸Šé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä»¥é€‚åº” IMDb è¯„è®ºçš„è¯­æ–™åº“ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹æ¥è®­ç»ƒåˆ†ç±»å™¨ã€‚

##### 4.3.1 è¯­è¨€è¯†åˆ«-æ•°æ®åŠ è½½å™¨DataBlock

* **å®ä¾‹æ–¹æ³•**ï¼Œéœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç„¶åæ‰èƒ½è°ƒç”¨çš„æ–¹æ³•ï¼ŒMyClass.instance_method()ä¼šæŠ¥é”™ï¼›**ç±»æ–¹æ³•**å°±ä¸éœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç›´æ¥è°ƒç”¨MyClass.class_method()ä¸ä¼šæŠ¥é”™ï¼Œè€Œä¸”å¯ä»¥è®¿é—®ç±»å˜é‡ï¼›**é™æ€æ–¹æ³•**ä¹Ÿä¸éœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç›´æ¥è°ƒç”¨MyClass.static_method()ä¹Ÿä¸ä¼šæŠ¥é”™ï¼Œä½†æ²¡åŠæ³•è®¿é—®ç±»å˜é‡ã€‚

```python
from fastai.text.all import *
path = untar_data(URLs.IMDB)
# è¿™æ˜¯ä¸Šé¢å…¨éƒ¨æ‰‹åŠ¨ä»£ç çš„æ±‡æ€»--------------------------------------------------------------------------------
files = get_text_files(path, folders=['train', 'test', 'unsup'])
#txt = files[0].open().read()
spacy = WordTokenizer()
#toks = first(spacy([txt]))
tkn = Tokenizer(spacy)
# è¯»å–200ä¸ªfilesä¸­çš„å¥å­txts
txts = L(o.open().read() for o in files[:200])
# å°†å¥å­txtsæ‹†åˆ†æˆszä¸ªvocabularyï¼Œå¹¶tokenize txt
#def subword(sz):
#    sp = SubwordTokenizer(vocab_sz=sz)
#    sp.setup(txts)
#    return ' '.join(first(sp([txt]))[:40])
#tks = tkn(txt)
toks = txts.map(tkn)
num = Numericalize()
num.setup(toks)
#nums = num(toks)[:20]
num = toks.map(num)
dl = LMDataLoader(num) #æ²¡æ³•æŒ‡å®šbatch_size
x,y = first(dl)
# fastaiæœ‰ç°æˆçš„æ–¹æ³•---------------------------------------------------------------------------------------
get_imdb = partial(get_text_files, folders=['train','test','unsup'])
# è¯­è¨€æ¨¡å‹çš„æ•°æ®åŠ è½½å™¨
dls_lm = DataBlock(
    blocks = TextBlock.from_folder(path,is_lm=True),
    get_items = get_imdb,
    splitter = RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
dls_lm.show_batch(max_n=3)
```

|      | text                                                         | text_                                                        |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 0    | xxbos xxmaj it â€™s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard | xxmaj it â€™s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard xxunk |
| 1    | what xxmaj i â€˜ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this | xxmaj i â€˜ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this is |

##### 4.3.2 è¯­è¨€è¯†åˆ«-Fine-tune

* **Embedding**ï¼šåµŒå…¥æ˜¯æŠŠæ–‡å­—è½¬æ¢æˆè®¡ç®—æœºèƒ½ç†è§£çš„æ•°å­—ï¼Œè€Œä¸”è¿™ç§è½¬æ¢ä¸æ˜¯ç®€å•çš„ 1 å¯¹ 1 æ˜ å°„ï¼Œè€Œæ˜¯è®©è¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨æ•°å€¼ç©ºé—´é‡Œä¹Ÿé å¾—æ›´è¿‘ã€‚å¸¸è§çš„ NLP ä»»åŠ¡éƒ½ä¼šç”¨åˆ° Embeddingï¼Œæ¯”å¦‚ï¼š**Word2Vec**ï¼ˆGoogle å¼€å‘çš„è¯å‘é‡æ¨¡å‹ï¼‰ã€**GloVe**ï¼ˆæ–¯å¦ç¦å¼€å‘çš„è¯å‘é‡ï¼‰ã€**FastText**ï¼ˆFacebook å¼€å‘çš„è¯å‘é‡ï¼‰ã€**BERT / GPT**ï¼ˆç°ä»£ NLP æ¨¡å‹çš„åº•å±‚éƒ½ä¼šç”¨æ›´é«˜çº§çš„ Embeddingï¼‰ã€‚

```python
learn = language_model_learner(
    dls_lm,
    AWD_LSTM,
    drop_mult=0.3,      # Dropout ä¹˜æ•°ï¼ˆæ§åˆ¶æ­£åˆ™åŒ–çš„ç¨‹åº¦ï¼‰
    metrics=[accuracy, Perplexity()]     # è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ + å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
).to_fp16()      # å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦ï¼ˆFP16ï¼‰ï¼Œæå‡è®­ç»ƒé€Ÿåº¦
```

* **æŸå¤±å‡½æ•°**ï¼šäº¤å‰ç†µæŸå¤±
* **Perplexity** metricså¸¸ç”¨åœ¨NLPä¸­ï¼Œå®ƒæ˜¯æŸå¤±å‡½æ•°çš„æŒ‡æ•°ï¼ˆå³torch.exp(cross_entropy)ï¼‰

* **Dropout**æ˜¯ä¸€ç§é˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„æ–¹æ³•ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºâ€œä¸¢å¼ƒâ€ï¼ˆè®¾ä¸º 0ï¼‰ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦ä¾èµ–æŸäº›ç‰¹å®šçš„ç‰¹å¾ã€‚

###### â‘ **Dropout vs. Weight Decayï¼šåŒºåˆ«å¯¹æ¯”**

| ç‰¹æ€§        | Dropout                                              | Weight Decay (L2 æ­£åˆ™åŒ–)             |
| ---------------- | -------------------------------------------------------- | ---------------------------------------- |
| ä½œç”¨æ–¹å¼       | éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼Œè®©ç½‘ç»œå­¦ä¼šä¸åŒçš„ç‰¹å¾             | é™åˆ¶æƒé‡å¤§å°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ             |
| é€‚ç”¨èŒƒå›´    | æ›´é€‚ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå°¤å…¶æ˜¯ CNNã€RNNã€Transformerï¼‰ | é€‚ç”¨äºå‡ ä¹æ‰€æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹           |
| è®­ç»ƒä¸æµ‹è¯•   | åªåœ¨è®­ç»ƒæ—¶ç”Ÿæ•ˆï¼Œæµ‹è¯•æ—¶å…³é—­                           | è®­ç»ƒå’Œæµ‹è¯•æ—¶éƒ½ç”Ÿæ•ˆ                   |
| æ•°å­¦å…¬å¼     | è®©éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºè®¾ä¸º 0                                 | ç»™æŸå¤±å‡½æ•°å¢åŠ  Î»âˆ‘w^2 æƒ©ç½šé¡¹              |
| ç›´è§‚ç†è§£     | è®©ç¥ç»ç½‘ç»œå˜æˆä¸€ä¸ªå°å‹é›†æˆå­¦ä¹                        | å‡å°‘å¤§æƒé‡ï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ•°æ® |
| å¯¹è®¡ç®—çš„å½±å“ | å¢åŠ è®¡ç®—é‡ï¼Œå› ä¸ºæ¯æ¬¡è®­ç»ƒéƒ½è¦éšæœºä¸¢å¼ƒä¸åŒç¥ç»å…ƒ       | ä¸ä¼šå¢åŠ è®¡ç®—é‡                       |

###### â‘¡**ä»€ä¹ˆæ—¶å€™ç”¨ Dropoutï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨ Weight Decayï¼Ÿ**

è™½ç„¶å®ƒä»¬çš„å®ç°æ–¹å¼ä¸åŒï¼Œä½†ç›®çš„éƒ½æ˜¯ é˜²æ­¢æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

âœ” å¯ä»¥ä¸€èµ·ä½¿ç”¨ï¼

- Dropout ä¸»è¦ä½œç”¨åœ¨ ç½‘ç»œç»“æ„ å±‚é¢ï¼ˆä¸¢å¼ƒç¥ç»å…ƒï¼‰
- Weight Decay ä¸»è¦ä½œç”¨åœ¨ å‚æ•°ä¼˜åŒ– å±‚é¢ï¼ˆçº¦æŸæƒé‡å¤§å°ï¼‰
- ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸ ä¸¤è€…éƒ½ç”¨

| æƒ…å†µ                                   | æ›´é€‚åˆ Dropout    | æ›´é€‚åˆ Weight Decay |
| ------------------------------------------ | --------------------- | ----------------------- |
| æ•°æ®é‡å°ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ                   | âœ…                     | âœ…                       |
| ç¥ç»ç½‘ç»œå¾ˆæ·±ï¼ˆCNNã€LSTMã€Transformerï¼‰ | âœ…                     | âœ…                       |
| å‚æ•°é‡å°‘ï¼ˆå°å‹æ¨¡å‹ï¼Œå¦‚çº¿æ€§å›å½’ï¼‰       | âŒ                     | âœ…                       |
| è¿‡æ‹Ÿåˆä¸¥é‡                             | âœ…ï¼ˆæé«˜ Dropout ç‡ï¼‰  | âœ…ï¼ˆå¢å¤§ Weight Decayï¼‰  |
| æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆRNNï¼‰                    | âœ…ï¼ˆDropout ä¹Ÿèƒ½å¸®å¿™ï¼‰ | âŒ                       |

ğŸ’¡ ç»éªŒæ³•åˆ™ï¼š

- å¤§æ¨¡å‹ï¼ˆCNN/RNNï¼‰ â†’ ä¸¤è€…éƒ½ç”¨ï¼Œdropout=0.3~0.5 + wd=1e-4
- å°æ¨¡å‹ï¼ˆçº¿æ€§å›å½’ï¼‰ â†’ ä¸»è¦ç”¨ Weight Decay
- æ•°æ®é‡ç‰¹åˆ«å° â†’ Dropout å¯ä»¥å°‘ç”¨ï¼Œä½† Weight Decay ä»ç„¶æœ‰æ•ˆ

**è®­ç»ƒ**ï¼š

åƒ`vision_learner`ä¸€æ ·ï¼Œå½“ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆè¿™æ˜¯é»˜è®¤è®¾ç½®ï¼‰æ—¶ï¼Œ`language_model_learner`åœ¨ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨`freeze`ã€‚å› æ­¤è¿™å°†ä»…è®­ç»ƒåµŒå…¥å±‚ï¼Œå…¶ä»–éƒ¨åˆ†çš„æƒé‡æ˜¯è¢«å†»ç»“çš„ã€‚ä¹‹æ‰€ä»¥åªè®­ç»ƒåµŒå…¥å±‚ï¼Œæ˜¯å› ä¸ºåœ¨ IMDb è¯­æ–™åº“ä¸­ï¼Œå¯èƒ½ä¼šæœ‰ä¸€äº›è¯æ±‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯è¡¨ä¸­æ‰¾ä¸åˆ°ï¼Œè¿™äº›è¯çš„åµŒå…¥ï¼ˆembeddingsï¼‰éœ€è¦éšæœºåˆå§‹åŒ–ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œè€Œé¢„è®­ç»ƒæ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†å·²ç»æœ‰äº†è¾ƒå¥½çš„å‚æ•°ï¼Œå› æ­¤æš‚æ—¶ä¸ä¼šè¢«è°ƒæ•´ã€‚

fine_tuneä¸ä¼šä¿å­˜åŠæˆå“æ¨¡å‹ç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨äº†fit_one_cycle

```python
learn.fit_one_cycle(1, 2e-2)
```

###### â‘¢**fit vs. fit_one_cycle å¯¹æ¯”**

| å¯¹æ¯”é¡¹     | fit                      | fit_one_cycle               |
| ---------- | ------------------------ | --------------------------- |
| å­¦ä¹ ç‡è°ƒåº¦ | å›ºå®šå­¦ä¹ ç‡               | åŠ¨æ€è°ƒæ•´ï¼ˆwarm-up + decayï¼‰ |
| åŠ¨é‡è°ƒåº¦   | ä¸å˜                     | è‡ªé€‚åº”è°ƒæ•´                  |
| é€‚ç”¨åœºæ™¯   | å°è§„æ¨¡è®­ç»ƒã€ç®€å•ä»»åŠ¡     | æ·±åº¦å­¦ä¹ ã€å¤§è§„æ¨¡è®­ç»ƒ        |
| ä¼˜ç‚¹       | ç®€å•ã€ç¨³å®š               | æé«˜æ³›åŒ–èƒ½åŠ›ã€æ”¶æ•›æ›´å¿«      |
| ç¼ºç‚¹       | å¯èƒ½è®­ç»ƒæ…¢ï¼Œæ³›åŒ–èƒ½åŠ›ä¸ä½³ | éœ€è¦è°ƒæ•´å‚æ•°ï¼Œç¨å¤æ‚        |

##### 4.3.3 è¯­è¨€è¯†åˆ«-ä¿å­˜æ¨¡å‹

```python
# ä¿å­˜ç»å†1æ¬¡epochçš„æ¨¡å‹çŠ¶æ€
learn.save('1epoch')
# åŠ è½½æ¨¡å‹
learn = learn.load('1epoch')
# åˆå§‹è®­ç»ƒå®Œæˆï¼Œè§£å†»åç»§ç»­å¾®è°ƒæ¨¡å‹
learn.unfreeze()
learn.fit_one_cycle(10,2e-3)
# ä¿å­˜ç¼–ç å™¨encoderï¼šç¼–ç å™¨å°±æ˜¯æˆ‘ä»¬è®­ç»ƒçš„æ‰€æœ‰æ¨¡å‹ï¼Œé™¤äº†æœ€åä¸€å±‚ï¼ˆå®ƒå°†activationsè½¬æ¢æˆé¢„æµ‹tokençš„æ¦‚ç‡ï¼‰
learn.save_encoder('finetuned')
```

æ­¤æ—¶å·²ç»å®Œæˆç¬¬äºŒé˜¶æ®µï¼š

![](D:\Git\a\Path-Records\img\dlcf_1001.png)

```python
# Kaggleä¸­å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¸‹è½½ä¸‹æ¥
path1 = Path('/kaggle/working/')
learn.save(path1/'mymodel')  # ä¿å­˜æ¨¡å‹
learn.load(path1/'mymodel') #åŠ è½½æ¨¡å‹

path2 = Path('/kaggle/working/')
learn.save_encoder(path2/'finetuned')
learn.load_encoder(path2/'finetuned')
```

###### fastaiçš„å¿«æ·æ„é€ å™¨learn

ä¸åŒä»»åŠ¡æœ‰ä¸åŒçš„å¿«æ·æ„é€ å™¨ï¼š

| ä»»åŠ¡                 | å¿«æ·æ–¹æ³•                                            | åº•å±‚ç±»    |
| -------------------- | --------------------------------------------------- | --------- |
| è®¡ç®—æœºè§†è§‰           | `vision_learner`                                    | `Learner` |
| è‡ªç„¶è¯­è¨€å¤„ç†         | `language_model_learner`ã€`text_classifier_learner` | `Learner` |
| è¡¨æ ¼æ•°æ®             | `tabular_learner`                                   | `Learner` |
| ååŒè¿‡æ»¤ï¼ˆæ¨èç³»ç»Ÿï¼‰ | `collab_learner`                                    | `Learner` |

##### 4.3.4 æ–‡æœ¬ç”Ÿæˆï¼ˆä¸åœ¨é˜¶æ®µä¸­ï¼‰

```python
TEXT = "I liked this movie because"
N_WORDS = 40
N_SENTENCES = 2
# temperature=0.75ï¼šæ§åˆ¶éšæœºæ€§ã€‚>1æ›´éšæœºæ›´æœ‰åˆ›é€ åŠ›ä½†å¯èƒ½æ²¡æ„ä¹‰ï¼›<1æ›´ç¡®å®šæ›´åˆç†ä½†å¯èƒ½è¾ƒæ— èŠ
preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)
         for _ in range(N_SENTENCES)]
print("\n".join(preds))
```

##### 4.3.5 æ–‡æœ¬åˆ†ç±»-æ•°æ®åŠ è½½å™¨DataBlock

```python
# åˆ›å»ºæ•°æ®åŠ è½½å™¨
dls_clas = DataBlock(
    blocks=(TextBlock.from_folder(path,vocab=dls_lm.vocab),CategoryBlock),
    '''ä½¿ç”¨ dls_lm.vocab çš„è¯æ±‡è¡¨å¯¹æ–‡æœ¬è¿›è¡Œæ•°å€¼åŒ–ï¼Œä»¥ä¿æŒå’Œ dls_lm ç›¸åŒçš„è¯ç´¢å¼•ç¼–å·ï¼Œè¿™ä¸ªè¯æ±‡è¡¨ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å­—åºåˆ—ï¼Œ1 tokenâ†’1 integer
    é€šè¿‡ä¼ é€’ `is_lm=False`ï¼ˆæˆ–è€…æ ¹æœ¬ä¸ä¼ é€’ `is_lm`ï¼Œå› ä¸ºå®ƒé»˜è®¤ä¸º `False`ï¼‰ï¼Œæˆ‘ä»¬å‘Šè¯‰ `TextBlock` æˆ‘ä»¬æœ‰å¸¸è§„æ ‡è®°çš„æ•°æ®ï¼Œè€Œä¸æ˜¯å°†ä¸‹ä¸€ä¸ªæ ‡è®°ä½œä¸ºæ ‡ç­¾'''
    get_y=parent_label,
    get_items=partial(get_text_files,folders=['train','test']),
    splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path,path=path,bs=128,seq_len=72)
dls_clas.show_batch(max_n=4)
```

```python
# åªä¸ºäº†è¯´æ˜æ€ä¹ˆå¤„ç†åœ¨åˆ†ç±»ç®—æ³•ä¸­texté•¿çŸ­ä¸ä¸€çš„é—®é¢˜
files = get_text_files(path, folders=['train','test','unsup'])
txts = L(o.open().read() for o in files[:200])
spacy = WordTokenizer()
tkn = Tokenizer(spacy)
toks200 = txts.map(tkn)
num = Numericalize()
num.setup(toks200)
nums_samp = toks200[:10].map(num)
nums_samp.map(len)
'(#10) [158,319,181,193,114,145,260,146,252,295]ï¼šå¯è§Lç±»æ¯æ¡å¤§å°ä¸ç­‰'
```

ä½†æ˜¯ï¼ŒPyTorch çš„ DataLoaderéœ€è¦å°†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰é¡¹ç›®æ•´åˆåˆ°ä¸€ä¸ªå¼ é‡ä¸­ï¼Œè€Œä¸€ä¸ªå¼ é‡å…·æœ‰å›ºå®šçš„å½¢çŠ¶ï¼ˆå³ï¼Œæ¯ä¸ªè½´ä¸Šéƒ½æœ‰ç‰¹å®šçš„é•¿åº¦ï¼Œå¹¶ä¸”æ‰€æœ‰é¡¹ç›®å¿…é¡»ä¸€è‡´ï¼‰ï¼Œä¸ºäº†è®©å®ƒä»¬ç›¸ç­‰ï¼Œå°±å¾—å¡«å……ã€‚

* å¡«å……ï¼šæˆ‘ä»¬å°†æ‰©å±•æœ€çŸ­çš„æ–‡æœ¬ä»¥ä½¿å®ƒä»¬éƒ½å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„å¡«å……æ ‡è®°ï¼Œè¯¥æ ‡è®°å°†è¢«æˆ‘ä»¬çš„æ¨¡å‹å¿½ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…å†…å­˜é—®é¢˜å¹¶æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬å°†å¤§è‡´ç›¸åŒé•¿åº¦çš„æ–‡æœ¬æ‰¹é‡å¤„ç†åœ¨ä¸€èµ·ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªepochå‰ï¼ˆå¯¹äºè®­ç»ƒé›†ï¼‰æŒ‰é•¿åº¦å¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼Œåˆ†æˆå‡ ä¸ªbatchesï¼Œä½¿ç”¨æ¯ä¸ªbatchä¸­æœ€å¤§æ–‡æ¡£çš„å¤§å°ä½œä¸ºè¿™ä¸ªbatchçš„ç›®æ ‡å¤§å°ã€‚

* ä½¿ç”¨DataBlock+is_lm=Falseæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬æ“ä½œã€‚

###### DataBlock & DataLoaders

| **ç»„ä»¶**         | **ä½œç”¨**                            |
| ---------------- | ----------------------------------- |
| `DataBlock`      | åªæ˜¯ æ•°æ®å¤„ç†çš„æ¨¡æ¿ï¼Œä¸åŒ…å«æ•°æ®     |
| `.dataloaders()` | å°† `DataBlock` è½¬ä¸º `DataLoaders`   |
| `DataLoaders`    | çœŸæ­£çš„æ•°æ®åŠ è½½å™¨ï¼ŒåŒ…å«è®­ç»ƒ/éªŒè¯æ•°æ® |

###### DataLoadersçš„ä¸»è¦å˜ç§

| å˜ç§åç§°                  | é€‚ç”¨ä»»åŠ¡                                |
| ------------------------- | --------------------------------------- |
| `ImageDataLoaders`        | å›¾åƒåˆ†ç±»ï¼ˆè‡ªåŠ¨ä»æ–‡ä»¶å¤¹åŠ è½½å›¾åƒï¼‰        |
| `TextDataLoaders`         | è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼ˆè‡ªåŠ¨å¤„ç†æ–‡æœ¬æ•°æ®ï¼‰ |
| `TabularDataLoaders`      | è¡¨æ ¼æ•°æ®ï¼ˆç”¨äºç»“æ„åŒ–æ•°æ®ï¼Œå¦‚ CSVï¼‰      |
| `SegmentationDataLoaders` | å›¾åƒåˆ†å‰²ï¼ˆåƒç´ çº§åˆ†ç±»ä»»åŠ¡ï¼‰              |
| `DataLoaders`ï¼ˆåŸºç±»ï¼‰     | é€šç”¨æ•°æ®åŠ è½½å™¨ï¼Œé€‚ç”¨äºè‡ªå®šä¹‰æ•°æ®        |

##### 4.3.6 æ–‡æœ¬åˆ†ç±»-Fine-tune

```python
learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,metrics=accuracy).to_fp16()
learn = learn.load_encoder('/kaggle/input/finetuned/pytorch/default/1/finetuned')
```

* é€å±‚è§£å†»gradual unfreezingï¼šå†»ç»“æ¨¡å‹çš„éƒ¨åˆ†å±‚ï¼Œå…ˆè®­ç»ƒæ¨¡å‹çš„æœ€åä¸€å±‚ï¼Œç„¶åé€æ¸è§£å†»å‰é¢çš„å±‚ã€‚ä»æœ€åä¸€å±‚å¼€å§‹è®­ç»ƒï¼Œå› ä¸ºè¿™äº›å±‚å·²ç»åŒ…å«äº†å¤§éƒ¨åˆ†çš„ç‰¹å¾ä¿¡æ¯ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸è§£å†»å‰é¢çš„å±‚ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ã€‚
* é€å±‚è§£å†»çš„åŸå› ï¼šâ‘ é¢„è®­ç»ƒçš„ç‰¹å¾æå–å±‚-åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œé€šå¸¸å·²ç»å­¦åˆ°äº†é€šç”¨çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒè¿™äº›å±‚ï¼Œè€Œæ˜¯å†»ç»“å®ƒä»¬ï¼Œä¸“æ³¨äºè®­ç»ƒæ–°ä»»åŠ¡çš„æœ€åå‡ å±‚ã€‚â‘¡é¿å…è¿‡æ‹Ÿåˆ-å¦‚æœä¸€æ¬¡æ€§è§£å†»æ‰€æœ‰å±‚ï¼Œæ¨¡å‹å¯èƒ½ä¼šå¿«é€Ÿè¿‡æ‹Ÿåˆï¼Œå› ä¸ºè¾ƒå°çš„æ•°æ®é›†æ— æ³•æ”¯æŒæ‰€æœ‰å±‚çš„è®­ç»ƒã€‚é€å±‚è§£å†»æœ‰åŠ©äºé€æ¸é€‚åº”æ–°ä»»åŠ¡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆçš„é£é™©ã€‚â‘¢è®­ç»ƒæ•ˆç‡-å†»ç»“å‰é¢å‡ å±‚åï¼Œè®¡ç®—é‡å‡å°‘ï¼Œè®­ç»ƒé€Ÿåº¦åŠ å¿«ã€‚

```python
learn.fit_one_cycle(1,2e-2) #é»˜è®¤æ¨¡å‹ä¼šå†»ç»“ï¼Œå› æ­¤åªæ˜¯è®­ç»ƒæœ€åä¸€å±‚
# è®­ç»ƒå€’æ•°2å±‚
learn.freeze_to(-2)
learn.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2))
# è®­ç»ƒå€’æ•°3å±‚
learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))
# è®­ç»ƒæ‰€æœ‰å±‚
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
```

