**<font size=8>fast.ai</font>**

## [Practical Deep Learning 2022](https://course.fast.ai/) & [Fastbook](https://github.com/fastai/fastbook)
### 4: Natural Language (NLP) (PDL2022)

#### 4.1 å‘å±•

ï¼ˆ1ï¼‰**ULMFit** (ç”¨çš„RNN): Wikitext(103)Language Model (30%å‡†ç¡®ç‡) â†’ IMDb Language Model â†’ IMDb Classifier

ï¼ˆ2ï¼‰**Transformers** (æ©ç è¯­è¨€å»ºæ¨¡): ç”¨äºæœºå™¨é˜…è¯»ç†è§£ã€å¥å­åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦ç­‰

ï¼ˆ3ï¼‰...

å¯ä»¥çœ‹åˆ°ä¼¼ä¹Transformerè¦æ¯”ULMFité«˜çº§ï¼Œå®é™…ä¸Šä¸¤è€…çš„ç”¨é€”ä¸åŒï¼›å¦å¤–ï¼ŒULMFitèƒ½å¤Ÿé˜…è¯»æ›´é•¿çš„å¥å­ï¼Œå¦‚æœä¸€ä¸ªdocumentåŒ…å«è¶…è¿‡2000ä¸ªå•è¯ï¼Œé‚£ä¹ˆå°±æ›´æ¨èä½¿ç”¨ULMFitè¿›è¡Œåˆ†ç±»ã€‚

#### 4.2 æœ€é‡è¦çš„package

1/ pandas

2/ numpy

3/ matplotlib

4/ pytorch

##### å‚è€ƒä¹¦ï¼š[Python for Data Analysis, 3E About the Open Edition]([Python for Data Analysis, 3E](https://wesmckinney.com/book/))

#### 4.3 Tokenization

A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:

- *Tokenization*: Split each text up into words (or actually, as we'll see, into *tokens*)
- *Numericalization*: Convert each word (or token) into a number.

The details about how this is done actually depend on the particular model we use. So first we'll need to pick a model. There are thousands of models available, but a reasonable starting point for nearly any NLP problem is to use thisï¼š

`microsoft/deberta-v3-small`

```python
tokz.tokenize("A platypus is an ornithorhynchus anatinus.")
'''
['â–A',
 'â–platypus',
 'â–is',
 'â–an',
 'â–or',
 'ni',
 'tho',
 'rhynch',
 'us',
 'â–an',
 'at',
 'inus',
 '.']
'''
```

#### 4.4 è®­ç»ƒé›†ä¸éªŒè¯é›†çš„åˆ’åˆ†

Training set & Validation set

In practice, a random split like we've used here might not be a good idea -- here's what Dr Rachel Thomas has to say about it:

> "*One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a `train_test_split` method, this method takes a random subset of the data, which is a poor choice for many real-world problems.*"

I strongly recommend reading her article [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/) to more fully understand this critical topic.

* éšæœºåˆ’åˆ†æ•°æ®é›†ä¸é€‚ç”¨çš„æƒ…å†µï¼šæ—¶é—´åºåˆ—ã€æ–°äººè„¸ã€æ–°çš„èˆ¹ç­‰ç­‰ï¼›
* cross-validationæ¯”è¾ƒå±é™©ï¼Œé™¤éç”¨åˆ°çš„caseæ˜¯é‚£ç§å¯ä»¥éšæœºæ´—ç‰Œçš„æƒ…å†µï¼ˆéšæœºåˆ†ABCä¸‰ç»„æ•°æ®é›†ï¼ŒABåˆå¹¶åšè®­ç»ƒé›†-CåšéªŒè¯é›†ï¼Œä¸‰ç»„å¾ªç¯ï¼Œæœ€åæ±‚å¹³å‡å€¼ä½œä¸ºæ¨¡å‹çš„performanceï¼‰ï¼›
* æ‰€ä»¥ç”¨ä¸€ä¸ªtest setæµ‹è¯•é›†å»æœ€ç»ˆç¡®è®¤ä¸€ä¸‹æ¨¡å‹çš„å¥½åä¹Ÿè›®é‡è¦çš„ã€‚

#### 4.5 Metrics

In real life, outside of Kaggle, things not easy... As my partner Dr Rachel Thomas notes in [The problem with metrics is a big problem for AI](https://www.fast.ai/2019/09/24/metrics/):

> At their heart, what most current AI approaches do is to optimize metrics. The practice of optimizing metrics is not new nor unique to AI, yet AI can be particularly efficient (even too efficient!) at doing so. This is important to understand, because any risks of optimizing metrics are heightened by AI. While metrics can be useful in their proper place, there are harms when they are unthinkingly applied. Some of the scariest instances of algorithms run amok all result from over-emphasizing metrics. We have to understand this dynamic in order to understand the urgent risks we are facing due to misuse of AI.

* Metricså¾ˆå¤šæ—¶å€™å¹¶ä¸æ˜¯*æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„äº‹æƒ…*ï¼Œå®ƒåªæ˜¯*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*çš„ä¸€ä¸ªä»£ç†ï¼Œå¦‚æˆ‘ä»¬å…³å¿ƒè€å¸ˆçš„æ•™å­¦æ•ˆæœï¼Œmetricsæ˜¯å­¦ç”Ÿçš„åˆ†æ•°ï¼›
* Metricsä¼šè¢«æ•…æ„åœ°ã€ä½œå¼Šåœ°æ‹‰é«˜ï¼Œä½¿å®ƒå¤±å»äº†è¡¡é‡*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*çš„èƒ½åŠ›ï¼Œå¦‚è€å¸ˆäººä¸ºæ‹‰é«˜å­¦ç”Ÿçš„åˆ†æ•°ï¼Œå®ƒä¸å†èƒ½åæ˜ è€å¸ˆçš„æ•™å­¦æ•ˆæœï¼›
* Metricsä¼šæ›´çŸ­è§†ï¼Œæ¯”å¦‚é“¶è¡Œä¸€æ—¦å°†cross-sellingè¿™ä¸ªmetricsä½œä¸ºç›®æ ‡ï¼Œå°±ä¼šå‚¬ç”Ÿå‡ºå„ç§è™šå‡å¼€æˆ·ï¼Œè€Œå®é™…ä¸Šé“¶è¡Œçš„ç›®æ ‡æ˜¯ç»´æŠ¤è‰¯å¥½çš„å®¢æˆ·å…³ç³»ï¼Œåè€…æ‰æ˜¯é•¿è¿œçš„æˆ˜ç•¥ï¼Œæ¯”å¦‚*æˆ‘ä»¬å…³å¿ƒçš„äº‹æƒ…*æ˜¯æé«˜è§†é¢‘å½±å“åŠ›ï¼Œç”¨äº†ç‚¹å‡»ç‡ä½œä¸ºMetricsï¼Œå°±æ²¡æœ‰è€ƒè™‘åˆ°ä¸€äº›è§†é¢‘é•¿æœŸæ¥çœ‹å¯¹è¯»è€…çš„å¸®åŠ©å’Œå¡‘é€ ï¼›
* å¾ˆå¤šMetricsæ˜¯åœ¨ä¸€ä¸ªé«˜åº¦æˆç˜¾çš„ç¯å¢ƒæ”¶é›†æ•°æ®çš„ï¼Œæ¯”å¦‚æ•°æ®æ”¶é›†åˆ°å°æœ‹å‹å–œæ¬¢åƒç”œé£Ÿï¼Œç®—æ³•ä¼šè®©é£Ÿç‰©è¶Šæ¥è¶Šç”œï¼Œæ°¸è¿œä¸å¯èƒ½outputå‡ºæœ‰è¥å…»çš„é£Ÿç‰©ï¼›
* å°½ç®¡å¦‚æ­¤Metricsä¾ç„¶å¾ˆæœ‰ç”¨ï¼Œéœ€è¦è€ƒè™‘å¤šä¸ªmetricsæ¥é¿å…ä¸Šè¿°é—®é¢˜ï¼Œä½†æœ€ç»ˆæˆ‘ä»¬è¦åŠªåŠ›å°†å®ƒä»¬æ•´åˆï¼›
* Metricsé€šè¿‡å®šé‡æ–¹å¼è¡¡é‡ç»“æœï¼Œä½†æˆ‘ä»¬ä¾ç„¶éœ€è¦å®šæ€§çš„ä¿¡æ¯æ‰èƒ½è·å¾—å¥½çš„metricsï¼›
* å»è¯¢é—®å·²åœ¨æ­¤å±±ä¸­çš„äººæ°¸è¿œå¯ä»¥foreseeä¸€äº›ä¸è‰¯åæœï¼Œå¦‚è€å¸ˆå¯ä»¥å¾ˆå®¹æ˜“åœ°çŸ¥é“ï¼Œç”¨å­¦ç”Ÿåˆ†æ•°ä½œä¸ºå”¯ä¸€è¡¡é‡æ ‡å‡†ä¼šå¯¼è‡´ä»€ä¹ˆç³Ÿç³•çš„ç»“æœã€‚

#### 4.6 codes

```python
# æ£€æŸ¥æ˜¯å¦ä¸ºkaggleç¯å¢ƒ
import os
iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')
iskaggle

# ä¸‹è½½datasets
from pathlib import Path
if iskaggle:
    path = Path('../input/us-patent-phrase-to-phrase-matching')
    ! pip install -q datasets #åªéœ€è¦è·‘ä¸€æ¬¡å°±åˆ°äº†é¡µé¢ä¸­äº†
    
# ï¼è¡¨ç¤ºåé¢çš„ä¸æ˜¯pythonå‘½ä»¤ï¼Œæ˜¯shellå‘½ä»¤ï¼Œä½†æ˜¯å¦‚æœæƒ³åˆ©ç”¨åˆ°pythonä¸­çš„å‚æ•°ï¼Œå°±ç”¨{}æ¡†èµ·æ¥
!ls {path}
```

```python
#æ•°æ®çš„é¢„å¤„ç†å·¥ä½œ
# å®šä¹‰ä¸€ä¸ªè®­ç»ƒé›†çš„dataframe
import pandas as pd
df = pd.read_csv(path/'train.csv')
df.describe(include='object') #ä¸€ä¸ªéå¸¸é‡è¦çš„dataframeæ–¹æ³•

# æ„å»ºä¸€ä¸ªdf
df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor
df.input.head()
'''
0    TEXT1: A47; TEXT2: abatement of pollution; ANC...
1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...
2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...
3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...
4    TEXT1: A47; TEXT2: forest region; ANC1: abatement
Name: input, dtype: object
'''
df['input'][0], df.input[0]
''TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement''
            
# å®ä¾‹åŒ–ä¸€ä¸ªè®­ç»ƒé›†çš„Dataset
from datasets import Dataset,DatasetDict
ds = Dataset.from_pandas(df)
'''
Dataset({
    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],
    num_rows: 36473
})
å…¶ä¸­inputæ˜¯æ•´ä¸ªstringå¥å­
'''

# é€‰æ‹©ä¸€ä¸ªmodel,å°±æœ‰äº†å’Œè¿™ä¸ªmodelå¯¹åº”çš„vocabularyï¼Œç„¶åå®ä¾‹åŒ–å®ƒçš„tokzå·¥å…·
model_nm = 'microsoft/deberta-v3-small'
from transformers import AutoModelForSequenceClassification,AutoTokenizer
tokz = AutoTokenizer.from_pretrained(model_nm)

# å®šä¹‰ä¸€ä¸ªç”¨æ¥tokzæŸæ®µæ–‡å­—çš„å‡½æ•°ï¼Œå¹¶åº”ç”¨
def tok_func(x): return tokz(x["input"])
toknum_ds = ds.map(tok_func, batched=True) #æ¥è‡ªHuggingFaceçš„datasetsåº“,è¿™ä½¿å¾—å®ƒä¸å†åªæœ‰'input'è¿˜æœ‰'input_ids'
'''
.map(tok_func, batched=True)ï¼šå¯¹ ds ä¸­çš„æ•°æ®åº”ç”¨ tok_func è¿›è¡Œæ˜ å°„ï¼ˆmapï¼‰ï¼Œå¹¶ä¸”å¯ç”¨æ‰¹å¤„ç†ï¼ˆbatched=Trueï¼‰ï¼Œä»¥æé«˜æ•ˆç‡ã€‚
tok_dsï¼šè¿”å›ä¸€ä¸ªæ–°çš„ Datasetï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬å·²ç»è¢« tok_func å¤„ç†è¿‡ï¼ˆé€šå¸¸æ˜¯ Tokenizerï¼‰
'''
#row = toknum_ds[0]
#row['input'], row['input_ids'] 'â‘ ç¬¬ä¸€å¥è¯stringï¼Œâ‘¡ä¸€ä¸²æ•°å­—'
#tokz.vocab['â–of'] #è¿™é‡Œæœ‰ä¸€ä¸ªvocabularyï¼Œtockenâ†’æ•°å­—
#'265'

# å‡†å¤‡ä¸€ä¸ªlablesï¼Œtransformerä¸€å‘éƒ½è®¤ä¸ºlabelså°±è¯´æœ‰ä¸€åˆ—å«åšlabelsï¼Œæ‰€ä»¥å¾—æ”¹åå­—
toknum_ds = toknum_ds.rename_columns({'score':'labels'})

# æŠŠä¹‹å‰å·²ç»æ•°å­—åŒ–äº†çš„Datasetåˆ†ä¸€ä¸‹ï¼Œåˆ†æˆä¸€ä¸ªè®­ç»ƒé›†ä¸€ä¸ªéªŒè¯é›†
dds = toknum_ds.train_test_split(0.25, seed=42)

# å®šä¹‰Metrics
import numpy as np
def corr(x,y): return np.corrcoef(x,y)[0][1]
def corr_d(eval_pred): return {'pearson': corr(*eval_pred)} #eval_predé€šå¸¸æ˜¯åŒ…å«é¢„æµ‹å€¼å’Œå®é™…å€¼çš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œ*è¡¨ç¤ºå°†é¢„æµ‹å€¼å’Œå®é™…å€¼æ‹†è§£æˆä½ç½®å‚æ•°ä¼ é€’ç»™corr() [å¦å¤–ï¼Œè¿™ä¸ª'pearson'æ ‡ç­¾æœ€åä¼šå‡ºç°åœ¨è®­ç»ƒç»“æœé‡Œ]
```

```python
#è®­ç»ƒ
from transformers import TrainingArguments,Trainer
bs = 128
epochs = 4
lr = 8e-5
args = TrainingArguments(
    'outputs',                # è¾“å‡ºç›®å½•ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹å’Œæ—¥å¿—å°†ä¿å­˜åœ¨æ­¤ç›®å½•ä¸­
    learning_rate=lr,         # å­¦ä¹ ç‡ï¼šæ¨¡å‹ä¼˜åŒ–çš„æ­¥é•¿ï¼ˆlr æ˜¯äº‹å…ˆå®šä¹‰å¥½çš„å˜é‡ï¼‰
    warmup_ratio=0.1,         # é¢„çƒ­æ¯”ä¾‹ï¼šå­¦ä¹ ç‡ä» 0 çº¿æ€§å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡çš„æ¯”ä¾‹ï¼ˆ0.1 è¡¨ç¤º 10% çš„è®­ç»ƒæ­¥æ•°ä½œä¸ºé¢„çƒ­é˜¶æ®µï¼‰
    lr_scheduler_type='cosine',  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼šä½¿ç”¨ä½™å¼¦é€€ç«ï¼ˆCosine Annealingï¼‰ç­–ç•¥æ¥é€æ­¥é™ä½å­¦ä¹ ç‡
    fp16=True,                # æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦è®­ç»ƒï¼šå°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æµ®ç‚¹æ•°ç²¾åº¦é™ä¸º 16 ä½ï¼Œå¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜å ç”¨
    evaluation_strategy="epoch",  # è¯„ä¼°ç­–ç•¥ï¼šæ¯ä¸ªè®­ç»ƒè½®ï¼ˆepochï¼‰ç»“æŸåè¿›è¡Œè¯„ä¼°
    per_device_train_batch_size=bs,  # æ¯ä¸ªè®¾å¤‡ï¼ˆGPUï¼‰ä¸Šè®­ç»ƒçš„æ‰¹é‡å¤§å°ï¼ˆ`bs` æ˜¯äº‹å…ˆå®šä¹‰çš„å˜é‡ï¼‰
    per_device_eval_batch_size=bs*2,  # æ¯ä¸ªè®¾å¤‡ä¸Šè¯„ä¼°çš„æ‰¹é‡å¤§å°ï¼Œé€šå¸¸è¯„ä¼°æ—¶æ‰¹é‡å¯ä»¥ç¨å¤§ä¸€äº›
    num_train_epochs=epochs,  # è®­ç»ƒè½®æ•°ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¿›è¡Œçš„å®Œæ•´æ•°æ®é›†è¿­ä»£æ¬¡æ•°ï¼ˆ`epochs` æ˜¯äº‹å…ˆå®šä¹‰çš„å˜é‡ï¼‰
    weight_decay=0.01,        # æƒé‡è¡°å‡ï¼šL2 æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    report_to='none' # ç¦ç”¨æŠ¥å‘Šï¼ˆå¦‚ä¸æŠ¥å‘Šåˆ° TensorBoard æˆ– WandBï¼‰ï¼Œå¦‚æœéœ€è¦æŠ¥å‘Šï¼Œå¯ä»¥è®¾ç½®ä¸º 'tensorboard' æˆ– 'wandb'
)
model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) #å°†ç”¨çš„æ¨¡å‹
trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'], tokenizer=tokz,
                  compute_metrics=corr_d) #å°†modelã€è¶…å‚æ•°ä»¬ã€dataæ•´åˆåœ¨ä¸€èµ·çš„ç±»

trainer.train()
```

```python
#æµ‹è¯•é›†
# å®šä¹‰ä¸€ä¸ªæµ‹è¯•é›†çš„dataframeï¼Œæ„å»ºä¸€ä¸ªeval_dfï¼ŒåŸºäºè¿™å®ä¾‹åŒ–ä¸€ä¸ªeval_dså¹¶å°†å®ƒtokenization
eval_df = pd.read_csv(path/'test.csv')
eval_df.describe()
eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor
eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)

# é¢„æµ‹æµ‹è¯•é›†
preds = trainer.predict(eval_ds).predictions.astype(float)
preds = np.clip(preds, 0, 1) #å°†<0>1çš„æ•°å€¼è§„æ•´åˆ°0~1

#å°†ç»“æœå¯¼å‡ºåˆ°csv
import datasets
submission = datasets.Dataset.from_dict({
    'id': eval_ds['id'],
    'score': preds
})
submission.to_csv('submission.csv', index=False)
```

#### 4.7 è¶…å‚æ•°ï¼šæƒé‡è¡°å‡weight_decay

L2 æ­£åˆ™åŒ–é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ªä¸æ¨¡å‹æƒé‡çš„å¹³æ–¹å’Œæˆæ­£æ¯”çš„é¡¹æ¥å®ç°æƒ©ç½šã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæŸå¤±å‡½æ•° `L(w)`ï¼Œè¡¨ç¤ºæ¨¡å‹çš„æŸå¤±ï¼Œå…¶ä¸­ `w` æ˜¯æ¨¡å‹çš„æƒé‡å‚æ•°ï¼Œé‚£ä¹ˆåŠ å…¥ L2 æ­£åˆ™åŒ–åçš„æŸå¤±å‡½æ•° `L2(w)` å°±æ˜¯ï¼š
$$
L2(w)=L(w) + \lambda \sum_{i} w_i^2
$$
å…¶ä¸­ï¼š

- `L(w)` æ˜¯åŸå§‹çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€å‡æ–¹è¯¯å·®ç­‰ï¼‰ã€‚
- `w_i` æ˜¯æ¨¡å‹çš„ç¬¬ `i` ä¸ªæƒé‡ã€‚
- `Î»` æ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„è¶…å‚æ•°ï¼Œæ§åˆ¶ L2 æ­£åˆ™åŒ–çš„å½±å“ã€‚è¾ƒå¤§çš„ `Î»` ä¼šå¯¹æ¨¡å‹çš„è®­ç»ƒäº§ç”Ÿè¾ƒå¤§çš„å½±å“ã€‚

**ä½œç”¨**

- **é™åˆ¶æƒé‡çš„å¤§å°**ï¼šL2 æ­£åˆ™åŒ–é¼“åŠ±æ¨¡å‹çš„æƒé‡ `w` å°½å¯èƒ½å°ï¼Œé¿å…å‡ºç°è¿‡å¤§çš„æƒé‡å€¼ï¼Œè¿™æ ·å¯ä»¥å‡å°‘æ¨¡å‹çš„å¤æ‚åº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- **å¹³æ»‘æ¨¡å‹**ï¼šé€šè¿‡æŠ‘åˆ¶å¤§æƒé‡ï¼ŒL2 æ­£åˆ™åŒ–ä¿ƒä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´åŠ å¹³æ»‘çš„å‡½æ•°ï¼Œè€Œéè¿‡äºå¤æ‚çš„ã€è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„å‡½æ•°ã€‚
- **æ”¹è¿›æ³›åŒ–èƒ½åŠ›**ï¼šé€šè¿‡é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒL2 æ­£åˆ™åŒ–ä½¿å¾—æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ï¼ˆæµ‹è¯•é›†ï¼‰ä¸Šçš„è¡¨ç°æ›´åŠ ç¨³å®šã€‚

### 4: Chapter 10: nlp (fastbook10)

#### 4.1 è‡ªç›‘ç£å­¦ä¹ 

ä½¿ç”¨åµŒå…¥åœ¨è‡ªå˜é‡ä¸­çš„æ ‡ç­¾æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸æ˜¯éœ€è¦å¤–éƒ¨æ ‡ç­¾ã€‚ä¾‹å¦‚ï¼Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚è‡ªç›‘ç£å­¦ä¹ ä¹Ÿå¯ä»¥ç”¨äºå…¶ä»–é¢†åŸŸï¼›ä¾‹å¦‚ï¼Œå‚è§[â€œè‡ªç›‘ç£å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰â€](https://oreil.ly/ECjfJ)ä»¥äº†è§£è§†è§‰åº”ç”¨ã€‚

è‡ªç›‘ç£å­¦ä¹ é€šå¸¸ä¸ç”¨äºç›´æ¥è®­ç»ƒçš„æ¨¡å‹ï¼Œè€Œæ˜¯ç”¨äºé¢„è®­ç»ƒç”¨äºè¿ç§»å­¦ä¹ çš„æ¨¡å‹ã€‚

* é€šç”¨è¯­è¨€æ¨¡å‹å¾®è°ƒï¼ˆULMFiTï¼‰æ–¹æ³•ï¼šæœ‰ä¸€ä¸ªåŸºäºç»´åŸºç™¾ç§‘è¯­æ–™åº“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨IMDbçš„è¯­æ–™åº“è¿›è¡Œå¾®è°ƒï¼Œå†è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»

![](D:\Git\a\Path-Records\img\dlcf_1001.png)

* åˆ†è¯æ–¹æ³•ï¼šåŸºäºå•è¯çš„ã€åŸºäºå­è¯çš„å’ŒåŸºäºå­—ç¬¦çš„

#### 4.2 Tokenization & Numericalization

ç”±åˆ†è¯è¿‡ç¨‹åˆ›å»ºçš„åˆ—è¡¨çš„ä¸€ä¸ªå…ƒç´ ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªå•è¯*word tokenization*ï¼Œä¸€ä¸ªå•è¯çš„ä¸€éƒ¨åˆ†ï¼ˆä¸€ä¸ª*å­è¯*ï¼‰*subword tokenization*ï¼Œæˆ–ä¸€ä¸ªå•ä¸ªå­—ç¬¦ã€‚

##### 4.2.1 Word Tokenization

```python
from fastai.text.all import *
path = untar_data(URLs.IMDB)
path.ls()
# è·å–pathä¸­æŒ‡å®šfoldersä¸­çš„files
files = get_text_files(path, folders=['train', 'test', 'unsup'])
txt = files[0].open().read()
txt[:100]
```

```python
# WordTokenizeræ˜¯ä¸€ä¸ªåˆ†è¯å™¨ç±»ï¼Œå®ä¾‹åŒ–WordTokenizerç±»
spacy = WordTokenizer()
# first()ä½œç”¨æ˜¯è¿”å›åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚è¿™é‡Œå®ƒå–çš„æ˜¯spacy([txt])å¤„ç†åè¿”å›çš„ç¬¬ä¸€ä¸ªDocå¯¹è±¡çš„tokenåˆ—è¡¨ã€‚åˆ†è¯å™¨æ¥å—æ–‡æ¡£é›†åˆï¼Œæ‰€ä»¥ç”¨[txt]
toks = first(spacy([txt]))
# æ˜¾ç¤º`collection`çš„å‰`n`ä¸ªé¡¹ç›®ï¼Œä»¥åŠå®Œæ•´çš„å¤§å°â€”â€”è¿™æ˜¯`L`é»˜è®¤ä½¿ç”¨çš„
print(coll_repr(toks,30))
```

```python
# é€šè¿‡Tokenizerç±»å¢åŠ é¢å¤–çš„åŠŸèƒ½
tkn = Tokenizer(spacy)
print(coll_repr(tkn(txt),30))
'''
(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]
'''
```

ğŸ‘†ä¸€äº›ç‰¹æ®Šæ ‡è®°ï¼š

| ä¸»è¦ç‰¹æ®Šæ ‡è®° | ä»£è¡¨                                                         |
| ------------ | ------------------------------------------------------------ |
| xxbos        | æŒ‡ç¤ºæ–‡æœ¬çš„å¼€å§‹ï¼Œæ„æ€æ˜¯â€œæµçš„å¼€å§‹â€ï¼‰ã€‚é€šè¿‡è¯†åˆ«è¿™ä¸ªå¼€å§‹æ ‡è®°ï¼Œæ¨¡å‹å°†èƒ½å¤Ÿå­¦ä¹ éœ€è¦â€œå¿˜è®°â€å…ˆå‰è¯´è¿‡çš„å†…å®¹ï¼Œä¸“æ³¨äºå³å°†å‡ºç°çš„å•è¯ã€‚ |
| xxmaj        | æŒ‡ç¤ºä¸‹ä¸€ä¸ªå•è¯ä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼ˆå› ä¸ºå‡å°vocabularyçš„ä½“é‡ï¼ŒèŠ‚çœè®¡ç®—å’Œå†…å­˜èµ„æºï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å­—æ¯è½¬æ¢ä¸ºå°å†™ï¼‰ |
| xxunk        | æŒ‡ç¤ºä¸‹ä¸€ä¸ªå•è¯æ˜¯æœªçŸ¥çš„                                       |

##### 4.2.2 Subword Tokenization

```python
# è¯»å–200ä¸ªfilesä¸­çš„å¥å­txts
txts = L(o.open().read() for o in files[:200])
# å°†å¥å­txtsæ‹†åˆ†æˆszä¸ªvocabularyï¼Œå¹¶tokenize txt
def subword(sz):
    sp = SubwordTokenizer(vocab_sz=sz)
    sp.setup(txts)
    return ' '.join(first(sp([txt]))[:40])
# åˆ†æˆ1000ä¸ªvocabulary
subword(1000)
'â– J ian g â– X ian â–us es â–the â–comp le x â–back st or y â–of â–L ing â–L ing â–and â–Ma o â–D a o b ing â–to â–st ud y â–Ma o \' s â–" c'
# åˆ†æˆ100ä¸ªvocabularyï¼Œä¸ºäº†èƒ½æ‹†åˆ†ï¼Œå¥½å¤šéƒ½æ‹†æˆå­—æ¯äº†
subword(100)
'â– J i a n g â– X i a n â– u s e s â–the â– c o m p l e x â– b a c k s t o r y â– o f â– L'
```

##### 4.2.3 Numericalization

```python
# å°†txtså‰200æ¡textéƒ½Word Tokenizationï¼ˆä¹Ÿå¯ä»¥subword tokenizationï¼Œæœ¬ä¾‹ç”¨äº†å‰è€…ï¼‰
toks200 = txts[:200].map(tkn)
toks200[0]
'''
è·å¾—äº†ä¸€ä¸ªåˆ†è¯åçš„åˆ—è¡¨ï¼Œåˆ—è¡¨é•¿åº¦ä¸º200
(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to'...]
'''

# ç±»æ¯”ä¸Šé¢çš„subword(),å› ä¸ºè¦æ‰‹åŠ¨å»ºç«‹ä¸ªvocabularyï¼Œè¿™ä¸ªå®ä¾‹åŒ–åä¹Ÿè¦setupä¸€ä¸‹
num = Numericalize()
num.setup(toks200) #åŸºäºåˆ†è¯åçš„ç»“æœè®¾ç½®æ•°å­—æ˜ å°„
nums = num(toks)[:20]
'''
TensorText([   0,    0, 1269,    9, 1270,    0,   14,    0,    0,   12,    0,
               0,   15, 1271,    0,   22,   24,    0,  795,   24])
'''
# å°†æ•°å­—åŒ–çš„å¥å­å†æ˜ å°„å›tokens
' '.join(num.vocab[o] for o in nums)
'''
'xxunk xxunk uses the complex xxunk of xxunk xxunk and xxunk xxunk to study xxunk \'s " xxunk revolution "'
'''
```

`Numericalize`çš„é»˜è®¤å€¼ä¸º`min_freq=3`å’Œ`max_vocab=60000`ã€‚`max_vocab=60000`å¯¼è‡´ fastai ç”¨ç‰¹æ®Šçš„*æœªçŸ¥å•è¯*æ ‡è®°`xxunk`æ›¿æ¢é™¤æœ€å¸¸è§çš„ 60,000 ä¸ªå•è¯ä¹‹å¤–çš„æ‰€æœ‰å•è¯ã€‚è¿™æœ‰åŠ©äºé¿å…è¿‡å¤§çš„åµŒå…¥çŸ©é˜µï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦å¹¶å ç”¨å¤ªå¤šå†…å­˜ï¼Œå¹¶ä¸”è¿˜å¯èƒ½æ„å‘³ç€æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥è®­ç»ƒç¨€æœ‰å•è¯çš„æœ‰ç”¨è¡¨ç¤ºã€‚ç„¶è€Œï¼Œé€šè¿‡è®¾ç½®`min_freq`æ¥å¤„ç†æœ€åä¸€ä¸ªé—®é¢˜æ›´å¥½ï¼›é»˜è®¤å€¼`min_freq=3`æ„å‘³ç€å‡ºç°å°‘äºä¸‰æ¬¡çš„ä»»ä½•å•è¯éƒ½å°†è¢«æ›¿æ¢ä¸º`xxunk`ã€‚

##### 4.2.4 å°†è¿™äº›txtæ”¾è¿›batchesé‡Œé¢ï¼Œå½¢æˆDataLoader

<img src="D:\Git\a\Path-Records\img\04-2-4.jpg" style="zoom:100%;" />

æ­¥éª¤ï¼š

- å°†200æ¡txtåˆ†è¯åæ˜ å°„æˆæ•°å­—ï¼Œå†å°†200æ¡æ•°å­—æ‹¼æ¥æˆä¸€ä¸ªstream
- ä¸¤ç§æƒ…å†µ
  - streamåˆ‡æˆå›ºå®šé•¿åº¦çš„mini-streamï¼Œå¹¶è®²å®ƒä»¬åˆ†æˆå›ºå®šå¤§å°çš„batchï¼Œè¿™æ ·batch1ä¸­çš„mini-stream1å’Œmini-stream2è¿ç»­
  - stream reshapeæˆè§„æ•´çš„äºŒç»´ç»“æ„ï¼Œç„¶ååˆ‡æˆä¸åŒçš„batchï¼ˆå¦‚ä¸Šå›¾ï¼‰ï¼Œè¿™æ ·batch1ä¸­çš„mini-stream1å’Œbatch2ä¸­mini-stream1è¿ç»­

```python
# toks200æ˜¯200æ¡txtåˆ†è¯åçš„ï¼Œnum200å°±æ˜¯200æ¡åˆ†è¯å¥å­æ˜ å°„æˆæ•°å­—çš„
num200 = toks200.map(num)
# DataLoader
dl = LMDataLoader(num200)
x,y = first(dl)
x.shape, y.shape
'(torch.Size([64, 72]), torch.Size([64, 72]))ï¼Œå¯è§DataLoaderå°†streamæ‹†æˆäº†64ä¸ªmini-streamï¼Œæ¯ä¸ªmini-streamæœ‰72ä¸ªtokens'
# xå’Œyåªæ˜¯ç›¸å·®ä¸€ä¸ªtoken
' '.join(num.vocab[o] for o in x[0][:15])
'xxbos xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and'
' '.join(num.vocab[o] for o in y[0][:15])
'xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj'
```

#### 4.3 è®­ç»ƒæ–‡æœ¬åˆ†ç±»å™¨

* ä½¿ç”¨è¿ç§»å­¦ä¹ è®­ç»ƒæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ†ç±»å™¨æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¾®è°ƒåœ¨ Wikipedia ä¸Šé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä»¥é€‚åº” IMDb è¯„è®ºçš„è¯­æ–™åº“ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹æ¥è®­ç»ƒåˆ†ç±»å™¨ã€‚

##### 4.3.1 è¯­è¨€è¯†åˆ«-æ•°æ®åŠ è½½å™¨DataBlock

* **å®ä¾‹æ–¹æ³•**ï¼Œéœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç„¶åæ‰èƒ½è°ƒç”¨çš„æ–¹æ³•ï¼ŒMyClass.instance_method()ä¼šæŠ¥é”™ï¼›**ç±»æ–¹æ³•**å°±ä¸éœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç›´æ¥è°ƒç”¨MyClass.class_method()ä¸ä¼šæŠ¥é”™ï¼Œè€Œä¸”å¯ä»¥è®¿é—®ç±»å˜é‡ï¼›**é™æ€æ–¹æ³•**ä¹Ÿä¸éœ€è¦å®ä¾‹åŒ–ç±»ï¼Œç›´æ¥è°ƒç”¨MyClass.static_method()ä¹Ÿä¸ä¼šæŠ¥é”™ï¼Œä½†æ²¡åŠæ³•è®¿é—®ç±»å˜é‡ã€‚

```python
from fastai.text.all import *
path = untar_data(URLs.IMDB)
# è¿™æ˜¯ä¸Šé¢å…¨éƒ¨æ‰‹åŠ¨ä»£ç çš„æ±‡æ€»--------------------------------------------------------------------------------
files = get_text_files(path, folders=['train', 'test', 'unsup'])
#txt = files[0].open().read()
spacy = WordTokenizer()
#toks = first(spacy([txt]))
tkn = Tokenizer(spacy)
# è¯»å–200ä¸ªfilesä¸­çš„å¥å­txts
txts = L(o.open().read() for o in files[:200])
# å°†å¥å­txtsæ‹†åˆ†æˆszä¸ªvocabularyï¼Œå¹¶tokenize txt
#def subword(sz):
#    sp = SubwordTokenizer(vocab_sz=sz)
#    sp.setup(txts)
#    return ' '.join(first(sp([txt]))[:40])
#tks = tkn(txt)
toks = txts.map(tkn)
num = Numericalize()
num.setup(toks)
#nums = num(toks)[:20]
num = toks.map(num)
dl = LMDataLoader(num) #æ²¡æ³•æŒ‡å®šbatch_size
x,y = first(dl)
# fastaiæœ‰ç°æˆçš„æ–¹æ³•---------------------------------------------------------------------------------------
get_imdb = partial(get_text_files, folders=['train','test','unsup'])
# è¯­è¨€æ¨¡å‹çš„æ•°æ®åŠ è½½å™¨
dls_lm = DataBlock(
    blocks = TextBlock.from_folder(path,is_lm=True),
    get_items = get_imdb,
    splitter = RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
dls_lm.show_batch(max_n=3)
```

|      | text                                                         | text_                                                        |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 0    | xxbos xxmaj it â€™s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard | xxmaj it â€™s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard xxunk |
| 1    | what xxmaj i â€˜ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this | xxmaj i â€˜ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this is |

##### 4.3.2 è¯­è¨€è¯†åˆ«-Fine-tune

* **Embedding**ï¼šåµŒå…¥æ˜¯æŠŠæ–‡å­—è½¬æ¢æˆè®¡ç®—æœºèƒ½ç†è§£çš„æ•°å­—ï¼Œè€Œä¸”è¿™ç§è½¬æ¢ä¸æ˜¯ç®€å•çš„ 1 å¯¹ 1 æ˜ å°„ï¼Œè€Œæ˜¯è®©è¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨æ•°å€¼ç©ºé—´é‡Œä¹Ÿé å¾—æ›´è¿‘ã€‚å¸¸è§çš„ NLP ä»»åŠ¡éƒ½ä¼šç”¨åˆ° Embeddingï¼Œæ¯”å¦‚ï¼š**Word2Vec**ï¼ˆGoogle å¼€å‘çš„è¯å‘é‡æ¨¡å‹ï¼‰ã€**GloVe**ï¼ˆæ–¯å¦ç¦å¼€å‘çš„è¯å‘é‡ï¼‰ã€**FastText**ï¼ˆFacebook å¼€å‘çš„è¯å‘é‡ï¼‰ã€**BERT / GPT**ï¼ˆç°ä»£ NLP æ¨¡å‹çš„åº•å±‚éƒ½ä¼šç”¨æ›´é«˜çº§çš„ Embeddingï¼‰ã€‚

```python
learn = language_model_learner(
    dls_lm,
    AWD_LSTM,
    drop_mult=0.3,      # Dropout ä¹˜æ•°ï¼ˆæ§åˆ¶æ­£åˆ™åŒ–çš„ç¨‹åº¦ï¼‰
    metrics=[accuracy, Perplexity()]     # è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ + å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
).to_fp16()      # å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦ï¼ˆFP16ï¼‰ï¼Œæå‡è®­ç»ƒé€Ÿåº¦
```

* **æŸå¤±å‡½æ•°**ï¼šäº¤å‰ç†µæŸå¤±
* **Perplexity** metricså¸¸ç”¨åœ¨NLPä¸­ï¼Œå®ƒæ˜¯æŸå¤±å‡½æ•°çš„æŒ‡æ•°ï¼ˆå³torch.exp(cross_entropy)ï¼‰

* **Dropout**æ˜¯ä¸€ç§é˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„æ–¹æ³•ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºâ€œä¸¢å¼ƒâ€ï¼ˆè®¾ä¸º 0ï¼‰ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦ä¾èµ–æŸäº›ç‰¹å®šçš„ç‰¹å¾ã€‚

###### â‘ **Dropout vs. Weight Decayï¼šåŒºåˆ«å¯¹æ¯”**

| ç‰¹æ€§        | Dropout                                              | Weight Decay (L2 æ­£åˆ™åŒ–)             |
| ---------------- | -------------------------------------------------------- | ---------------------------------------- |
| ä½œç”¨æ–¹å¼       | éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼Œè®©ç½‘ç»œå­¦ä¼šä¸åŒçš„ç‰¹å¾             | é™åˆ¶æƒé‡å¤§å°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ             |
| é€‚ç”¨èŒƒå›´    | æ›´é€‚ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå°¤å…¶æ˜¯ CNNã€RNNã€Transformerï¼‰ | é€‚ç”¨äºå‡ ä¹æ‰€æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹           |
| è®­ç»ƒä¸æµ‹è¯•   | åªåœ¨è®­ç»ƒæ—¶ç”Ÿæ•ˆï¼Œæµ‹è¯•æ—¶å…³é—­                           | è®­ç»ƒå’Œæµ‹è¯•æ—¶éƒ½ç”Ÿæ•ˆ                   |
| æ•°å­¦å…¬å¼     | è®©éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºè®¾ä¸º 0                                 | ç»™æŸå¤±å‡½æ•°å¢åŠ  Î»âˆ‘w^2 æƒ©ç½šé¡¹              |
| ç›´è§‚ç†è§£     | è®©ç¥ç»ç½‘ç»œå˜æˆä¸€ä¸ªå°å‹é›†æˆå­¦ä¹                        | å‡å°‘å¤§æƒé‡ï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ•°æ® |
| å¯¹è®¡ç®—çš„å½±å“ | å¢åŠ è®¡ç®—é‡ï¼Œå› ä¸ºæ¯æ¬¡è®­ç»ƒéƒ½è¦éšæœºä¸¢å¼ƒä¸åŒç¥ç»å…ƒ       | ä¸ä¼šå¢åŠ è®¡ç®—é‡                       |

###### â‘¡**ä»€ä¹ˆæ—¶å€™ç”¨ Dropoutï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨ Weight Decayï¼Ÿ**

è™½ç„¶å®ƒä»¬çš„å®ç°æ–¹å¼ä¸åŒï¼Œä½†ç›®çš„éƒ½æ˜¯ é˜²æ­¢æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

âœ” å¯ä»¥ä¸€èµ·ä½¿ç”¨ï¼

- Dropout ä¸»è¦ä½œç”¨åœ¨ ç½‘ç»œç»“æ„ å±‚é¢ï¼ˆä¸¢å¼ƒç¥ç»å…ƒï¼‰
- Weight Decay ä¸»è¦ä½œç”¨åœ¨ å‚æ•°ä¼˜åŒ– å±‚é¢ï¼ˆçº¦æŸæƒé‡å¤§å°ï¼‰
- ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸ ä¸¤è€…éƒ½ç”¨

| æƒ…å†µ                                   | æ›´é€‚åˆ Dropout    | æ›´é€‚åˆ Weight Decay |
| ------------------------------------------ | --------------------- | ----------------------- |
| æ•°æ®é‡å°ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ                   | âœ…                     | âœ…                       |
| ç¥ç»ç½‘ç»œå¾ˆæ·±ï¼ˆCNNã€LSTMã€Transformerï¼‰ | âœ…                     | âœ…                       |
| å‚æ•°é‡å°‘ï¼ˆå°å‹æ¨¡å‹ï¼Œå¦‚çº¿æ€§å›å½’ï¼‰       | âŒ                     | âœ…                       |
| è¿‡æ‹Ÿåˆä¸¥é‡                             | âœ…ï¼ˆæé«˜ Dropout ç‡ï¼‰  | âœ…ï¼ˆå¢å¤§ Weight Decayï¼‰  |
| æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆRNNï¼‰                    | âœ…ï¼ˆDropout ä¹Ÿèƒ½å¸®å¿™ï¼‰ | âŒ                       |

ğŸ’¡ ç»éªŒæ³•åˆ™ï¼š

- å¤§æ¨¡å‹ï¼ˆCNN/RNNï¼‰ â†’ ä¸¤è€…éƒ½ç”¨ï¼Œdropout=0.3~0.5 + wd=1e-4
- å°æ¨¡å‹ï¼ˆçº¿æ€§å›å½’ï¼‰ â†’ ä¸»è¦ç”¨ Weight Decay
- æ•°æ®é‡ç‰¹åˆ«å° â†’ Dropout å¯ä»¥å°‘ç”¨ï¼Œä½† Weight Decay ä»ç„¶æœ‰æ•ˆ

**è®­ç»ƒ**ï¼š

åƒ`vision_learner`ä¸€æ ·ï¼Œå½“ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆè¿™æ˜¯é»˜è®¤è®¾ç½®ï¼‰æ—¶ï¼Œ`language_model_learner`åœ¨ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨`freeze`ã€‚å› æ­¤è¿™å°†ä»…è®­ç»ƒåµŒå…¥å±‚ï¼Œå…¶ä»–éƒ¨åˆ†çš„æƒé‡æ˜¯è¢«å†»ç»“çš„ã€‚ä¹‹æ‰€ä»¥åªè®­ç»ƒåµŒå…¥å±‚ï¼Œæ˜¯å› ä¸ºåœ¨ IMDb è¯­æ–™åº“ä¸­ï¼Œå¯èƒ½ä¼šæœ‰ä¸€äº›è¯æ±‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯è¡¨ä¸­æ‰¾ä¸åˆ°ï¼Œè¿™äº›è¯çš„åµŒå…¥ï¼ˆembeddingsï¼‰éœ€è¦éšæœºåˆå§‹åŒ–ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œè€Œé¢„è®­ç»ƒæ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†å·²ç»æœ‰äº†è¾ƒå¥½çš„å‚æ•°ï¼Œå› æ­¤æš‚æ—¶ä¸ä¼šè¢«è°ƒæ•´ã€‚

fine_tuneä¸ä¼šä¿å­˜åŠæˆå“æ¨¡å‹ç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨äº†fit_one_cycle

```python
learn.fit_one_cycle(1, 2e-2)
```

###### â‘¢**fit vs. fit_one_cycle å¯¹æ¯”**

| å¯¹æ¯”é¡¹     | fit                      | fit_one_cycle               |
| ---------- | ------------------------ | --------------------------- |
| å­¦ä¹ ç‡è°ƒåº¦ | å›ºå®šå­¦ä¹ ç‡               | åŠ¨æ€è°ƒæ•´ï¼ˆwarm-up + decayï¼‰ |
| åŠ¨é‡è°ƒåº¦   | ä¸å˜                     | è‡ªé€‚åº”è°ƒæ•´                  |
| é€‚ç”¨åœºæ™¯   | å°è§„æ¨¡è®­ç»ƒã€ç®€å•ä»»åŠ¡     | æ·±åº¦å­¦ä¹ ã€å¤§è§„æ¨¡è®­ç»ƒ        |
| ä¼˜ç‚¹       | ç®€å•ã€ç¨³å®š               | æé«˜æ³›åŒ–èƒ½åŠ›ã€æ”¶æ•›æ›´å¿«      |
| ç¼ºç‚¹       | å¯èƒ½è®­ç»ƒæ…¢ï¼Œæ³›åŒ–èƒ½åŠ›ä¸ä½³ | éœ€è¦è°ƒæ•´å‚æ•°ï¼Œç¨å¤æ‚        |

##### 4.3.3 è¯­è¨€è¯†åˆ«-ä¿å­˜æ¨¡å‹

```python
# ä¿å­˜ç»å†1æ¬¡epochçš„æ¨¡å‹çŠ¶æ€
learn.save('1epoch')
# åŠ è½½æ¨¡å‹
learn = learn.load('1epoch')
# åˆå§‹è®­ç»ƒå®Œæˆï¼Œè§£å†»åç»§ç»­å¾®è°ƒæ¨¡å‹
learn.unfreeze()
learn.fit_one_cycle(10,2e-3)
# ä¿å­˜ç¼–ç å™¨encoderï¼šç¼–ç å™¨å°±æ˜¯æˆ‘ä»¬è®­ç»ƒçš„æ‰€æœ‰æ¨¡å‹ï¼Œé™¤äº†æœ€åä¸€å±‚ï¼ˆå®ƒå°†activationsè½¬æ¢æˆé¢„æµ‹tokençš„æ¦‚ç‡ï¼‰
learn.save_encoder('finetuned')
```

æ­¤æ—¶å·²ç»å®Œæˆç¬¬äºŒé˜¶æ®µï¼š

![](D:\Git\a\Path-Records\img\dlcf_1001.png)

```python
# Kaggleä¸­å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¸‹è½½ä¸‹æ¥
path1 = Path('/kaggle/working/')
learn.save(path1/'mymodel')  # ä¿å­˜æ¨¡å‹
learn.load(path1/'mymodel') #åŠ è½½æ¨¡å‹

path2 = Path('/kaggle/working/')
learn.save_encoder(path2/'finetuned')
learn.load_encoder(path2/'finetuned')
```

###### fastaiçš„å¿«æ·æ„é€ å™¨learn

ä¸åŒä»»åŠ¡æœ‰ä¸åŒçš„å¿«æ·æ„é€ å™¨ï¼š

| ä»»åŠ¡                 | å¿«æ·æ–¹æ³•                                            | åº•å±‚ç±»    |
| -------------------- | --------------------------------------------------- | --------- |
| è®¡ç®—æœºè§†è§‰           | `vision_learner`                                    | `Learner` |
| è‡ªç„¶è¯­è¨€å¤„ç†         | `language_model_learner`ã€`text_classifier_learner` | `Learner` |
| è¡¨æ ¼æ•°æ®             | `tabular_learner`                                   | `Learner` |
| ååŒè¿‡æ»¤ï¼ˆæ¨èç³»ç»Ÿï¼‰ | `collab_learner`                                    | `Learner` |

##### 4.3.4 æ–‡æœ¬ç”Ÿæˆï¼ˆä¸åœ¨é˜¶æ®µä¸­ï¼‰

```python
TEXT = "I liked this movie because"
N_WORDS = 40
N_SENTENCES = 2
# temperature=0.75ï¼šæ§åˆ¶éšæœºæ€§ã€‚>1æ›´éšæœºæ›´æœ‰åˆ›é€ åŠ›ä½†å¯èƒ½æ²¡æ„ä¹‰ï¼›<1æ›´ç¡®å®šæ›´åˆç†ä½†å¯èƒ½è¾ƒæ— èŠ
preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)
         for _ in range(N_SENTENCES)]
print("\n".join(preds))
```

##### 4.3.5 æ–‡æœ¬åˆ†ç±»-æ•°æ®åŠ è½½å™¨DataBlock

```python
# åˆ›å»ºæ•°æ®åŠ è½½å™¨
dls_clas = DataBlock(
    blocks=(TextBlock.from_folder(path,vocab=dls_lm.vocab),CategoryBlock),
    '''ä½¿ç”¨ dls_lm.vocab çš„è¯æ±‡è¡¨å¯¹æ–‡æœ¬è¿›è¡Œæ•°å€¼åŒ–ï¼Œä»¥ä¿æŒå’Œ dls_lm ç›¸åŒçš„è¯ç´¢å¼•ç¼–å·ï¼Œè¿™ä¸ªè¯æ±‡è¡¨ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å­—åºåˆ—ï¼Œ1 tokenâ†’1 integer
    é€šè¿‡ä¼ é€’ `is_lm=False`ï¼ˆæˆ–è€…æ ¹æœ¬ä¸ä¼ é€’ `is_lm`ï¼Œå› ä¸ºå®ƒé»˜è®¤ä¸º `False`ï¼‰ï¼Œæˆ‘ä»¬å‘Šè¯‰ `TextBlock` æˆ‘ä»¬æœ‰å¸¸è§„æ ‡è®°çš„æ•°æ®ï¼Œè€Œä¸æ˜¯å°†ä¸‹ä¸€ä¸ªæ ‡è®°ä½œä¸ºæ ‡ç­¾'''
    get_y=parent_label,
    get_items=partial(get_text_files,folders=['train','test']),
    splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path,path=path,bs=128,seq_len=72)
dls_clas.show_batch(max_n=4)
```

```python
# åªä¸ºäº†è¯´æ˜æ€ä¹ˆå¤„ç†åœ¨åˆ†ç±»ç®—æ³•ä¸­texté•¿çŸ­ä¸ä¸€çš„é—®é¢˜
files = get_text_files(path, folders=['train','test','unsup'])
txts = L(o.open().read() for o in files[:200])
spacy = WordTokenizer()
tkn = Tokenizer(spacy)
toks200 = txts.map(tkn)
num = Numericalize()
num.setup(toks200)
nums_samp = toks200[:10].map(num)
nums_samp.map(len)
'(#10) [158,319,181,193,114,145,260,146,252,295]ï¼šå¯è§Lç±»æ¯æ¡å¤§å°ä¸ç­‰'
```

ä½†æ˜¯ï¼ŒPyTorch çš„ DataLoaderéœ€è¦å°†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰é¡¹ç›®æ•´åˆåˆ°ä¸€ä¸ªå¼ é‡ä¸­ï¼Œè€Œä¸€ä¸ªå¼ é‡å…·æœ‰å›ºå®šçš„å½¢çŠ¶ï¼ˆå³ï¼Œæ¯ä¸ªè½´ä¸Šéƒ½æœ‰ç‰¹å®šçš„é•¿åº¦ï¼Œå¹¶ä¸”æ‰€æœ‰é¡¹ç›®å¿…é¡»ä¸€è‡´ï¼‰ï¼Œä¸ºäº†è®©å®ƒä»¬ç›¸ç­‰ï¼Œå°±å¾—å¡«å……ã€‚

* å¡«å……ï¼šæˆ‘ä»¬å°†æ‰©å±•æœ€çŸ­çš„æ–‡æœ¬ä»¥ä½¿å®ƒä»¬éƒ½å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„å¡«å……æ ‡è®°ï¼Œè¯¥æ ‡è®°å°†è¢«æˆ‘ä»¬çš„æ¨¡å‹å¿½ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…å†…å­˜é—®é¢˜å¹¶æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬å°†å¤§è‡´ç›¸åŒé•¿åº¦çš„æ–‡æœ¬æ‰¹é‡å¤„ç†åœ¨ä¸€èµ·ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªepochå‰ï¼ˆå¯¹äºè®­ç»ƒé›†ï¼‰æŒ‰é•¿åº¦å¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼Œåˆ†æˆå‡ ä¸ªbatchesï¼Œä½¿ç”¨æ¯ä¸ªbatchä¸­æœ€å¤§æ–‡æ¡£çš„å¤§å°ä½œä¸ºè¿™ä¸ªbatchçš„ç›®æ ‡å¤§å°ã€‚

* ä½¿ç”¨DataBlock+is_lm=Falseæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬æ“ä½œã€‚

###### DataBlock & DataLoaders

| **ç»„ä»¶**         | **ä½œç”¨**                            |
| ---------------- | ----------------------------------- |
| `DataBlock`      | åªæ˜¯ æ•°æ®å¤„ç†çš„æ¨¡æ¿ï¼Œä¸åŒ…å«æ•°æ®     |
| `.dataloaders()` | å°† `DataBlock` è½¬ä¸º `DataLoaders`   |
| `DataLoaders`    | çœŸæ­£çš„æ•°æ®åŠ è½½å™¨ï¼ŒåŒ…å«è®­ç»ƒ/éªŒè¯æ•°æ® |

###### DataLoadersçš„ä¸»è¦å˜ç§

| å˜ç§åç§°                  | é€‚ç”¨ä»»åŠ¡                                |
| ------------------------- | --------------------------------------- |
| `ImageDataLoaders`        | å›¾åƒåˆ†ç±»ï¼ˆè‡ªåŠ¨ä»æ–‡ä»¶å¤¹åŠ è½½å›¾åƒï¼‰        |
| `TextDataLoaders`         | è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼ˆè‡ªåŠ¨å¤„ç†æ–‡æœ¬æ•°æ®ï¼‰ |
| `TabularDataLoaders`      | è¡¨æ ¼æ•°æ®ï¼ˆç”¨äºç»“æ„åŒ–æ•°æ®ï¼Œå¦‚ CSVï¼‰      |
| `SegmentationDataLoaders` | å›¾åƒåˆ†å‰²ï¼ˆåƒç´ çº§åˆ†ç±»ä»»åŠ¡ï¼‰              |
| `DataLoaders`ï¼ˆåŸºç±»ï¼‰     | é€šç”¨æ•°æ®åŠ è½½å™¨ï¼Œé€‚ç”¨äºè‡ªå®šä¹‰æ•°æ®        |

##### 4.3.6 æ–‡æœ¬åˆ†ç±»-Fine-tune

```python
learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,metrics=accuracy).to_fp16()
learn = learn.load_encoder('/kaggle/input/finetuned/pytorch/default/1/finetuned')
```

* é€å±‚è§£å†»gradual unfreezingï¼šå†»ç»“æ¨¡å‹çš„éƒ¨åˆ†å±‚ï¼Œå…ˆè®­ç»ƒæ¨¡å‹çš„æœ€åä¸€å±‚ï¼Œç„¶åé€æ¸è§£å†»å‰é¢çš„å±‚ã€‚ä»æœ€åä¸€å±‚å¼€å§‹è®­ç»ƒï¼Œå› ä¸ºè¿™äº›å±‚å·²ç»åŒ…å«äº†å¤§éƒ¨åˆ†çš„ç‰¹å¾ä¿¡æ¯ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸è§£å†»å‰é¢çš„å±‚ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ã€‚
* é€å±‚è§£å†»çš„åŸå› ï¼šâ‘ é¢„è®­ç»ƒçš„ç‰¹å¾æå–å±‚-åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œé€šå¸¸å·²ç»å­¦åˆ°äº†é€šç”¨çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒè¿™äº›å±‚ï¼Œè€Œæ˜¯å†»ç»“å®ƒä»¬ï¼Œä¸“æ³¨äºè®­ç»ƒæ–°ä»»åŠ¡çš„æœ€åå‡ å±‚ã€‚â‘¡é¿å…è¿‡æ‹Ÿåˆ-å¦‚æœä¸€æ¬¡æ€§è§£å†»æ‰€æœ‰å±‚ï¼Œæ¨¡å‹å¯èƒ½ä¼šå¿«é€Ÿè¿‡æ‹Ÿåˆï¼Œå› ä¸ºè¾ƒå°çš„æ•°æ®é›†æ— æ³•æ”¯æŒæ‰€æœ‰å±‚çš„è®­ç»ƒã€‚é€å±‚è§£å†»æœ‰åŠ©äºé€æ¸é€‚åº”æ–°ä»»åŠ¡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆçš„é£é™©ã€‚â‘¢è®­ç»ƒæ•ˆç‡-å†»ç»“å‰é¢å‡ å±‚åï¼Œè®¡ç®—é‡å‡å°‘ï¼Œè®­ç»ƒé€Ÿåº¦åŠ å¿«ã€‚

```python
learn.fit_one_cycle(1,2e-2) #é»˜è®¤æ¨¡å‹ä¼šå†»ç»“ï¼Œå› æ­¤åªæ˜¯è®­ç»ƒæœ€åä¸€å±‚
# è®­ç»ƒå€’æ•°2å±‚
learn.freeze_to(-2)
learn.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2))
# è®­ç»ƒå€’æ•°3å±‚
learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))
# è®­ç»ƒæ‰€æœ‰å±‚
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
```

## 5: From-scratch model (Tabular) (PDL2022)

### 5.1 æ¸…æ´—æ•°æ®

- ä¸€äº›èµ·æ‰‹codeï¼Œä½ åº”è¯¥å·²ç»ç†Ÿæ‚‰

```python
import os
from pathlib import Path

iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')
if iskaggle: path = Path('../input/titanic')
else:
    path = Path('titanic')
    if not path.exists():
        import zipfile,kaggle
        kaggle.api.competition_download_cli(str(path))
        zipfile.ZipFile(f'{path}.zip').extractall(path)
        
import torch, numpy as np, pandas as pd
np.set_printoptions(linewidth=140)
torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)
pd.set_option('display.width', 140)
```

- æ•°æ®æ¸…æ´—

```python
df = pd.read_csv(path/'train.csv')
#æ£€æŸ¥missing data
df.isna().sum()
 #modeè®¡ç®—ä¼—æ•°
modes = df.mode().iloc[0]
df.fillna(modes, inplace=True)
df.isna().sum()
#ä»¥ä¸Šï¼Œå¤„ç†äº†æ‰€æœ‰çš„missing data
```

#### imputing missing values

å¡«è¡¥ç¼ºå¤±å€¼ï¼Œåœ¨Jeremyçš„ä¾‹å­ä¸­ï¼Œä»–ä½¿ç”¨äº†ä¼—æ•°å»å¡«è¡¥ç¼ºå¤±å€¼ï¼Œä»–è¯´è¿™ä¸ªæ–¹æ³•é€šå¸¸æœ‰ç”¨ã€‚ä¸€èˆ¬å½“æ•°æ®ä¸­åŒ…å«ç¼ºå¤±å€¼ï¼Œä¸å»ºè®®ç›´æ¥åˆ é™¤åˆ—ï¼Œå› ä¸ºå¹¶æ²¡æœ‰å®é™…çš„ç†ç”±åˆ é™¤åˆ—ï¼Œä½†æœ‰å¾ˆå¤šç†ç”±ä¿ç•™åˆ—ã€‚æœ‰å¦ä¸€ç§å¤„ç†æ–¹å¼ï¼Œå°±æ˜¯æ–°å¢ä¸€åˆ—å»è¯´æ˜æŸåˆ—æ˜¯å¦æ˜¯ç¼ºå¤±å€¼ï¼Œè¿™æ ·ä¿¡æ¯å°±å¾—ä»¥ä¿å…¨ã€‚

ç”¨ä¼—æ•°å¡«è¡¥ç¼ºå¤±å€¼æ˜¯æœ€æœ€ç®€å•çš„æ–¹æ³•ï¼Œä¸ºä»€ä¹ˆè¦ç”¨è¿™ä¹ˆç®€å•çš„æ–¹æ³•å‘¢ï¼Ÿå› ä¸ºå¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œç”¨ä»€ä¹ˆæ–¹æ³•å¡«è¡¥ç¼ºå¤±å€¼éƒ½æ²¡æœ‰ä»€ä¹ˆå¤ªå¤§çš„åŒºåˆ«ï¼Œæ‰€ä»¥åœ¨åˆšå¼€å§‹å»ºç«‹ä¸€ä¸ªbaselineçš„æ—¶å€™ï¼Œæ²¡æœ‰å¿…è¦æå¾—å¤ªå¤æ‚ã€‚

```python
#å¤§è‡´reviewä¸€ä¸‹åŒ…å«æ•°å­—çš„æ•°æ®
df.describe(include=(np.number))
df['Fare'].hist();
#å‘ç°Fareæ˜¯ä¸ªé•¿å°¾æ•°æ®ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¸å¤ªå–œæ¬¢é•¿å°¾æ•°æ®ï¼Œä¼šå½±å“æ¨¡å‹æ•ˆæœï¼Œä¸è¿‡æœ‰ä¸ªå‡ ä¹è‚¯å®šæˆæœçš„åŠæ³•ï¼Œå¯ä»¥è®²é•¿å°¾æ•°æ®è½¬æ¢æˆç¦»æ•£æ•°æ®
df['LogFare'] = np.log(df['Fare']+1)
#å‘ç°Pclassçœ‹ä¸Šå»ä¸åƒä¸ªæ­£ç»æ•°æ®ï¼Œåƒä¸ªåˆ†ç±»æ•°æ®ï¼Œç¡®è®¤ä¸€ä¸‹
pclasses = sorted(df.Pclass.unique())

#å¤§è‡´reviewä¸€ä¸‹ä¸æ˜¯æ•°å­—çš„æ•°æ®ï¼šNameã€Sexã€Ticketã€Cabinã€Embarkedï¼Œå¯ä»¥çœ‹åˆ°æœ‰å‡ ä¸ªæ•°æ®å¯ä»¥åˆ†ç±»ä¸ºå‡ ç±»ï¼Œæœ‰æœºä¼šè½¬æ¢æˆå¯ç”¨æ•°æ®
df.describe(include=[object])
#æƒ³ç”¨çš„åˆ†ç±»æ•°æ®æ˜¾ç„¶ä¸èƒ½å°±ç”¨objectè®­ç»ƒï¼Œæˆ‘ä»¬å°†å®ƒè½¬æ¢æˆdummy column
df = pd.get_dummies(df, columns=["Sex","Pclass","Embarked"])
df.columns
'''Index(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',
       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],
      dtype='object')'''
added_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']
df[added_cols].head()

#ç°åœ¨å¯ä»¥æ„å»ºæ•°æ®äº†
from torch import tensor
t_dep = tensor(df.Survived) #output
indep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols #input
t_indep = tensor(df[indep_cols].values, dtype=torch.float) #pytorchéœ€è¦æ•°æ®æ˜¯float
t_indep.shape
len(t_indep.shape) #rank
```

### 5.2 æ„å»ºçº¿æ€§æ¨¡å‹â›”

- åˆå§‹åŒ–parameters

```python
torch.manual_seed(442) #è¿™ä¸ªä¼šä½¿éšæœºç”Ÿæˆæ•°å¯å†ç°ï¼Œä¸è¿‡Jeremyåœ¨å®é™…æ“ä½œä¸­å¹¶ä¸ä¼šä½¿ç”¨å®ƒ
n_coeff = t_indep.shape[1]
coeffs = torch.rand(n_coeff)-0.5 #torch.rand()ç”Ÿæˆ0~1çš„æ•°ï¼Œæ‰€ä»¥å‡0.5
```

- åˆ©ç”¨broadcastingè¿›è¡Œè®¡ç®—å¹¶æ±‚loss

```python
#å½’ä¸€åŒ–
vals,indices = t_indep.max(dim=0)
t_indep = t_indep / vals
#è®¡ç®—preds
preds = (t_indep*coeffs).sum(axis=1)
#è®¡ç®—loss
loss = torch.abs(preds-t_dep).mean()
#å°†ä¸Šè¿°è®¡ç®—å†™æˆå‡½æ•°
def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)
def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()
```

### 5.3 æ¢¯åº¦ä¸‹é™(one step)â›”

```python
#å¼€å¯æ¢¯åº¦è·Ÿè¸ª
coeffs.requires_grad_()
#è®¡ç®—loss
loss = calc_loss(coeffs, t_indep, t_dep)
#åå‘ä¼ æ’­
loss.backward()
coeffs.grad #æŸ¥çœ‹
#æ‰‹åŠ¨
with torch.no_grad(): #æš‚æ—¶å…³é—­PyTorchçš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶
    coeffs.sub_(coeffs.grad * 0.1)
    coeffs.grad.zero_()
    print(calc_loss(coeffs, t_indep, t_dep))
```

### 5.4 è®­ç»ƒæ¨¡å‹

- æ¸…æ´—å¥½æ•°æ®ï¼Œå¹¶å½’ä¸€åŒ–
- æ‹†åˆ†æ•°æ®ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†

```python
from fastai.data.transforms import RandomSplitter
trn_split,val_split=RandomSplitter(seed=42)(df) #trn_split/val_splitæ˜¯dfçš„rank-0å¤§å°çš„index
'(#10) [788,525,821,253,374,98,215,313,281,305]ä¸€å…±æœ‰713rowsï¼Œéšæœºé€‰å–rowçš„æ•°'
trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]
trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]
len(trn_indep),len(val_indep)
```

- å†™æ›´æ–°æ¢¯åº¦çš„å‡½æ•°ï¼ˆæ€è·¯å¯ä»¥å‚è€ƒ5.3 withä¸‹é¢çš„éƒ¨åˆ†ï¼‰

```python
def update_coeffs(coeffs, lr):
    coeffs.sub_(coeffs.grad * lr)
    coeffs.grad.zero_()
```

- å†™è®­ç»ƒä¸€ä¸ªepochçš„å‡½æ•°ï¼ˆæ€è·¯å¯ä»¥å‚è€ƒ5.2loss/5.3ï¼‰

```python
#5.2æ„å»ºçš„æ¨¡å‹æŒªåˆ°è¿™é‡Œ
def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)
def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()
#5.3å†™ä¸€ä¸ªepoch
def one_epoch(coeffs, lr):
    loss = calc_loss(coeffs, trn_indep, trn_dep) #å¼•ç”¨å‡½æ•°
    loss.backward()
    with torch.no_grad(): update_coeffs(coeffs, lr) #å¼•ç”¨å‡½æ•°
    print(f"{loss:.3f}", end="; ")
```

- åˆå§‹åŒ–å‚æ•°ï¼ˆå‚è€ƒ5.2/5.3ï¼‰

```python
def init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()
```

- å†™è®­ç»ƒnä¸ªepochçš„å‡½æ•°ï¼Œè¾“å‡ºlosså˜åŒ–ï¼Œè¿”å›å‚æ•°

```python
def train_model(epochs=30, lr=0.01):
    torch.manual_seed(442) #ä¸ºäº†å¯å¤ç°
    coeffs = init_coeffs()
    for i in range(epocks): one_epoch(coeffs, lr=lr)
    return coeffs
```

- è®­ç»ƒ

```python
coeffs = train_model(18, lr=0.02)
```

- æ˜¾ç¤ºå‚æ•°

```python
def show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))
show_coeffs()
```

### 5.5 è¡¡é‡å‡†ç¡®åº¦

```python
preds = calc_preds(coeffs, val_indep)
results = val_dep.bool()==(preds>0.5)
results.float().mean()

#ä¹Ÿå†™æˆå‡½æ•°
def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs,val_indep)>0.5)).float().mean()
acc(coeffs)
```

### 5.6 ä½¿ç”¨æ¿€æ´»å‡½æ•°

æ›´æ–°äº†ä¸€ä¸‹predsçš„è®¡ç®—å‡½æ•°ï¼ˆå‚è€ƒ5.2/5.4ï¼‰

```python
def calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))
```

å…¶å®ƒæ²¡ä»€ä¹ˆå˜åŒ–ï¼Œä½†ä¸ºäº†æ–¹ä¾¿åˆ—å‡º

```python
coeffs = train_model(lr=100)
acc(coeffs)
show_coeffs()
```

å¦‚æœä¸ç”¨æ¿€æ´»å‡½æ•°ä¼šå¯¼è‡´å¾ˆå¤šæ•°å­—æº¢å‡ºåˆ°0~1ä¹‹å¤–ï¼Œæœ€ç»ˆä¼šç‰¹åˆ«å½±å“è®­ç»ƒæ•ˆæœã€‚æ¯”å¦‚å¦‚æœä¸ç”¨æ¿€æ´»å‡½æ•°ï¼Œä¾ç„¶lr=100ï¼Œé‚£ä¹ˆæœ€åä¼šå¯¼è‡´æ¨¡å‹ä¸æ”¶æ•›ï¼Œåä¹‹æ¨¡å‹æ”¶æ•›å¾—å¾ˆå¥½ï¼Œaccuracyæé«˜äº†å¾ˆå¤šã€‚

### 5.7 æµ‹è¯•é›†

```python
tst_df = pd.read_csv(path/'test.csv')
tst_df['Fare'] = tst_df.Fare.fillna(0)
tst_df.fillna(modes, inplace=True)
tst_df['LogFare'] = np.log(tst_df['Fare']+1)
tst_df = pd.get_dummies(tst_df, columns=["Sex","Pclass","Embarked"])
tst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)
tst_indep = tst_indep / vals
tst_df['Survived'] = (calc_preds(coeffs, tst_indep)>0.5).int()
sub_df = tst_df[['PassengerId','Survived']]
sub_df.to_csv('sub.csv', index=False)
!head sub.csv
```

### 5.8 matrixè½¬æ¢

è¿˜æ˜¯5.6æåˆ°çš„predsçš„è®¡ç®—å‡½æ•°ï¼Œå†æ›´æ–°ä¸€ä¸‹ï¼Œç”¨æ›´åŠ çŸ©é˜µçš„æ–¹æ³•è®¡ç®—

```python
def calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)
```

åŒæ ·çš„ï¼Œ5.4ä¸­coeffséšæœºç”Ÿæˆä¹Ÿè¦ç”ŸæˆçŸ©é˜µ

```python
def init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()
```

åŒæ ·çš„ï¼Œ5.4ä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å› å˜é‡ï¼Œä¹Ÿè½¬æˆçŸ©é˜µï¼Œè¿™ä¸ªå¿…é¡»è¦è½¬æ¢æˆçŸ©é˜µï¼Œè¿™æ ·æ‰èƒ½å¾—åˆ°n*1çš„çŸ©é˜µå†æ±‚å¹³å‡ï¼Œå¦åˆ™åœ¨è®¡ç®—lossçš„æ—¶å€™çŸ©é˜µ-rank1çš„tensorä¼šè¢«å¹¿æ’­æˆn\*nçš„çŸ©é˜µ

```python
trn_dep = trn_dep[:,None]
val_dep = val_dep[:,None]
```

### 5.9 ç¥ç»ç½‘ç»œ

- æ„å»ºæ¨¡å‹å‰å‚æ•°çš„å½¢å¼å·²ç»æƒ³å¥½äº†ï¼Œæ‰€ä»¥å…ˆåˆå§‹åŒ–å‚æ•°

```python
def init_coeffs(n_hidden=20):
    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden
    layer2 = torch.rand(n_hidden, 1)-0.3
    const = torch.rand(1)[0]
    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()
```

- æ„å»ºæ¨¡å‹çš„å‡½æ•°

```python
import torch.nn.functional as F
def calc_preds(coeffs, indeps):
    l1,l2,const = coeffs
    res = F.relu(indeps@l1)
    res = res@l2 + const
    return torch.sigmoid(res)
```

- è®¡ç®—lossçš„å‡½æ•°

```python
def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()
```

- æ›´æ–°å‚æ•°çš„å‡½æ•°

```python
def update_coeffs(coeffs, lr):
    for layer in coeffs:
        layer.sub_(layer.grad * lr)
        layer.grad.zero_()
```

- 1 epochçš„å‡½æ•°

```python
def one_epoch(coeffs, lr):
    loss = calc_loss(coeffs, trn_indep, trn_dep) #å¼•ç”¨å‡½æ•°
    loss.backward()
    with torch.no_grad(): update_coeffs(coeffs, lr) #å¼•ç”¨å‡½æ•°
    print(f"{loss:.3f}", end="; ")
```

- è®­ç»ƒæ¨¡å‹çš„å‡½æ•°

```python
def train_model(epochs=30, lr=0.01):
    torch.manual_seed(442) #ä¸ºäº†å¯å¤ç°
    coeffs = init_coeffs()
    for i in range(epocks): one_epoch(coeffs, lr=lr)
    return coeffs
```

- è·‘èµ·æ¥

```python
coeffs = train_model(lr=20)
```

- accuracyçš„å‡½æ•°

```python
def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs,val_indep)>0.5)).float().mean()
```

- è®¡ç®—accuracy

```python
acc(coeffs)
```

### 5.10 æ·±åº¦å­¦ä¹ 

- æ•°æ®å‡†å¤‡ä¸å˜
- æ„å»ºæ¨¡å‹å‰å‚æ•°çš„å½¢å¼å·²ç»æƒ³å¥½äº†ï¼Œæ‰€ä»¥å…ˆåˆå§‹åŒ–å‚æ•°

```python
#æ„å»ºä¸€ä¸ªå¤šå±‚ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬2ä¸ªéšè—å±‚å’Œ1ä¸ªè¾“å‡ºå±‚ï¼Œæ¯ä¸ªéšè—å±‚çš„è¾“å‡ºéƒ½æ˜¯10ä¸ª
def init_coeffs():
    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want
    sizes = [n_coeff] + hiddens + [1]
    n = len(sizes)
    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)] #å…ƒç´ ä¸ºtensorçš„list
    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)] #å…ƒç´ ä¸ºtensorçš„list
    for l in layers+consts: l.requires_grad_() #listä¸­çš„å…ƒç´ ç®€å•å †å 
    return layers,consts
```

- æ„å»ºæ¨¡å‹

```python
import torch.nn.functional as F
def calc_preds(coeffs, indeps):
    layers,consts = coeffs
    n = len(layers)
    res = indeps #è¾“å…¥ä½œä¸ºç»“æœçš„åˆå§‹åŒ–
    for i,l in enumerate(layers):
        res = res@l + consts[i] #è¿™ä¸ªæ˜¯tensorçš„ç›¸åŠ ä¼šè‡ªåŠ¨å¹¿æ’­
        if i!=n-1: res = F.relu(res) #é™¤äº†è¾“å‡ºå±‚ï¼Œå…¶å®ƒå±‚éƒ½ç”¨reluåšæ¿€æ´»å‡½æ•°
    return torch.sigmoid(res)
```

- è®¡ç®—lossçš„å‡½æ•°ï¼ˆæŠ„5.9ï¼‰

```python
def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()
```

- æ›´æ–°å‚æ•°çš„å‡½æ•°

```python
def update_coeffs(coeffs, lr):
    layers,consts = coeffs
    for layer in layers+consts:
        layer.sub_(layer.grad * lr)
        layer.grad.zero_()
```

- 1 epochçš„å‡½æ•°ï¼ˆæŠ„5.9ï¼‰

```python
def one_epoch(coeffs, lr):
    loss = calc_loss(coeffs, trn_indep, trn_dep) #å¼•ç”¨å‡½æ•°
    loss.backward()
    with torch.no_grad(): update_coeffs(coeffs, lr) #å¼•ç”¨å‡½æ•°
    print(f"{loss:.3f}", end="; ")
```

- è®­ç»ƒæ¨¡å‹çš„å‡½æ•°ï¼ˆæŠ„5.9ï¼‰

```python
def train_model(epochs=30, lr=0.01):
    torch.manual_seed(442) #ä¸ºäº†å¯å¤ç°
    coeffs = init_coeffs()
    for i in range(epocks): one_epoch(coeffs, lr=lr)
    return coeffs
```

- è·‘èµ·æ¥ï¼ˆæŠ„5.9ï¼Œä½†lråšäº†è°ƒæ•´ï¼Œå› ä¸ºå‚æ•°å¤šäº†ï¼‰

```python
coeffs = train_model(lr=2)
```

- accuracyçš„å‡½æ•°ï¼ˆæŠ„5.9ï¼‰

```python
def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs,val_indep)>0.5)).float().mean()
```

- è®¡ç®—accuracyï¼ˆæŠ„5.9ï¼‰

```python
acc(coeffs)
```

### 5.11 ç”¨frameworkæ„å»ºæ·±åº¦å­¦ä¹ 

#### 5.11.1 æ•°æ®å‡†å¤‡

- ä¸5.1å†…å®¹å¯¹æ¯”

```python
#æ•°æ®ä¸‹è½½
from pathlib import Path
import os

iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')
if iskaggle:
    path = Path('../input/titanic')
    !pip install -Uqq fastai
else:
    import zipfile,kaggle
    path = Path('titanic')
    if not path.exists():
        kaggle.api.competition_download_cli(str(path))
        zipfile.ZipFile(f'{path}.zip').extractall(path)
        
#è®¾ç½®æ ¼å¼
from fastai.tabular.all import * #éšå¼å¯¼å…¥äº†pandas as pd
pd.options.display.float_format = '{:.2f}'.format
set_seed(42)

#æ•°æ®å¤„ç†
df = pd.read_csv(path/'train.csv')
def add_features(df):
    df['LogFare'] = np.log1p(df['Fare'])
    df['Deck'] = df.Cabin.str[0].map(dict(A="ABC", B="ABC", C="ABC", D="DE", E="DE", F="FG", G="FG"))
    df['Family'] = df.SibSp+df.Parch
    df['Alone'] = df.Family==0
    df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')
    df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]
    df['Title'] = df.Title.map(dict(Mr="Mr",Miss="Miss",Mrs="Mrs",Master="Master"))
add_features(df)
#'Age', 'SibSp', 'Parch', 'LogFare'ï¼Œ'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'
#ç”ŸæˆDataLoadersï¼Œé‡Œé¢åŒ…å«äº†è®­ç»ƒé›†å’ŒéªŒè¯é›†
splits = RandomSplitter(seed=42)(df)
dls = TabularPandas(
    df, splits=splits,
    procs = [Categorify, FillMissing, Normalize],
    cat_names=["Sex","Pclass","Embarked","Deck", "Title"],
    cont_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Alone', 'TicketFreq', 'Family'],
    y_names="Survived", y_block = CategoryBlock(),
).dataloaders(path=".")
```

#### 5.11.2 æ¨¡å‹è®­ç»ƒ

- ç”¨äº†ç°æˆçš„å…¨è¿æ¥æ¨¡å‹

```python
#éšè—å±‚æ˜¯ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯è¾“å…¥è‡ªå˜é‡ä¸ªæ•°*10ï¼Œä¸€ä¸ªæ˜¯10*10ï¼Œè¾“å‡ºå±‚æ˜¯10*1
learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])
learn.lr_find(suggest_funcs=(slide, valley))
#Jeremyè¯´lré€‰å–slideå’Œvalleyä¹‹é—´çš„æ•°ä¼šå¾ˆå¥½
```

![](D:\Git\a\Path-Records\img\05-1-1.png)

```python
learn.fit(16, lr=0.03)
```

#### 5.11.3 æµ‹è¯•é›†

- è®­ç»ƒå¥½çš„æ¨¡å‹ç”¨å…¨æ–°çš„æµ‹è¯•é›†å»æµ‹è¯•æ³›åŒ–èƒ½åŠ›

```python
#trainæ–‡ä»¶ä¸­çš„Fareæ˜¯æ²¡æœ‰ç©ºçš„ï¼Œä½†æ˜¯testä¸­çš„æœ‰ç©ºï¼Œæ‰€ä»¥å¾—å…ˆè¡¥ä¸Šï¼Œå†åšå’Œtrainä¸­åŒæ ·çš„æ“ä½œ
tst_df = pd.read_csv(path/'test.csv')
tst_df['Fare'] = tst_df.Fare.fillna(0)
add_features(tst_df)

tst_dl = learn.dls.test_dl(tst_df)
preds,_ = learn.get_preds(dl=tst_dl)

tst_dl = learn.dls.test_dl(tst_df)
preds,_ = learn.get_preds(dl=tst_dl)
tst_df['Survived'] = (preds[:,1]>0.5).int()
sub_df = tst_df[['PassengerId','Survived']]
sub_df.to_csv('sub.csv', index=False)
!head sub.csv
```

#### 5.11.4 æ•´ä½“è¿è¡Œå¤šæ¬¡

- ç”±äºæ¯æ¬¡å‚æ•°éƒ½æ˜¯éšæœºç”Ÿæˆçš„ï¼Œæ‰€ä»¥éšæœºç”Ÿæˆå‚æ•°çš„è´¨é‡ä¹Ÿä¼šå½±å“æ¨¡å‹æœ€ç»ˆçš„è®­ç»ƒæ•ˆæœï¼Œå› æ­¤æˆ‘ä»¬å¤šæ¬¡ç”Ÿæˆå‚æ•°è®­ç»ƒæ¨¡å‹ï¼Œå–å¹³å‡å€¼

```python
def ensemble():
    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])
    with learn.no_bar(),learn.no_logging(): learn.fit(16, lr=0.03)
    return learn.get_preds(dl=tst_dl)[0] #[0]æ˜¯predsï¼Œæ˜¯ä¸€ä¸ªn*2çš„tensorï¼Œ[1]æ˜¯targets
learns = [ensemble() for _ in range(5)] #ç”Ÿæˆä¸€ä¸ªlistï¼Œè¿™ä¸ªlistä¸­åŒ…å«5ä¸ªtensor
ens_preds = torch.stack(learns).mean(0) #torch.stack(learns).shape = [5, 418, 2]ï¼Œens_preds.shape = [418, 2]
```

- æ±‚å¹³å‡å€¼æœ‰å¥½å‡ ç§æ–¹æ³•ï¼šâ‘ ç›´æ¥æ±‚5æ¬¡æ¦‚ç‡çš„å¹³å‡å€¼ï¼Œå†01åŒ–ï¼›â‘¡å…ˆ01åŒ–å†æ±‚5æ¬¡å¹³å‡å€¼ï¼Œå†01åŒ–ï¼›â‘¢å…ˆ01åŒ–ç„¶åå–ä¼—æ•°ã€‚ä¸€èˆ¬æƒ…å†µä¸‹â‘ å’Œâ‘¡çš„æ•ˆæœç›¸å¯¹æ¯”è¾ƒå¥½ï¼Œå¶å°”â‘¢æ¯”è¾ƒå¥½ã€‚

```python
tst_df['Survived'] = (ens_preds[:,1]>0.5).int()
sub_df = tst_df[['PassengerId','Survived']]
sub_df.to_csv('ens_sub.csv', index=False)
```

## 5: Tabular (fastbook9)

### 5.1 åˆ†ç±»è‡ªå˜é‡çš„å¤„ç†â€”â€”åˆ†ç±»åµŒå…¥

- åµŒå…¥å±‚Embedding = ç‹¬çƒ­ç¼–ç one-hot + çº¿æ€§å±‚ï¼ˆæ²¡æœ‰biasï¼‰

- Embeddingæœ¬èº«æ˜¯ä¸€ä¸ªrank-2çš„çŸ©é˜µ

- ä¸¾ä¾‹ï¼š

  - ç‹¬çƒ­ç¼–ç +çº¿æ€§å±‚

    å‡è®¾ä½ æœ‰ 5 ä¸ªç±»åˆ«ï¼Œæƒ³è¦å°†å…¶æ˜ å°„æˆä¸€ä¸ªé•¿åº¦ä¸º 3 çš„å‘é‡ã€‚ä½ å¯ä»¥è¿™æ ·åšï¼šâ‘ ç‹¬çƒ­ç¼–ç ï¼ˆone-hotï¼‰ä¸€ä¸ªç±»åˆ«ï¼š

    ```
    ç±»åˆ« 2 â†’ [0, 1, 0, 0, 0]
    ```

    â‘¡ç„¶åé€šè¿‡çº¿æ€§å±‚ï¼ˆæƒé‡çŸ©é˜µ `W.shape = (5, 3)`ï¼‰ï¼š

    ```
    è¾“å‡º = one_hot_vector @ W
    ```

    è¿™ä¸ªæœ¬è´¨å°±æ˜¯ç”¨ one-hot é€‰ä¸­çŸ©é˜µ `W` ä¸­çš„æŸä¸€è¡Œã€‚
  
  - åµŒå…¥å±‚
  
    åµŒå…¥å±‚ä¹Ÿæ˜¯ä¸€ä¸ªæŸ¥è¡¨æ“ä½œï¼Œ`Embedding(num_embeddings=5, embedding_dim=3)` æœ¬è´¨ä¸Šç»´æŠ¤ä¸€ä¸ªå½¢çŠ¶ä¸º `(5, 3)` çš„æƒé‡çŸ©é˜µï¼Œæ¯æ¬¡æ ¹æ®ç±»åˆ«ç´¢å¼•ï¼Œç›´æ¥å–å‡ºå¯¹åº”è¡Œä½œä¸ºè¾“å‡ºã€‚æ‰€ä»¥å®ƒåšçš„äº‹æƒ…æ˜¯ï¼š
  
    ```
    embedding = nn.Embedding(5, 3)
    output = embedding(torch.tensor([2]))  # å°±æ˜¯è¿”å› embedding.weight[2]
    ```

![](D:\Git\a\Path-Records\img\dlcf_0901.png)

- è¿™æœ¬ä¹¦ä¸¾ä¾‹äº†ä¸€ç¯‡è®ºæ–‡ï¼Œè®ºæ–‡æ˜¯é€šè¿‡å„ç§æ•°æ®é¢„æµ‹é”€é‡ï¼Œå…¶ä¸­ä¸€ä¸ªè‡ªå˜é‡å°±æ˜¯åŸå¸‚ï¼Œè®­ç»ƒå¥½ä¹‹åä½œè€…ç”»å‡ºäº†åµŒå…¥çŸ©é˜µï¼ˆä¸‹å›¾ï¼‰ï¼Œå¯ä»¥çœ‹åˆ°embeddingè‡ªåŠ¨å­¦ä¹ åˆ°äº†åŸå¸‚çš„åœ°ç†ä½ç½®ã€‚äº‹å®ä¸Šï¼Œåœ¨è®­ç»ƒä¹‹åï¼Œå•†åº—åµŒå…¥ä¹‹é—´çš„è·ç¦»å’Œå®é™…çš„åœ°ç†ä½ç½®è·ç¦»éå¸¸ç›¸è¿‘ã€‚

![](D:\Git\a\Path-Records\img\dlcf_0902.png)

- ç‹¬çƒ­dummyå’ŒåµŒå…¥çš„å·®å¼‚ï¼šç±»åˆ«å¤šã€ä½¿ç”¨æ·±åº¦å­¦ä¹  â†’ å¼ºçƒˆå»ºè®®ä½¿ç”¨åµŒå…¥ï¼ˆembeddingï¼‰

| å¯¹æ¯”ç»´åº¦            | ç‹¬çƒ­ç¼–ç ï¼ˆOne-hot Encodingï¼‰           | åµŒå…¥ï¼ˆEmbeddingï¼‰                                    |
| ------------------- | -------------------------------------- | ---------------------------------------------------- |
| **ç»´åº¦å¤§å°**        | é«˜ç»´ç¨€ç–ï¼ˆæ¯ä¸ªç±»åˆ«ä¸€ä¸ªç»´åº¦ï¼‰           | ä½ç»´ç¨ å¯†ï¼ˆé€šå¸¸å‡ ç»´~å‡ åç»´ï¼‰                          |
| **å†…å­˜/è®¡ç®—æ•ˆç‡**   | å å†…å­˜å¤šï¼Œè®¡ç®—æ…¢                       | å å†…å­˜å°‘ï¼Œè®¡ç®—å¿«                                     |
| **ç±»åˆ«ç›¸ä¼¼æ€§è¡¨è¾¾**  | æ— æ³•è¡¨è¾¾ç±»åˆ«é—´å…³ç³»ï¼ˆæ¯ä¸ªç±»åˆ«å½¼æ­¤ç‹¬ç«‹ï¼‰ | å¯ä»¥è¡¨è¾¾ç±»åˆ«é—´ç›¸ä¼¼æ€§ï¼ˆè·ç¦»è¶Šè¿‘è¶Šç›¸ä¼¼ï¼‰               |
| **æ˜¯å¦å¯è®­ç»ƒ**      | å¦ï¼Œå›ºå®šä¸å¯å­¦ä¹                        | æ˜¯ï¼ŒåµŒå…¥å‘é‡æ˜¯æ¨¡å‹å‚æ•°ï¼Œ**å¯å­¦ä¹ **                   |
| **æ¨¡å‹æ³›åŒ–èƒ½åŠ›**    | å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œç‰¹åˆ«æ˜¯ç±»åˆ«å¾ˆå¤šæ—¶           | æ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼Œèƒ½ä»ç›¸ä¼¼ç±»åˆ«ä¸­å€ŸåŠ›                     |
| **é€‚åˆçš„æ¨¡å‹ç±»å‹**  | çº¿æ€§æ¨¡å‹ã€æ ‘æ¨¡å‹ç­‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹     | æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯ç¥ç»ç½‘ç»œï¼‰                       |
| **å¯è§£é‡Šæ€§/å¯è§†åŒ–** | ä¸å…·å¤‡å¯è§†åŒ–è¯­ä¹‰ç»“æ„çš„èƒ½åŠ›             | åµŒå…¥å‘é‡å¯é™ç»´å¯è§†åŒ–ï¼ˆå¦‚ç”¨ t-SNEã€PCA å±•ç¤ºèšç±»ç»“æ„ï¼‰ |
| **ç±»åˆ«æ•°é‡é€‚åº”æ€§**  | ç±»åˆ«å°‘ï¼ˆ<10ï¼‰è¾ƒé€‚åˆ                    | ç±»åˆ«å¤šï¼ˆå‡ åä¸ªåˆ°å‡ åƒä¸ªï¼‰æ›´é€‚åˆ                       |
| **è®­ç»ƒé€Ÿåº¦å½±å“**    | è®­ç»ƒæ…¢ï¼Œå°¤å…¶æ˜¯é«˜ç»´æ•°æ®                 | è®­ç»ƒå¿«ï¼Œå‚æ•°å°‘ï¼Œä¼˜åŒ–ç©ºé—´å°                           |

### 5.2 Beyond Deep Learning

ç°ä»£æœºå™¨å­¦ä¹ å¯ä»¥å½’ç»“ä¸ºå‡ ç§å¹¿æ³›é€‚ç”¨çš„å…³é”®æŠ€æœ¯ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç»å¤§å¤šæ•°æ•°æ®é›†æœ€é€‚åˆç”¨ä¸¤ç§æ–¹æ³•å»ºæ¨¡ï¼š

+   å†³ç­–æ ‘é›†æˆï¼ˆå³éšæœºæ£®æ—å’Œæ¢¯åº¦æå‡æœºï¼‰ï¼Œä¸»è¦ç”¨äºç»“æ„åŒ–æ•°æ®ï¼ˆæ¯”å¦‚å¤§å¤šæ•°å…¬å¸æ•°æ®åº“è¡¨ä¸­å¯èƒ½æ‰¾åˆ°çš„æ•°æ®ï¼‰è®­ç»ƒå¿«ï¼Œè§£å®æ€§å¼ºï¼Œæœ‰å·¥å…·å’Œæ–¹æ³•å¯ä»¥å›ç­”ç›¸å…³é—®é¢˜ï¼Œæ¯”å¦‚ï¼šæ•°æ®é›†ä¸­å“ªäº›åˆ—å¯¹ä½ çš„é¢„æµ‹æœ€é‡è¦ï¼Ÿå®ƒä»¬ä¸å› å˜é‡æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿå®ƒä»¬å¦‚ä½•ç›¸äº’ä½œç”¨ï¼Ÿå“ªäº›ç‰¹å®šç‰¹å¾å¯¹æŸä¸ªç‰¹å®šè§‚å¯Ÿæœ€é‡è¦ï¼Ÿ

+   ä½¿ç”¨ SGD å­¦ä¹ çš„å¤šå±‚ç¥ç»ç½‘ç»œï¼ˆå³æµ…å±‚å’Œ/æˆ–æ·±åº¦å­¦ä¹ ï¼‰ï¼Œä¸»è¦ç”¨äºéç»“æ„åŒ–æ•°æ®ï¼ˆæ¯”å¦‚éŸ³é¢‘ã€å›¾åƒå’Œè‡ªç„¶è¯­è¨€ï¼‰

è¿™ä¸€å‡†åˆ™çš„ä¾‹å¤–æƒ…å†µæ˜¯å½“æ•°æ®é›†ç¬¦åˆä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€æ—¶ï¼š

+   æœ‰ä¸€äº›é«˜åŸºæ•°åˆ†ç±»å˜é‡éå¸¸é‡è¦ï¼ˆâ€œåŸºæ•°â€æŒ‡ä»£è¡¨ç¤ºç±»åˆ«çš„ç¦»æ•£çº§åˆ«çš„æ•°é‡ï¼Œå› æ­¤é«˜åŸºæ•°åˆ†ç±»å˜é‡æ˜¯æŒ‡åƒé‚®æ”¿ç¼–ç è¿™æ ·å¯èƒ½æœ‰æ•°åƒä¸ªå¯èƒ½çº§åˆ«çš„å˜é‡ï¼‰ã€‚

+   æœ‰ä¸€äº›åŒ…å«æœ€å¥½ç”¨ç¥ç»ç½‘ç»œç†è§£çš„æ•°æ®çš„åˆ—ï¼Œæ¯”å¦‚çº¯æ–‡æœ¬æ•°æ®ã€‚

åœ¨å®è·µä¸­ï¼Œäº‹æƒ…å¾€å¾€æ²¡æœ‰é‚£ä¹ˆæ˜ç¡®ï¼Œé€šå¸¸ä¼šæœ‰é«˜åŸºæ•°å’Œä½åŸºæ•°åˆ†ç±»å˜é‡ä»¥åŠè¿ç»­å˜é‡çš„æ··åˆã€‚å¾ˆæ˜æ˜¾æˆ‘ä»¬éœ€è¦å°†**å†³ç­–æ ‘**é›†æˆæ·»åŠ åˆ°æˆ‘ä»¬çš„å»ºæ¨¡å·¥å…·ç®±ä¸­ï¼

è¦ç”¨åˆ°å†³ç­–æ ‘ï¼Œæˆ‘ä»¬éœ€è¦å‡ ä¸ªpackageï¼šScikit-learn æ˜¯ä¸€ä¸ªæµè¡Œçš„åº“ï¼Œç”¨äºåˆ›å»ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨çš„æ–¹æ³•ä¸åŒ…æ‹¬æ·±åº¦å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€äº›è¡¨æ ¼æ•°æ®å¤„ç†å’ŒæŸ¥è¯¢ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ Pandas åº“ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜éœ€è¦ NumPyï¼Œå› ä¸ºè¿™æ˜¯ sklearn å’Œ Pandas éƒ½ä¾èµ–çš„ä¸»è¦æ•°å€¼ç¼–ç¨‹åº“ã€‚

### 5.3 æ•°æ®é›†

#### 5.3.1 å¤„ç†æ•°æ®é›†

```python
from fastai.tabular.all import *
path = Path('../input/bluebook-for-bulldozers')
path.ls(file_type='text')
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
df = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)
df.columns
#ä¸€äº›åˆ†ç±»æ•°æ®å¯èƒ½åšçš„æ•´ç†
df.ProductSize.unique()
sizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'
df['ProductSize'] = df['ProductSize'].astype('category')
df['ProductSize'] = df['ProductSize'].cat.set_categories(sizes, ordered=True)
df.ProductSize.unique()
#å› å˜é‡
df['SalePrice']=np.log(df['SalePrice'])

#æ—¥æœŸç±»æ•°æ®çš„å¤„ç†
df = add_datepart(df, 'saledate')
df_test = pd.read_csv(path/'Test.csv', low_memory=False)
df_test = add_datepart(df_test, 'saledate')
' '.join(o for o in df.columns if o.startswith('sale'))
```

- è®­ç»ƒå†³ç­–æ ‘çš„åŸºæœ¬æ­¥éª¤å¯ä»¥å¾ˆå®¹æ˜“åœ°å†™ä¸‹æ¥ï¼š

  1ã€ä¾æ¬¡å¾ªç¯æ•°æ®é›†çš„æ¯ä¸€åˆ—ã€‚

  2ã€å¯¹äºæ¯ä¸€åˆ—ï¼Œä¾æ¬¡å¾ªç¯è¯¥åˆ—çš„æ¯ä¸ªå¯èƒ½çº§åˆ«ã€‚

  3ã€å°è¯•å°†æ•°æ®åˆ†æˆä¸¤ç»„ï¼ŒåŸºäºå®ƒä»¬æ˜¯å¦å¤§äºæˆ–å°äºè¯¥å€¼ï¼ˆæˆ–è€…å¦‚æœå®ƒæ˜¯ä¸€ä¸ªåˆ†ç±»å˜é‡ï¼Œåˆ™åŸºäºå®ƒä»¬æ˜¯å¦ç­‰äºæˆ–ä¸ç­‰äºè¯¥åˆ†ç±»å˜é‡çš„æ°´å¹³ï¼‰ã€‚

  4ã€æ‰¾åˆ°è¿™ä¸¤ç»„ä¸­æ¯ç»„çš„å¹³å‡é”€å”®ä»·æ ¼ï¼Œå¹¶æŸ¥çœ‹è¿™ä¸è¯¥ç»„ä¸­æ¯ä¸ªè®¾å¤‡çš„å®é™…é”€å”®ä»·æ ¼æœ‰å¤šæ¥è¿‘ã€‚å°†è¿™è§†ä¸ºä¸€ä¸ªéå¸¸ç®€å•çš„â€œæ¨¡å‹â€ï¼Œå…¶ä¸­æˆ‘ä»¬çš„é¢„æµ‹åªæ˜¯è¯¥é¡¹ç»„çš„å¹³å‡é”€å”®ä»·æ ¼ã€‚

  5ã€åœ¨å¾ªç¯éå†æ‰€æœ‰åˆ—å’Œæ¯ä¸ªå¯èƒ½çš„çº§åˆ«åï¼Œé€‰æ‹©ä½¿ç”¨è¯¥ç®€å•æ¨¡å‹ç»™å‡ºæœ€ä½³é¢„æµ‹çš„åˆ†å‰²ç‚¹ã€‚

  6ã€ç°åœ¨æˆ‘ä»¬çš„æ•°æ®æœ‰ä¸¤ç»„ï¼ŒåŸºäºè¿™ä¸ªé€‰å®šçš„åˆ†å‰²ã€‚å°†æ¯ä¸ªç»„è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡è¿”å›åˆ°æ­¥éª¤ 1 ä¸ºæ¯ä¸ªç»„æ‰¾åˆ°æœ€ä½³åˆ†å‰²ã€‚7ã€é€’å½’åœ°ç»§ç»­è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æ¯ä¸ªç»„è¾¾åˆ°æŸä¸ªåœæ­¢æ ‡å‡†-ä¾‹å¦‚ï¼Œå½“ç»„ä¸­åªæœ‰ 20 ä¸ªé¡¹ç›®æ—¶åœæ­¢è¿›ä¸€æ­¥åˆ†å‰²ã€‚

- æˆ‘ä»¬å¯ä»¥èŠ‚çœä¸€äº›æ—¶é—´ï¼Œä½¿ç”¨å†…ç½®åœ¨ sklearn ä¸­çš„å®ç°ã€‚ä¸ºæ­¤ï¼Œè¦åšä¸€äº›æ•°æ®å‡†å¤‡ã€‚

#### 5.3.2 å¤„ç†å­—ç¬¦ä¸²-ä½¿ç”¨ TabularPandas å’Œ TabularProc

- å¤„ç†å­—ç¬¦ä¸²å’Œç¼ºå¤±æ•°æ®

```python
#æ‹†åˆ†æµ‹è¯•é›†å’ŒéªŒè¯é›†id
cond = (df.saleYear<2011) | (df.saleMonth<10)
train_idx = np.where( cond)[0]
valid_idx = np.where(~cond)[0]
splits = (list(train_idx),list(valid_idx))

#è¯†åˆ«ä¸€äº›è¿ç»­è‡ªå˜é‡ã€åˆ†ç±»è‡ªå˜é‡å’Œå› å˜é‡ï¼Œå¹¶ä½¿å¾—å®ƒä»¬å…·æœ‰æ•°å­—æ˜¯å±æ€§
procs = [Categorify, FillMissing]
dep_var = 'SalePrice'
cont,cat = cont_cat_split(df, 1, dep_var=dep_var) #contè¿”å›è¿ç»­è‡ªå˜é‡çš„title
to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)
#len(to.train),len(to.valid)
#to.show(3) #å¯ä»¥çœ‹åˆ°toæ²¡æœ‰saledateï¼Œå˜æˆäº†å¾ˆå¤šæ—¥æœŸæ‹†åˆ†ï¼›salepriceåœ¨æœ€å
#to.items.head(3) #å˜é‡éƒ½ä¸æ˜¯å­—ç¬¦ä¸²äº†ï¼Œè€Œæ˜¯æ•°å­—

#ä¿å­˜
'''
from fastcore.foundation import Path
from fastcore.xtras import save_pickle, load_pickle
path_out = Path('../working')
save_pickle(path_out/'to.pkl', to)
to_loaded = load_pickle(path_out/'to.pkl')
'''
```

- TabularPandasï¼šæ˜¯ fastai åº“ä¸­ç”¨äºå¤„ç†è¡¨æ ¼æ•°æ®ï¼ˆtabular dataï¼‰çš„ä¸€ä¸ªå…³é”®ç±»ï¼Œç”¨äºå°è£…å¹¶é¢„å¤„ç†è¡¨æ ¼æ•°æ®ï¼Œä»¥ä¾¿ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ƒç›¸å½“äºæŠŠ pandas.DataFrame åŒ…è£…æˆäº†ä¸€ä¸ªå¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒçš„ç»“æ„ï¼ˆï¼‰ã€‚

| åŠŸèƒ½                          | è¯´æ˜                                                         |
| ----------------------------- | ------------------------------------------------------------ |
| **é¢„å¤„ç†æ•°æ®**                | åŒ…æ‹¬ç±»åˆ«ç¼–ç ï¼ˆCategorifyï¼‰ã€ç¼ºå¤±å€¼å¡«è¡¥ï¼ˆFillMissingï¼‰ã€æ•°å€¼æ ‡å‡†åŒ–ï¼ˆNormalizeï¼‰ç­‰ |
| **åˆ’åˆ†æ•°æ®é›†**                | è®­ç»ƒé›†ã€éªŒè¯é›†ç­‰                                             |
| **æ”¯æŒ fastai çš„ DataLoader** | å¯ä»¥ç”¨ `.dataloaders()` ç›´æ¥è½¬æˆå¯è®­ç»ƒçš„ DataLoader          |
| **å°è£…å¤„ç†æµç¨‹**              | åŒ…å« `procs`ï¼ˆé¢„å¤„ç†æµç¨‹ï¼‰ã€`cont_names`ï¼ˆè¿ç»­å˜é‡ï¼‰ã€`cat_names`ï¼ˆåˆ†ç±»å˜é‡ï¼‰ç­‰å‚æ•° |

### 5.4 å†³ç­–æ ‘

å†³ç­–æ ‘é€šå¸¸æƒ…å†µä¸‹æ˜¯äºŒåˆ†æ³•çš„ï¼›

è€Œä¸”åŒä¸€ä¸ªç‰¹å¾åœ¨åŒä¸€æ£µæ ‘ä¸Šå¯èƒ½å‡ºç°åœ¨ä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸ŠèŠ‚ç‚¹ä¸Šï¼Œå°¤å…¶æ˜¯å½“ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³ç³»è¾ƒå¼ºæ—¶ï¼›

é€‰æ‹©èŠ‚ç‚¹ï¼šâ‘ éå†æ‰€æœ‰ç‰¹å¾ï¼Œâ‘¡å¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œå°è¯•æ‰€æœ‰å¯èƒ½çš„åˆ‡åˆ†ç‚¹ï¼ˆæ•°å€¼ç‰¹å¾å°è¯•é˜ˆå€¼ã€åˆ†ç±»ç‰¹å¾å°è¯•å­é›†ï¼‰ï¼Œâ‘¢è®¡ç®—æ¯ä¸ªåˆ‡åˆ†æ–¹å¼ä¸‹çš„â€œä¸çº¯åº¦æŒ‡æ ‡â€ï¼ˆå¦‚åŸºå°¼æŒ‡æ•°ï¼‰ï¼Œâ‘£é€‰æ‹©èƒ½å¤Ÿå¸¦æ¥æœ€å¤§çº¯åº¦æå‡ï¼ˆå³æœ€å¤§ä¿¡æ¯å¢ç›Šæˆ–æœ€å°åŸºå°¼æŒ‡æ•°ï¼‰çš„ç‰¹å¾å’Œåˆ‡åˆ†ç‚¹ï¼Œâ‘¤åˆ›å»ºè¯¥èŠ‚ç‚¹å¹¶å‘ä¸‹ç»§ç»­åˆ†è£‚ï¼ˆé€’å½’ï¼‰ï¼›

å‰ªæï¼šæœ€å¤§æ·±åº¦ï¼ˆæ ‘çš„å±‚ï¼‰ï¼ŒèŠ‚ç‚¹æ ·æœ¬æ•°ï¼Œå­èŠ‚ç‚¹æ ·æœ¬æ•°ï¼ŒèŠ‚ç‚¹å®Œå…¨çº¯å‡€ï¼Œæ²¡æœ‰ç‰¹å¾å†åˆ’åˆ†ï¼Œåˆ’åˆ†ä¸èƒ½å¸¦æ¥è¶³å¤Ÿå¢ç›Šï¼Œæœ€å¤§å¶å­èŠ‚ç‚¹æ ‘ã€‚

#### 5.4.1 åˆ›å»ºå†³ç­–æ ‘

- è¯•ä¸€è¯•å†³ç­–æ ‘

```python
#æ ¹æ®TabularPandasæ„å»ºè‡ªå˜é‡å’Œå› å˜é‡
xs,y = to.train.xs,to.train.y
valid_xs,valid_y = to.valid.xs,to.valid.y

#è®­ç»ƒå†³ç­–æ ‘
from sklearn.tree import DecisionTreeRegressor
m = DecisionTreeRegressor(max_leaf_nodes=4)
m.fit(xs, y);

#ç»˜åˆ¶å†³ç­–æ ‘
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
plot_tree(m, feature_names=xs.columns, filled=True, precision=2)
plt.show()
!pip install -U dtreeviz
import dtreeviz
samp_idx = np.random.permutation(len(y))[:500]
viz = dtreeviz.model(
    m,                                # ä½ çš„è®­ç»ƒå¥½çš„æ¨¡å‹
    X_train=xs.iloc[samp_idx],              # ç‰¹å¾å˜é‡
    y_train=y.iloc[samp_idx],               # ç›®æ ‡å˜é‡
    feature_names=xs.columns.tolist(),      # åˆ—å
    target_name=dep_var                    # å­—ç¬¦ä¸²å½¢å¼çš„ç›®æ ‡åˆ—åï¼Œæ¯”å¦‚ "SalePrice"
)
viz.view(orientation="LR", scale=1.6, label_fontsize=8, fontname="DejaVu Sans")

#ä¿®æ­£ä¸€äº›æ˜¾ç„¶çš„é—®é¢˜ï¼Œç›®çš„æ˜¯ä½¿å¾—å›¾åƒæ›´åŠ æ¸…æ™°
xs.loc[xs['YearMade']<1900, 'YearMade'] = 1950
valid_xs.loc[valid_xs['YearMade']<1900, 'YearMade'] = 1950
m = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)
viz = dtreeviz.model(
    m,
    X_train=xs.iloc[samp_idx],
    y_train=y.iloc[samp_idx],
    feature_names=xs.columns.tolist(),
    target_name=dep_var
)
viz.view(orientation="LR", scale=1.6, label_fontsize=8, fontname="DejaVu Sans")
```

![](D:\Git\a\Path-Records\img\dlcf_09in02.png)

![](D:\Git\a\Path-Records\img\dlcf_09in03.png)

- æ·±åŒ–å†³ç­–æ ‘

```python
#ç–¯ç‹‚åˆ†å‰ï¼Œä¸è®¾ç½®ä»»ä½•åœæ­¢æ¡ä»¶
m = DecisionTreeRegressor()
m.fit(xs, y);
def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)
def m_rmse(m, xs, y): return r_mse(m.predict(xs), y)
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)
'(0.0, 0.335804)è¿‡æ‹Ÿåˆäº†'

#æŸ¥çœ‹å†³ç­–æ ‘å¶æ•°é‡
m.get_n_leaves(), len(xs)
'(324555, 404710)å¶å­èŠ‚ç‚¹ä¸ªæ•°ç‰¹åˆ«å¤šï¼Œåˆ†å‰å¤ªè¿‡äº†'

#è®¾ç½®æ¯ä¸ªå¶å­ä¸Šé¢æœ€å°‘çš„æ ·æœ¬æ•°é‡
m = DecisionTreeRegressor(min_samples_leaf=25)
m.fit(to.train.xs, to.train.y)
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)
'(0.248564, 0.32344)å¥½ä¸€ç‚¹äº†'
m.get_n_leaves(), len(xs)
'(12397, 404710)'
```

#### 5.4.2 å†³ç­–æ ‘ä¸­çš„åˆ†ç±»å˜é‡

- Pandas æœ‰ä¸€ä¸ª`get_dummies`æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œå®é™…ä¸Šå¹¶æ²¡æœ‰è¯æ®è¡¨æ˜è¿™ç§æ–¹æ³•ä¼šæ”¹å–„æœ€ç»ˆç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°½å¯èƒ½é¿å…ä½¿ç”¨å®ƒï¼Œå› ä¸ºå®ƒç¡®å®ä¼šä½¿æ‚¨çš„æ•°æ®é›†æ›´éš¾å¤„ç†ã€‚åªè¦å°†å®ƒå½“æˆæ­£å¸¸å˜é‡ä¸€èµ·å¤„ç†å³å¯ã€‚ï¼ˆä¸è¿‡åœ¨æ·±åº¦å­¦ä¹ æ—¶å°±ä¼šç”¨åˆ°ï¼‰

### 5.5 éšæœºæ£®æ—

- bagging

1.  éšæœºé€‰æ‹©æ•°æ®çš„å­é›†ï¼ˆå³â€œå­¦ä¹ é›†çš„è‡ªåŠ©å¤åˆ¶â€ï¼‰ã€‚
1.  ä½¿ç”¨è¿™ä¸ªå­é›†è®­ç»ƒæ¨¡å‹ã€‚
1.  ä¿å­˜è¯¥æ¨¡å‹ï¼Œç„¶åè¿”å›åˆ°æ­¥éª¤ 1 å‡ æ¬¡ã€‚
1.  è¿™å°†ä¸ºæ‚¨æä¾›å¤šä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡å‹ã€‚è¦è¿›è¡Œé¢„æµ‹ï¼Œè¯·ä½¿ç”¨æ‰€æœ‰æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œç„¶åå–æ¯ä¸ªæ¨¡å‹é¢„æµ‹çš„å¹³å‡å€¼ã€‚

#### 5.5.1 åˆ›å»ºéšæœºæ£®æ—

```python
from sklearn.ensemble import RandomForestRegressor
def rf(xs, y, n_estimators=40, max_samples=200_000,
       max_features=0.5, min_samples_leaf=5, **kwargs):
    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,
        max_samples=max_samples, max_features=max_features,
        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)
# å¹¶è¡Œä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒåŠ é€Ÿè®­ç»ƒ
# æ ‘çš„æ•°é‡
# æ¯æ£µæ ‘ç”¨çš„æ ·æœ¬é‡
# æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„æœ€å¤§ç‰¹å¾æ¯”ä¾‹
# æœ€å°å¶å­æ ·æœ¬æ•°
# å¯ç”¨ OOB ä¼°è®¡ï¼Œç”¨äºæ¨¡å‹è¯„ä¼°
m = rf(xs, y);
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)
'(0.170992, 0.233527)'
```

éšæœºæ£®æ—æœ€é‡è¦çš„ç‰¹æ€§ä¹‹ä¸€æ˜¯å®ƒå¯¹è¶…å‚æ•°é€‰æ‹©ä¸å¤ªæ•æ„Ÿ

#### 5.5.2 è¢‹å¤–è¯¯å·®

åœ¨éšæœºæ£®æ—ä¸­ï¼Œæ¯æ£µæ ‘çš„è®­ç»ƒæ•°æ®æ˜¯ä»åŸå§‹è®­ç»ƒé›†æœ‰æ”¾å›é‡‡æ ·ï¼ˆbootstrap samplingï¼‰å¾—åˆ°çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š

- æ¯æ£µæ ‘éƒ½ç”¨çš„æ˜¯è®­ç»ƒé›†çš„ä¸€ä¸ªâ€œæœ‰æ”¾å›æŠ½æ ·â€çš„å­é›†ï¼ˆçº¦63%çš„åŸå§‹æ ·æœ¬ï¼‰ã€‚
- å‰©ä¸‹æœªè¢«æŠ½ä¸­çš„çº¦37%æ ·æœ¬å°±å«åšâ€œè¢‹å¤–æ ·æœ¬â€ï¼ˆOut-Of-Bag samplesï¼‰ã€‚

1. å¯¹æ¯ä¸€ä¸ªæ ·æœ¬ xix_ixiï¼š
   - æ‰¾å‡ºé‚£äº›æ²¡æœ‰ä½¿ç”¨å®ƒè®­ç»ƒçš„æ ‘ï¼ˆä¹Ÿå°±æ˜¯å®ƒæ˜¯è¿™äº›æ ‘çš„è¢‹å¤–æ ·æœ¬ï¼‰ã€‚
   - ç”¨è¿™äº›æ ‘å¯¹ xix_ixi è¿›è¡Œé¢„æµ‹ã€‚
   - å–è¿™äº›é¢„æµ‹çš„å¹³å‡å€¼ï¼ˆå›å½’ï¼‰æˆ–æŠ•ç¥¨ï¼ˆåˆ†ç±»ï¼‰ä½œä¸º xix_ixi çš„æœ€ç»ˆé¢„æµ‹ã€‚
2. æŠŠæ‰€æœ‰æ ·æœ¬çš„çœŸå®å€¼å’Œè¢‹å¤–é¢„æµ‹å€¼å¯¹æ¯”ï¼Œè®¡ç®—æ•´ä½“è¯¯å·®ï¼Œä½œä¸º **OOBè¯¯å·®ä¼°è®¡**ã€‚

```python
r_mse(m.oob_prediction_, y)
'0.21091'
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„ OOB é”™è¯¯è¿œä½äºéªŒè¯é›†é”™è¯¯ï¼ˆ0.233527ï¼‰ã€‚è¿™æ„å‘³ç€é™¤äº†æ­£å¸¸çš„æ³›åŒ–é”™è¯¯ä¹‹å¤–ï¼Œè¿˜æœ‰å…¶ä»–åŸå› å¯¼è‡´äº†è¯¥é”™è¯¯ã€‚

### 5.6 æ¨¡å‹è§£é‡Š

å¯¹äºè¡¨æ ¼æ•°æ®ï¼Œæ¨¡å‹è§£é‡Šå°¤ä¸ºé‡è¦ã€‚å¯¹äºç»™å®šçš„æ¨¡å‹ï¼Œæˆ‘ä»¬æœ€æœ‰å…´è¶£çš„æ˜¯ä»¥ä¸‹å†…å®¹ï¼š

+   æˆ‘ä»¬å¯¹ä½¿ç”¨ç‰¹å®šæ•°æ®è¡Œè¿›è¡Œçš„é¢„æµ‹æœ‰å¤šè‡ªä¿¡ï¼Ÿâ€”â€”é¢„æµ‹ç½®ä¿¡åº¦

+   å¯¹äºä½¿ç”¨ç‰¹å®šæ•°æ®è¡Œè¿›è¡Œé¢„æµ‹ï¼Œæœ€é‡è¦çš„å› ç´ æ˜¯ä»€ä¹ˆï¼Œå®ƒä»¬å¦‚ä½•å½±å“è¯¥é¢„æµ‹ï¼Ÿâ€”â€”ç‰¹å¾é‡è¦æ€§+æ ‘è§£é‡Šå™¨

+   å“ªäº›åˆ—æ˜¯æœ€å¼ºçš„é¢„æµ‹å› å­ï¼Œå“ªäº›å¯ä»¥å¿½ç•¥ï¼Ÿâ€”â€”ç‰¹å¾é‡è¦æ€§

+   å“ªäº›åˆ—åœ¨é¢„æµ‹ç›®çš„ä¸Šå®é™…ä¸Šæ˜¯å¤šä½™çš„ï¼Ÿâ€”â€”ç‰¹å¾é‡è¦æ€§

+   å½“æˆ‘ä»¬æ”¹å˜è¿™äº›åˆ—æ—¶ï¼Œé¢„æµ‹ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿâ€”â€”éƒ¨åˆ†ä¾èµ–

#### 5.6.1 æ ‘æ–¹å·®â€”â€”é¢„æµ‹ç½®ä¿¡åº¦

ä½¿ç”¨æ ‘ä¹‹é—´é¢„æµ‹çš„æ ‡å‡†å·®ï¼Œè€Œä¸ä»…ä»…æ˜¯å‡å€¼ï¼Œè¿™å‘Šè¯‰æˆ‘ä»¬é¢„æµ‹çš„*ç›¸å¯¹*ç½®ä¿¡åº¦ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬ä¼šæ›´è°¨æ…åœ°ä½¿ç”¨æ ‘ç»™å‡ºéå¸¸ä¸åŒç»“æœçš„è¡Œçš„ç»“æœï¼ˆæ›´é«˜çš„æ ‡å‡†å·®ï¼‰ï¼Œè€Œä¸æ˜¯åœ¨æ ‘æ›´ä¸€è‡´çš„æƒ…å†µä¸‹ä½¿ç”¨ç»“æœï¼ˆæ›´ä½çš„æ ‡å‡†å·®ï¼‰ã€‚å¦‚ä¸‹ï¼š

```python
#ç½®ä¿¡åº¦
preds = np.stack([t.predict(valid_xs) for t in m.estimators_])
print(preds.shape)
preds_std = preds.std(0)
preds_std[:5]
'''
array([0.30618819, 0.13729507, 0.09630049, 0.25064758, 0.12353961])
ç¬¬ä¸€ä¸ªç½®ä¿¡åº¦ä½ï¼Œç¬¬ä¸‰ä¸ªç½®ä¿¡åº¦é«˜
'''
```

#### 5.6.2 ç‰¹å¾é‡è¦æ€§

##### ï¼ˆ1ï¼‰ç‰¹å¾è´¡çŒ®åº¦

**â€æˆ‘ä»¬å¯ä»¥ç›´æ¥ä» sklearn çš„éšæœºæ£®æ—ä¸­è·å–è¿™äº›ä¿¡æ¯ï¼Œæ–¹æ³•æ˜¯æŸ¥çœ‹`feature_importances_`å±æ€§**ã€‚å¦‚ä¸‹ï¼š

```python
#é‡è¦æ€§
def rf_feat_importance(m, df):
    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}
                       ).sort_values('imp', ascending=False)
fi = rf_feat_importance(m, xs)
#è¡¨æ ¼å½¢å¼
fi[:10]
#ç»˜å›¾å½¢å¼
def plot_fi(fi):
    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)
plot_fi(fi[:30]);
```

![](D:\Git\a\Path-Records\img\dlcf_09in05.png)

ç‰¹å¾é‡è¦æ€§çš„è®¡ç®—ï¼š

- åŸºäºèŠ‚ç‚¹çº¯åº¦å¢ç›Šï¼ˆGini impurity / MSE å‡å°‘ï¼‰ï¼ˆè¿™æ˜¯ Scikit-learn ä¸­é»˜è®¤çš„æ–¹æ³•ï¼‰

  å¦‚æœä¸€ä¸ªç‰¹å¾åœ¨åˆ†è£‚èŠ‚ç‚¹æ—¶å¸¦æ¥äº†è¾ƒå¤§çš„çº¯åº¦æå‡ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰æˆ–æ–¹å·®å‡å°‘ï¼ˆå›å½’ä»»åŠ¡ï¼‰ï¼Œè¯´æ˜å®ƒå¯¹æ¨¡å‹æ›´é‡è¦ã€‚

  - åˆ†ç±»ä»»åŠ¡ä¸­ï¼šç‰¹å¾é‡è¦æ€§ = è¯¥ç‰¹å¾åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šå¸¦æ¥çš„çº¯åº¦æå‡çš„æ€»å’Œï¼ˆåŠ æƒå¹³å‡ï¼‰
  - å›å½’ä»»åŠ¡ä¸­ï¼šç‰¹å¾é‡è¦æ€§ = æ¯æ¬¡ä½¿ç”¨è¯¥ç‰¹å¾åˆ†è£‚æ—¶ï¼Œè®­ç»ƒè¯¯å·®ï¼ˆMSEï¼‰ä¸‹é™çš„æ€»å’Œï¼ˆåŠ æƒï¼‰
  - åŠ æƒè¯´æ˜ï¼šæ¯æ¬¡åˆ†è£‚å¯¹ impurity çš„æ”¹å–„ï¼Œä¼šæ ¹æ®å½“å‰èŠ‚ç‚¹çš„æ ·æœ¬æ•°é‡è¿›è¡ŒåŠ æƒï¼ˆæ ·æœ¬å¤šåˆ™æ›´é‡è¦ï¼‰ã€‚

- åŸºäºâ€œè¢‹å¤–è¯¯å·®å¢åŠ â€è®¡ç®—ï¼ˆPermutation Importanceï¼Œæ‰“ä¹±æ³•ï¼‰ï¼ˆè¿™ä¸ªæ–¹æ³•ä¸æ˜¯é»˜è®¤çš„ï¼Œä½†æ›´ç¨³å®šã€æ›´å…·è§£é‡Šæ€§ï¼‰
  - ç”¨åŸå§‹æ•°æ®è®­ç»ƒå¥½éšæœºæ£®æ—ã€‚
  - è®°å½•è¢‹å¤–ï¼ˆOOBï¼‰æ ·æœ¬çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚
  - å¯¹æŸä¸ªç‰¹å¾åˆ—éšæœºæ‰“ä¹±é¡ºåºï¼ˆç ´åå…¶ä¸ç›®æ ‡çš„å…³è”ï¼‰ã€‚
  - å†æ¬¡è®¡ç®—è¢‹å¤–æ ·æœ¬çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚
  - é‡è¦æ€§ = å‡†ç¡®ç‡ä¸‹é™å¹…åº¦

- SHAPï¼ˆå¤–éƒ¨åº“-åŒ…å«ä½†ä¸é™äºæ ‘æ¨¡å‹å¦‚ï¼šXGBoostã€éšæœºæ£®æ—RFï¼‰

  - å‡è®¾åªæœ‰ 3 ä¸ªç‰¹å¾ï¼šAã€Bã€Cï¼Œæˆ‘ä»¬è¦è®¡ç®—ç‰¹å¾ A çš„ SHAP å€¼ï¼Œéœ€è¦åšï¼šåˆ—ä¸¾æ‰€æœ‰åŒ…å« A çš„æ’åˆ—ç»„åˆï¼ˆæ¯”å¦‚ A, BA, CA, BCA, etc.ï¼‰ï¼›çœ‹ A åŠ å…¥æ¯ä¸ªç»„åˆæ—¶ï¼Œé¢„æµ‹å€¼æ”¹å˜äº†å¤šå°‘ï¼Œæ¯”å¦‚ï¼šé¢„æµ‹(BC) = 100, é¢„æµ‹(ABC) = 130 â†’ A çš„è¾¹é™…è´¡çŒ® = 30ï¼›å¯¹æ‰€æœ‰ç»„åˆçš„è¾¹é™…è´¡çŒ®æ±‚å¹³å‡ï¼Œè¿™æ ·ä½ å°±å¾—åˆ°äº† A çš„ Shapley å€¼ã€‚

  - è´¡çŒ®å¤§â‰ é¢„æµ‹å‡†ï¼ŒSHAP è¡¡é‡çš„æ˜¯å½±å“åŠ›è€Œä¸æ˜¯æ­£ç¡®æ€§ã€‚å®ƒå›ç­”çš„é—®é¢˜æ˜¯ï¼šâ€œå¦‚æœæ²¡æœ‰è¿™ä¸ªç‰¹å¾ï¼Œè¿™æ¬¡é¢„æµ‹ä¼šæœ‰å¤šå¤§ä¸åŒï¼Ÿâ€

  - æ­£å€¼ SHAPï¼šè¯¥ç‰¹å¾å°†é¢„æµ‹ç»“æœæ¨é«˜äº†ã€‚

    è´Ÿå€¼ SHAPï¼šè¯¥ç‰¹å¾å°†é¢„æµ‹ç»“æœå‹ä½äº†ã€‚

    ç»å¯¹å€¼å¤§ï¼šä»£è¡¨è¯¥ç‰¹å¾å¯¹å½“å‰é¢„æµ‹çš„å½±å“è¶Šå¼ºã€‚

    ç»å¯¹å€¼å°æˆ–0ï¼šä»£è¡¨è¯¥ç‰¹å¾å¯¹å½“å‰é¢„æµ‹å½±å“å¾ˆå¼±ï¼Œç”šè‡³å‡ ä¹æ²¡å‚ä¸ã€‚

##### ï¼ˆ2ï¼‰é€‰æ‹©é‡è¦ç‰¹å¾

**â€æœ‰äº†ç‰¹å¾çš„é‡è¦æ€§ä¹‹åï¼Œä¸ºäº†é¿å…ä½¿ç”¨å¤ªå¤šç‰¹å¾è¿›è¡Œæ‹Ÿåˆï¼Œå¯ä»¥é€‰æ‹©æ¯”è¾ƒé‡è¦çš„ç‰¹å¾é‡æ–°æ„å»ºéšæœºæ£®æ—**ï¼š

```python
to_keep = fi[fi.imp>0.005].cols
len(to_keep) '21'
xs_imp = xs[to_keep]
valid_xs_imp = valid_xs[to_keep]
m = rf(xs_imp,y)
m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)
'(0.181079, 0.230649)'
'ä¸ç”¨æ‰€æœ‰ç‰¹å¾çš„ç»“æœ(0.170992, 0.233527)ç›¸æ¯”ï¼Œå¥½åƒä¹Ÿæ²¡æœ‰æ€§èƒ½ä¸‹é™å¤šå°‘ï¼Œä½†æ˜¯å¯ä»¥å‡å°‘å¾ˆå¤šè®¡ç®—é‡å’Œç‰¹å¾éœ€æ±‚'
plot_fi(rf_feat_importance(m, xs_imp));
```

![](D:\Git\a\Path-Records\img\dlcf_09in06.png)

##### ï¼ˆ3ï¼‰è¯†åˆ«ç‰¹å¾é—´ç›¸ä¼¼åº¦

**â€å»é™¤å†—ä½™ç‰¹å¾ï¼šæœ‰ä¸€äº›ç‰¹å¾ç‰¹åˆ«ç›¸ä¼¼ï¼Œæ¯”å¦‚ProductGroupå’ŒProductGroupDescï¼Œåº”è¯¥å»é™¤è¿™äº›å†—ä½™çš„ç‰¹å¾ã€‚**

```python
#ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§
import seaborn as sns
from scipy.cluster.hierarchy import linkage
from scipy.spatial.distance import squareform

def cluster_columns(df):
    corr = df.corr()  # è®¡ç®—ç›¸å…³çŸ©é˜µ
    d = 1 - corr.abs()  # è®¡ç®—è·ç¦»çŸ©é˜µï¼ˆè¶Šç›¸å…³è·ç¦»è¶Šå°ï¼‰
    dist_linkage = linkage(squareform(d), method='average')  # å±‚æ¬¡èšç±»
    sns.clustermap(corr, row_linkage=dist_linkage, col_linkage=dist_linkage,
                   cmap='coolwarm', center=0, figsize=(10,10))
    plt.show()
cluster_columns(xs_imp)
```

![](D:\Git\a\Path-Records\img\dlcf_09in07.png)

å¯ä»¥çœ‹åˆ°ï¼Œçº¢è‰²çš„éƒ¨åˆ†å°±æ˜¯ç–¯ç‹‚ç›¸å¹²çš„ç‰¹å¾ï¼Œé‚£ä¹ˆæŠŠè¿™äº›å¯†åˆ‡ç›¸å…³çš„ç‰¹å¾ã€‚

##### ï¼ˆ4ï¼‰æ£€æŸ¥æ¨¡å‹æ•ˆæœ

**â€è®©æˆ‘ä»¬å°è¯•åˆ é™¤ä¸€äº›è¿™äº›å¯†åˆ‡ç›¸å…³ç‰¹å¾ï¼Œçœ‹çœ‹æ¨¡å‹æ˜¯å¦å¯ä»¥ç®€åŒ–è€Œä¸å½±å“å‡†ç¡®æ€§ã€‚**

```python
#æŸ¥çœ‹å»é™¤å…±çº¿ç‰¹å¾çš„æ•ˆæœ
def get_oob(df):
    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,
        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)
    m.fit(df, y)
    return m.oob_score_  #å¯¹æ‹Ÿåˆéšæœºæ£®æ—æ¥è¯´ï¼Œobb_score_æ˜¯RÂ²
get_oob(xs_imp) #åŸºçº¿
'0.8761015614269996'
{c:get_oob(xs_imp.drop(c, axis=1)) for c in (
    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',
    'fiModelDesc', 'fiBaseModel',
    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}
'''
{'saleYear': 0.8759547835051612,
 'saleElapsed': 0.8731632755222668,
 'ProductGroupDesc': 0.8773314254721,
 'ProductGroup': 0.8774958058240945,
 'fiModelDesc': 0.8755700975380052,
 'fiBaseModel': 0.8757558713831249,
 'Hydraulics_Flow': 0.877585297419066,
 'Grouser_Tracks': 0.8773646824594399,
 'Coupler_System': 0.877924411741774}
'''
to_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']
get_oob(xs_imp.drop(to_drop, axis=1)) #æ›´æ–°
'0.8745177203238274'
xs_final = xs_imp.drop(to_drop, axis=1)
valid_xs_final = valid_xs_imp.drop(to_drop, axis=1)
m = rf(xs_final, y)
m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)
'(0.182724, 0.233131)ï¼›åœ¨å»é™¤å…±çº¿ç‰¹å¾å‰æ˜¯(0.180774, 0.230802)ï¼›ç”¨æ‰€æœ‰ç‰¹å¾çš„ç»“æœ(0.170992, 0.233527)'
'''
Index(['YearMade', 'ProductSize', 'Coupler_System', 'fiProductClassDesc',
       'ModelID', 'saleElapsed', 'Hydraulics_Flow', 'fiSecondaryDesc',
       'Enclosure', 'ProductGroup', 'fiModelDesc', 'SalesID', 'MachineID',
       'Hydraulics', 'fiModelDescriptor', 'Drive_System'],
      dtype='object')
'''
```

| å±æ€§            | æ˜¯å¦åŸºäºè¢‹å¤–æ ·æœ¬ | ç”¨é€”                                         | ä¸¾ä¾‹                                    |
| --------------- | ---------------- | -------------------------------------------- | --------------------------------------- |
| oob_prediction_ | æ˜¯               | è®°å½•æ¯ä¸ªæ ·æœ¬åœ¨æœªè¢«ç”¨äºè®­ç»ƒçš„æ ‘ä¸­å¾—åˆ°çš„é¢„æµ‹å€¼ | ä¸€ä¸ªæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ ·æœ¬çš„ OOB é¢„æµ‹ |
| oob_score_      | æ˜¯               | è¡¡é‡æ•´ä½“æ¨¡å‹æ€§èƒ½ï¼Œåœ¨è¢‹å¤–æ ·æœ¬ä¸Šçš„å¾—åˆ†         | é€šå¸¸æ˜¯å›å½’ä¸­çš„ RÂ² æˆ–åˆ†ç±»å‡†ç¡®ç‡          |

#### 5.6.3 éƒ¨åˆ†ä¾èµ–-PDPå›¾

éƒ¨åˆ†ä¾èµ–å›¾è¯•å›¾å›ç­”è¿™ä¸ªé—®é¢˜ï¼šå¦‚æœä¸€è¡Œé™¤äº†å…³æ³¨çš„ç‰¹å¾ä¹‹å¤–æ²¡æœ‰å˜åŒ–ï¼Œå®ƒä¼šå¦‚ä½•å½±å“å› å˜é‡ï¼Ÿ

å·²çŸ¥'YearMade', 'ProductSize'æ˜¯éå¸¸é‡è¦çš„ç‰¹å¾ï¼Œå…ˆçœ‹ä¸€ä¸‹è¿™ä¸¤ä¸ªç‰¹å¾çš„æƒ…å†µï¼š

```python
p = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()
c = to.classes['ProductSize']
plt.yticks(range(len(c)), c);
ax = valid_xs_final['YearMade'].hist()
```

ç„¶åç»˜åˆ¶åä¾èµ–å›¾PDPå›¾ï¼Œä»¥YearMadeä¸ºä¾‹ï¼Œæˆ‘ä»¬å°†YearMadeåˆ—ä¸­çš„æ¯ä¸ªå€¼æ›¿æ¢ä¸º 1950ï¼Œç„¶åè®¡ç®—æ¯ä¸ªæ‹å–å“çš„é¢„æµ‹é”€å”®ä»·æ ¼ï¼Œå¹¶å¯¹æ‰€æœ‰æ‹å–å“è¿›è¡Œå¹³å‡ã€‚ç„¶åæˆ‘ä»¬å¯¹ 1951ã€1952 ç­‰å¹´ä»½åšåŒæ ·çš„æ“ä½œï¼Œç›´åˆ°æˆ‘ä»¬çš„æœ€ç»ˆå¹´ä»½ 2011ï¼š

```python
from sklearn.inspection import PartialDependenceDisplay
PartialDependenceDisplay.from_estimator(m,valid_xs_final,features=['YearMade','ProductSize'],kind='average').figure_.set_size_inches(12,4)
```

ProductSizeçš„éƒ¨åˆ†å›¾æœ‰ç‚¹ä»¤äººæ‹…å¿§ã€‚å®ƒæ˜¾ç¤ºæˆ‘ä»¬çœ‹åˆ°çš„æœ€ç»ˆç»„ï¼Œå³ç¼ºå¤±å€¼ï¼Œä»·æ ¼æœ€ä½ã€‚è¦åœ¨å®è·µä¸­ä½¿ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾å‡ºä¸ºä»€ä¹ˆå®ƒç»å¸¸ç¼ºå¤±ä»¥åŠè¿™æ„å‘³ç€ä»€ä¹ˆã€‚ç¼ºå¤±å€¼æœ‰æ—¶å¯ä»¥æ˜¯æœ‰ç”¨çš„é¢„æµ‹å› å­-è¿™å®Œå…¨å–å†³äºå¯¼è‡´å®ƒä»¬ç¼ºå¤±çš„åŸå› ã€‚ç„¶è€Œï¼Œæœ‰æ—¶å®ƒä»¬å¯èƒ½è¡¨æ˜æ•°æ®æ³„æ¼ã€‚

- **æ•°æ®æ³„éœ²**

å…³äºæ•°æ®æŒ–æ˜é—®é¢˜çš„ç›®æ ‡çš„ä¿¡æ¯å¼•å…¥ï¼Œè¿™äº›ä¿¡æ¯ä¸åº”è¯¥åˆæ³•åœ°ä»ä¸­æŒ–æ˜å‡ºæ¥ã€‚æ³„æ¼çš„ä¸€ä¸ªå¾®ä¸è¶³é“çš„ä¾‹å­æ˜¯ä¸€ä¸ªæ¨¡å‹å°†ç›®æ ‡æœ¬èº«ç”¨ä½œè¾“å…¥ï¼Œå› æ­¤å¾—å‡ºä¾‹å¦‚â€œé›¨å¤©ä¸‹é›¨â€çš„ç»“è®ºã€‚å®é™…ä¸Šï¼Œå¼•å…¥è¿™ç§éæ³•ä¿¡æ¯æ˜¯æ— æ„çš„ï¼Œå¹¶ä¸”ç”±æ•°æ®æ”¶é›†ã€èšåˆå’Œå‡†å¤‡è¿‡ç¨‹ä¿ƒæˆã€‚

è¯†åˆ«æ•°æ®æ³„æ¼æœ€å®ç”¨å’Œç®€å•æ–¹æ³•ï¼Œå³æ„å»ºæ¨¡å‹ï¼Œç„¶åæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

â‘  æ£€æŸ¥æ¨¡å‹çš„å‡†ç¡®æ€§æ˜¯å¦è¿‡äºå®Œç¾ã€‚

â‘¡ å¯»æ‰¾åœ¨å®è·µä¸­ä¸åˆç†çš„é‡è¦é¢„æµ‹å› å­ã€‚

â‘¢ å¯»æ‰¾åœ¨å®è·µä¸­ä¸åˆç†çš„éƒ¨åˆ†ä¾èµ–å›¾ç»“æœã€‚

- **é€šå¸¸å…ˆæ„å»ºæ¨¡å‹ï¼Œç„¶åè¿›è¡Œæ•°æ®æ¸…ç†æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œè€Œä¸æ˜¯åè¿‡æ¥ã€‚æ¨¡å‹å¯ä»¥å¸®åŠ©æ‚¨è¯†åˆ«æ½œåœ¨çš„æ•°æ®é—®é¢˜ã€‚**

å®ƒè¿˜å¯ä»¥å¸®åŠ©æ‚¨ç¡®å®šå“ªäº›å› ç´ å½±å“ç‰¹å®šé¢„æµ‹ï¼Œä½¿ç”¨æ ‘è§£é‡Šå™¨ã€‚

##### ï¼ˆ7ï¼‰æ ‘è§£é‡Šå™¨

